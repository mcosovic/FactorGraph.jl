var documenterSearchIndex = {"docs":
[{"location":"man/continuousOutput/#outputContinuous","page":"Output Data","title":"Output Data","text":"","category":"section"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"The main inference results are kept in the composite type ContinuousModel in the subtype ContinuousInference with fields:","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"fromFactor,\ntoVariable\nmeanFactorVariable,\nvarianceFactorVariable,\nfromVariable\ntoFactor\nmeanVariableFactor,\nvarianceVariableFactor,\nmean,\nvariance.","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"The values of messages from factor nodes to variable nodes can be accessed using meanFactorVariable and varianceFactorVariable fields, while values of messages from variable nodes to factor nodes are stored in meanVariableFactor and varianceVariableFactor fields. These values correspond to edges defined by factor and variable nodes, with indexes preserved in fromFactor - toVariable and fromVariable - toFactor fields.","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"Fields mean and variance define state variable marginal distributions.","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"The ContinuousInference field contains the GBP algorithm results. To describe the outputs, we will use the example shown below.","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"using FactorGraph\n\n#     x1   x2   x3\nH = [1.0  0.0  0.0;  # f1\n     2.0 -2.0  0.0;  # f2\n    -5.0 -4.0  9.0;  # f3\n     0.0  0.0  1.0]  # f4\n\n#     f1   f2   f3   f4\nz = [0.0; 1.7; 1.9; 0.2]\n\n#       f1   f2   f3    f4\nv = [1e-10; 0.1; 0.1; 1e-2]","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"The factor graph construction and message initialization is accomplished using continuousModel() function.","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"gbp = continuousModel(H, z, v)","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/continuousOutput/#Factor-graph","page":"Output Data","title":"Factor graph","text":"","category":"section"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"The first step in solving/analysing the above system/system of equations is forming a factor graph, where set of variable nodes mathcalX = x_1 x_2 x_3  is defined by state variables. The set of equations denotes the set of factor nodes mathcalF = f_1 f_2 f_3 f_4 .","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"<img src=\"../../assets/factorgraph.png\" class=\"center\"/>\n<figcaption>Figure 1: The factor graph with three variable nodes and four factor nodes.</figcaption>\n&nbsp;","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"Additionaly, we include the virtual factor node f_v_1, where factor node f_v_1 is a singly connected factor node used when the variable node is not directly observed, hence having variance v_x_1 to infty or a priori given mean and variance of state variables. To change defualt values of virtual factor nodes use:","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"gbp = continuousModel(H, z, v; mean = 0.1, variance = 1e60)","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/continuousOutput/#Messages-initialization","page":"Output Data","title":"Messages initialization","text":"","category":"section"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"The initialization step starts with messages from leaf factor nodes f_1 f_v_1 f_4  to variable nodes mathcalX. Then, variable nodes mathcalX forward the incoming messages received from leaf factor nodes along remaining edges defined by f_2 f_3 and mathcalX.","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/continuousOutput/#Messages-from-factor-nodes-to-variable-nodes","page":"Output Data","title":"Messages from factor nodes to variable nodes","text":"","category":"section"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"The GBP iterations computing messages from branch factor nodes f_2 f_3 to variable nodes mathcalX, using incoming messages from variable nodes mathcalX to branch factor nodes f_2 f_3 obtained in the previus step.","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"messageFactorVariable(gbp)\n\njulia> T = gbp.inference\njulia> [T.fromFactor T.toVariable T.meanFactorVariable T.varianceFactorVariable]\n5×4 Matrix{Float64}:\n 2.0  1.0   0.95      1.0e60\n 3.0  1.0  -0.1       6.4e59\n 2.0  2.0  -0.85      0.025\n 3.0  2.0  -0.025     0.056875\n 3.0  3.0   0.255556  1.97531e59","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"The first row defines the message from factor node f_2 to variable node x_1, the second row keeps the message from factor node f_3 to variable node x_1, etc.","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/continuousOutput/#Messages-from-variable-nodes-to-factor-nodes","page":"Output Data","title":"Messages from variable nodes to factor nodes","text":"","category":"section"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"Next, the algorithm proceeds with computing messages from variable nodes mathcalX to branch factor nodes f_1 f_2, using incoming messages from factor nodes mathcalF to variable nodes mathcalX.","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"messageVariableFactor(gbp)\n\njulia> T = gbp.inference\njulia> [T.fromVariable T.toFactor T.meanVariableFactor T.varianceVariableFactor]\n5×4 Matrix{Float64}:\n 1.0  2.0  -1.5625e-71  1.0e-10\n 2.0  2.0  -0.025       0.056875\n 1.0  3.0   9.5e-71     1.0e-10\n 2.0  3.0  -0.85        0.025\n 3.0  3.0   0.2         0.01","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"The first row defines the message from variable node x_1 to factor node f_2, the second row keeps the message from variable node x_2 to factor node f_2, etc.","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/continuousOutput/#Marginals","page":"Output Data","title":"Marginals","text":"","category":"section"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"The marginal of variable nodes mathcalX can be obtained using messages from factor nodes mathcalF to variable nodes mathcalX. Note that the mean value of marginal is adopted as the estimated value of the state variable. Thus, after 100 iterations, we obtain:","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"marginal(gbp)\n\njulia> [gbp.inference.mean gbp.inference.variance]\n3×2 Matrix{Float64}:\n  2.26718e-9  1.0e-10\n -0.598092    0.0173664\n -0.0267176   0.00381679","category":"page"},{"location":"man/continuousOutput/","page":"Output Data","title":"Output Data","text":"Where rows correspond with mean and variance values of the state variables x_1 x_2 x_3 .","category":"page"},{"location":"man/continuousTreeModel/#graphicalTreeModelContinuous","page":"Graphical Model","title":"Tree Graphical Model","text":"","category":"section"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"The FactorGraph supports the composite type ContinuousTreeModel based on the forward–backward message passing schedule, with three fields:","category":"page"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"ContinuousTreeGraph;\nContinuousInference;\nContinuousSystem.","category":"page"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"The subtype ContinuousTreeGraph describes the tree factor graph obtained based on the input data. The GBP inference and marginal values are kept in the subtype ContinuousInference. The system of the linear equations being solved is preserved in the subtype ContinuousSystem. Note that the function continuousTreeModel() returns the main composite type ContinuousTreeModel with all subtypes.","category":"page"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/continuousTreeModel/#Build-graphical-model","page":"Graphical Model","title":"Build graphical model","text":"","category":"section"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"Input arguments of the function continuousTreeModel() describe the tree graphical model, while the function returns ContinuousTreeModel type.","category":"page"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"Loads the system data passing arguments:","category":"page"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"gbp = continuousTreeModel(coefficient, observation, variances)","category":"page"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/continuousTreeModel/#Virtual-factor-nodes","page":"Graphical Model","title":"Virtual factor nodes","text":"","category":"section"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"The function continuousTreeModel() receives arguments by keyword to set the mean and variance of the virtual factor nodes to initiate messages from leaf variable nodes if the corresponding variable node does not have a singly connected factor node.","category":"page"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"gbp = continuousTreeModel(DATA; mean = value, variance = value)","category":"page"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"Default setting of the mean value is mean = 0.0, while the default variance is equal to variance = 1e10.","category":"page"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/continuousTreeModel/#Root-variable-node","page":"Graphical Model","title":"Root variable node","text":"","category":"section"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"The function continuousTreeModel() receives argument by keyword to set the root variable node.","category":"page"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"gbp = continuousTreeModel(DATA; root = index)","category":"page"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"Default setting of the root variable node is root = 1.","category":"page"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/continuousTreeModel/#Tree-factor-graph","page":"Graphical Model","title":"Tree factor graph","text":"","category":"section"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"Function checks whether the factor graph has a tree structure.","category":"page"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"tree = isTree(gbp)","category":"page"},{"location":"man/continuousTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"The tree structure of tha factor graph is marked as tree = true, the opposite is tree = false. The function accepts the composite type ContinuousTreeModel, as well as the type ContinuousModel.","category":"page"},{"location":"man/theoreticalBelief/#continuousVariables","page":"Continuous Variables","title":"Continuous Gaussian random variables","text":"","category":"section"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"The Gaussian belief propagation (GBP) represents a class of the BP, where local function psi(mathcalX_i) is defined as a continuous Gaussian distribution:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    mathcalN(z_imathcalX_iv_i) propto expBigg-cfracz_i-h(mathcalX_i)^22v_iBigg","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"where v_i is the variance, and the function h(mathcalX_i) connects the set of state variables mathcalX_i to the known z_i value. The linear-GBP model implies the linear function h(mathcalX_i). If the linear-GBP algorithm converges, it will converge to a fixed point representing a true means [1], regardless of the structure of the factor graph. Unlike means, the variances of the linear-GBP algorithm may not converge to correct values for graphical models with loops, while for models without loops (i.e., tree factor graph) variances will have exact values.","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"Thus, as an input, we observe a noisy linear system of equations with real coefficients and variables:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"        mathbfz=mathbfh(mathbfx)+mathbfu","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"where mathbf x=x_1dotsx_n^T is the vector of the state variables, mathbfh(mathbfx)= h_1(mathbfx), dots, h_k(mathbfx)^T is the vector of observation functions, mathbfz = z_1dotsz_m^T is the vector of observation values, and mathbfu = u_1dotsu_k^T is the vector of uncorrelated observation errors. The linear system of equations is an overdetermined mn arising in many technical fields, such as statistics, signal processing, and control theory.","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"Each observation is associated with observed value z_i, error  u_i, and function h_i(mathbfx). Under the assumption that observation errors u_i follow a zero-mean Gaussian distribution, the probability density function associated with the i-th observation is proportional to:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    mathcalN(z_imathbfxv_i) propto expBigg-cfracz_i-h_i(mathbfx)^22v_iBigg","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"where v_i is the observation variance defined by the error u_i, and the function h_i(mathbfx) connects the vector of state variables mathbfx to the value of the i-th observation.","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"The goal is to determine state variables mathbfx according to the noisy observed data mathbfz and a prior knowledge:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    p(mathbfxmathbfz)= cfracp(mathbfzmathbfx)p(mathbfx)p(mathbfz)","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"Assuming that the prior probability distribution p(mathbfx) is uniform, and given that p(mathbfz) does not depend on mathbfx, the maximum a posteriori solution reduces to the maximum likelihood solution, as given below:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    hatmathbfx= mathrmargmax_mathbfxp(mathbfxmathbfz)= mathrmargmax_mathbfxp(mathbfzmathbfx)=\n    mathrmargmax_mathbfxmathcalL(mathbfzmathbfx)","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"One can find the solution via maximization of the likelihood function mathcalL(mathbfzmathbfx), which is defined via likelihoods:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    hatmathbf x= mathrmarg max_mathbfxmathcalL(mathbfzmathbfx)=\n    mathrmarg max_mathbfx prod_i=1^m mathcalN(z_imathbfxv_i)","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"It can be shown that the maximum a posteriori solution can be obtained by solving the following optimization problem, known as the weighted least-squares (WLS) problem:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    hatmathbf x = mathrmargmin_mathbfx sum_i=1^m  cfracz_i-h_i(mathbf x)^2v_i","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"The state estimate hatmathbf x representing the solution of the optimization problem is known as the WLS estimator. The maximum likelihood and WLS estimator are equivalent to the maximum a posteriori solution.","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"","category":"page"},{"location":"man/theoreticalBelief/#vanillaGBP","page":"Continuous Variables","title":"Vanilla GBP algorithm","text":"","category":"section"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"Under the vanilla GBP algorithm, we imply the algorithm in which messages are calculated as described below.","category":"page"},{"location":"man/theoreticalBelief/#Message-from-a-factor-node-to-a-variable-node","page":"Continuous Variables","title":"Message from a factor node to a variable node","text":"","category":"section"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"Let us assume that the incoming messages mu_f_w to x_s(x_s), dots, mu_f_W to x_s(x_s) into the variable node x_s are Gaussian and represented by their mean-variance pairs (z_f_w to x_sv_f_w to x_s), dots, (z_f_W to x_sv_f_W to x_s). Note that these messages carry beliefs about the variable node x_s provided by its neighbouring factor nodes mathcalF_ssetminus f_i. It can be shown that the message mu_x_s to f_i(x_s) from the variable node x_s to the factor node f_i is proportional to:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    mu_x_s to f_i(x_s) propto mathcalN(x_sz_x_s to f_i v_x_s to f_i)","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"with mean z_x_s to f_i and variance v_x_s to f_i obtained as:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    z_x_s to f_i = Bigg( sum_f_a in mathcalF_ssetminus f_i cfracz_f_a to x_sv_f_a to x_sBigg) v_x_s to f_i \n    cfrac1v_x_s to f_i = sum_f_a in mathcalF_ssetminus f_i cfrac1v_f_a to x_s","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"After the variable node x_s receives the messages from all of the neighbouring factor nodes from the set mathcalF_ssetminus f_i, it evaluates the message mu_x_s to f_i(x_s), and sends it to the factor node f_i.","category":"page"},{"location":"man/theoreticalBelief/#Message-from-a-factor-node-to-a-variable-node-2","page":"Continuous Variables","title":"Message from a factor node to a variable node","text":"","category":"section"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"Due to linearity of the functions h_i(mathcalX_i), closed form expressions for these messages is easy to obtain and follow a Gaussian form:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    mu_f_i to x_s(x_s) propto mathcalN(x_sz_f_i to x_sv_f_i to x_s)","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"The message mu_f_i to x_s(x_s) can be computed only when all other incoming messages (variable to factor node messages) are known. Let us assume that the messages into factor nodes are Gaussian, denoted by:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"        mu_x_l to f_i(x_l) propto mathcalN(x_lz_x_l to f_i v_x_l to f_i)\n        vdots\n        mu_x_L to f_i(x_L) propto mathcalN(x_Lz_x_L to f_i v_x_L to f_i)","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"The Gaussian function associated with the factor node f_i is:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    mathcalN(z_imathcalX_i v_i) propto expBigg-cfracz_i-h_i(mathcalX_i)^2 2v_iBigg","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"The model contains only linear functions which we represent in a general form as:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    h_i(mathcalX_i) = C_x_s x_s + sum_x_b in mathcalX_isetminus x_s C_x_b x_b","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"where mathcalX_isetminus x_s is the set of variable nodes incident to the factor node f_i, excluding the variable node x_s.","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"It can be shown that the message mu_f_i to x_s(x_s) from the factor node f_i to the variable node x_s is represented by the Gaussian function \\eqref{BPGaussfv}, with mean z_f_i to x_s and variance v_f_i to x_s obtained as:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"        z_f_i to x_s = cfrac1C_x_s Bigg(z_i - sum_x_b in mathcalX_i setminus x_s\n        C_x_b z_x_b to f_i Bigg)\n        v_f_i to x_s = cfrac1C_x_s^2 Bigg( v_i + sum_x_b in mathcalX_i setminus x_s C_x_b^2 v_x_b to f_i  Bigg)","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"To summarise, after the factor node f_i receives the messages from all of the neighbouring variable nodes from the set mathcalX_isetminus x_s, it evaluates the message mu_f_i to x_s(x_s), and sends it to the variable node x_s.","category":"page"},{"location":"man/theoreticalBelief/#Marginal-inference","page":"Continuous Variables","title":"Marginal inference","text":"","category":"section"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"It can be shown that the marginal of the state variable x_s is represented by:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    p(x_s) propto mathcalN(x_shat x_sv_x_s)","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"with the mean value hat x_s and variance v_x_s:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    hat x_s = Bigg( sum_f_c in mathcalF_s cfracz_f_c to x_sv_f_c to x_sBigg) v_x_s \n    cfrac1v_x_s = sum_f_c in mathcalF_s cfrac1v_f_c to x_s","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"Finally, the mean-value hat x_s is adopted as the estimated value of the state variable x_s.","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"","category":"page"},{"location":"man/theoreticalBelief/#broadcastGBP","page":"Continuous Variables","title":"Broadcast GBP algorithm","text":"","category":"section"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"We can make a substantial improvement to the vanilla GBP algorithm's complexity by reducing the number of calculations per outgoing messages. We achieve this reduction by summarisation of all incoming messages for each variable and factor node instead of summarising all incoming messages per each outgoing message. This simple trick, allow a single variable or factor node to share these summations across all outgoing messages, hence calculating these summations only once. As a result, each outgoing message involves a constant number of operations improving the worst-case running complexity to mathcalO(nm). In this framework, we calculate the message from the variable node to the factor node as:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"        z_x_s to f_i = Bigg(z_x_s - cfracz_f_i to x_sv_f_i to x_sBigg) v_x_s to f_i \n        cfrac1v_x_s to f_i = w_x_s - cfrac1v_f_i to x_s","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"where:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    z_x_s = sum_f_a in mathcalF_s cfracz_f_a to x_sv_f_a to x_s  quad\n    w_x_s = sum_f_a in mathcalF_s cfrac1v_f_a to x_s","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"Likewise, the message from the factor node to the variable node is:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    z_f_i to x_s = cfrac1C_x_s left(z_i - z_f_i right) + z_x_s to f_i \n    v_f_i to x_s = cfrac1C_x_s^2 left( v_i +  v_f_i  right) - v_x_s to f_i","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"where:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    z_f_i = sum_x_b in mathcalX_i C_x_b z_x_b to f_i  quad\n    v_f_i = sum_x_b in mathcalX_i C_x_b^2 v_x_b to f_i","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"","category":"page"},{"location":"man/theoreticalBelief/#kahanGBP","page":"Continuous Variables","title":"Broadcast GBP and Kahan–Babuška algorithm","text":"","category":"section"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"The major drawback of the broadcast GBP algorithm is sensitivity to numerical errors because of the summation of floating-point numbers, due to possible significant differences in the values of incoming means and variances. However, this limitation can be alleviated with a compensated summation algorithm, such as the Kahan summation or the improved Kahan–Babuška algorithm. These algorithms increase the complexity of the operations by a constant factor, which means the time complexity of the worst-case remains unaffected. More precisely, we do summation that exists in the messages as:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"function kahan(summands, total, epsilon)\n    t = total + summands\n    if abs(total) >= abs(summands)\n        epsilon += (total - t) + summands\n    else\n        epsilon += (summands - t) + total\n    end\n    total = t\n\n    return total, epsilon\nend","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"","category":"page"},{"location":"man/theoreticalBelief/#dampGBP","page":"Continuous Variables","title":"The GBP with randomized damping","text":"","category":"section"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"We propose a randomized damping approach, where each mean value message from factor node to a variable node is damped independently with probability p, otherwise, the message is calculated as in the standard the GBP algorithm. The damped message is evaluated as a linear combination of the message from the previous and the current iteration, with weights alpha and 1 - alpha, respectively. More, precisly, the proposed randomized damping scheduling updates of selected factor to variable node means in every iteration by combining them with their values from the previous iteration using convergence parameters p and alpha:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    z_f_i rightarrow x_s^(tau)=left(1-q_i sright) cdot z_f_i rightarrow x_s^(tau)+q_i s cdotleft(alpha cdot z_f_x rightarrow x_s^(tau-1)+(1-alpha) cdot z_f_i rightarrow x_a^(tau)right)","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"where q_i s sim operatornameBer(p) in01 is independently sampled with probability p for the mean from factor node f_i to the variable node x_s.","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"The randomised damping parameter pairs lead to a trade-off between the number of non-converging simulations and the rate of convergence. In general, we observe a large number of non-converging simulations for the selection of prob and alpha for which only a small fraction of messages are combined with their values in a previous iteration, and that is a case for prob close to 0 or alpha close to 1.","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"","category":"page"},{"location":"man/theoreticalBelief/#dynamicGBP","page":"Continuous Variables","title":"Dynamic GBP algorithm","text":"","category":"section"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"To recall, each factor node is associated with the observation z_i and variance value v_i. The dynamic framework allows the update of these values in any GBP iteration tau. This framework is an extension to the real-time model that operates continuously and accepts asynchronous data. Such data are continuously integrated into the running instances of the GBP algorithm. Hence, the GBP algorithm can update the state estimate vector in a time-continuous process.","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"Additionally, this framework allows for the artificial addition and removal of factor nodes. Then, the initial factor graph, described with the coefficient matrix, should include all possible factor nodes. Factor nodes that are not active are then taken into account via extremely large values of variances (e.g., 10^60). Consequently, estimates will have a unique solution according to variances whose values are much smaller than 10^60.","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"","category":"page"},{"location":"man/theoreticalBelief/#ageingGBP","page":"Continuous Variables","title":"Ageing GBP algorithm","text":"","category":"section"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"The ageing framework represents an extension of the dynamic model and establishes a model for data arrival processes and for the process of data deterioration or ageing over time (or GBP iterations). We integrate these data regularly into the running instances of the GBP algorithm.","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"Let us assume that factor node f_i receives the new variance v_i. After that moment, the ageing model increases variance value over iterations v_i(tau). More precisely, we associate the Gaussian distribution mathcalN(z_imathcalX_i v_i(tau)) to the corresponding factor node f_i, where the variance v_i(tau) increases its value starting from the predefined variance v_i(tau) = v_i. Finally, in practice ageing model requires defining a limit from above bar v_i of a function v_i(tau), instead of allowing variance to take on extremely large values.","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"Depending on the data arriving dynamic, an adaptive mechanism for increasing the variance over iterations v_i(tau) can be derived. The logarithmic growth model represents a promising solution for systems with a high sampling rate of the data, where a rapid increase in variance is required:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    v_i(tau) = begincases\n      a  textlog left(cfractau + 1 + b1 + b right ) + v_i  1 leq tau leq theta \n      bar v_i  tau geq theta\n  endcases","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"where a and b control the rate of growth.  In contrast, the exponential growth model corresponds to systems with a low sampling rate of the data:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    v_i(tau) = begincases\n      v_i(1+b)^atau  1 leq tau leq theta \n      bar v_i  tau geq theta\n  endcases","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"Finally, the linear growth model can be observed as a compromise between logarithmic and exponential growth models:","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"    v_i(tau) = begincases\n      atau + v_i  1 leq tau leq theta \n      bar v_i  tau geq theta\n  endcases","category":"page"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"","category":"page"},{"location":"man/theoreticalBelief/#refsBelief","page":"Continuous Variables","title":"References","text":"","category":"section"},{"location":"man/theoreticalBelief/","page":"Continuous Variables","title":"Continuous Variables","text":"[1] D. Bickson, Gaussian Belief Propagation: Theory and Aplication, ArXiv e-prints, Nov. 2008.","category":"page"},{"location":"man/discreteInput/#inputdataDiscrete","page":"Input Data","title":"Input Data","text":"","category":"section"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"The FactorGraph package requires knowledge about the joint probability density function g(mathcalX) of the set of random variables mathcalX = x_1dotsx_n that can be factorised proportionally  (propto) to a product of local functions:","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"    g(mathcalX) propto prod_i=1^m psi_i(mathcalX_i)","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"The FactorGraph package supports disrecte random variables, where each random variable x_i having k_i possible states, while local function psi_i(mathcalX_i) is defined as the conditional probability distribution:","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"  p_i(x_imathcalX_i setminus x_i)","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"Hence, the local function is associated with the conditional distribution p_i(x_imathcalX_i setminus x_i) and the set of random variables mathcalX_i. The conditional distribution takes probability values over all possible state combinations of the random variables from the set mathcalX_i forming the conditional probability table. To describe the joint probability density function g(mathcalX), it is enough to define the container that saves indices of discrete variables and conditional probability tables.","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"Thus, the parameters that describe the factor graph structure are represented by variable probability containing indices of random variables, while variable table saves conditional probability tables. The function discreteTreeModel() accepts variables probability and table.","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/discreteInput/#Build-the-graphical-model","page":"Input Data","title":"Build the graphical model","text":"","category":"section"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"Let us observe the following joint probability density function:","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"    g(mathcalX)  propto  p_1(x_1)p_2(x_1x_2)p_3(x_1x_2x_3)","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"where all variables have two states, state 1 and state 2. The conditional probability tables can be written in the compact form:","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"              \nx_1 1 2 1 2 1 2 1 2 1 2 1 2 1 2\nx_2   1 1 2 2 1 1 2 2 1 1 2 2\nx_3       1 1 1 1 2 2 2 2\nprobability 0.6 0.4 0.8 0.5 0.2 0.5 1.0 0.1 0.5 0.6 0.0 0.9 0.5 0.4\n              ","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"Let us define the variable probability that contains discrete variable indices:","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"probability1 = [1]\nprobability2 = [1; 2]\nprobability3 = [1; 2; 3]\n\nprobability = [probability1, probability2, probability3]","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"Further, we define the variable table that holds conditional probability tables:","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"table1 = zeros(2)\ntable1[1] = 0.6; table1[1] = 0.4\n\ntable2 = zeros(2, 2)\ntable2[1, 1] = 0.8; table2[2, 1] = 0.5; table2[1, 2] = 0.2; table2[2, 2] = 0.5\n\ntable3 = zeros(2, 2, 2)\ntable3[1, 1, 1] = 1.0; table3[2, 1, 1] = 0.1; table3[1, 2, 1] = 0.5; table3[2, 2, 1] = 0.6\ntable3[1, 1, 2] = 0.0; table3[2, 1, 2] = 0.9; table3[1, 2, 2] = 0.5; table3[2, 2, 2] = 0.4\n\ntable = [table1, table2, table3]","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"Passing data directly via command-line arguments supports the above format, where the function discreteTreeModel() accepts variables probability and table:","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"bp = discreteTreeModel(probability, table)","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"Here, the variable bp holds the main composite type related to the discrete model. We can also pack the input data as a dictionary and pass to the function discreteTreeModel():","category":"page"},{"location":"man/discreteInput/","page":"Input Data","title":"Input Data","text":"inputData = Dict(\"probability\" => (probability1, probability2, probability3),\n                 \"table\"  => (table1, table2, table3))\n\nbp = discreteTreeModel(inputData)","category":"page"},{"location":"man/utility/#utilityfunction","page":"Utility Functions","title":"Utility Functions","text":"","category":"section"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The FactorGraph provides several utility functions to evaluate and compare obtained results.","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"","category":"page"},{"location":"man/utility/#The-WLS-results","page":"Utility Functions","title":"The WLS results","text":"","category":"section"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The function provides the estimate obtained by the weighted least-squares (WLS) method and root mean square error (RMSE), the mean absolute error (MAE) and the weighted residual sum of squares (WRSS) error metrics evaluated according to the WLS solutions. These results can be used to compare results obtained by the GBP algorithm.","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"exact = wls(gbp)","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The function returns the composite type WeightedLeastSquares with fields estimate, rmse, mae, wrss. Note that results are obtained according to variables ContinuousSystem.coefficient, ContinuousSystem.observation and ContinuousSystem.variance.","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"","category":"page"},{"location":"man/utility/#The-GBP-error-metrics","page":"Utility Functions","title":"The GBP error metrics","text":"","category":"section"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The package provides the function to obtain RMSE, MAE, and WRSS error metrics of the GBP algorithm.","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"evaluation = errorMetric(gbp)","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The function returns the composite type ErrorMetric with fields rmse, mae, wrss. Further, passing the composite type WeightedLeastSquares, we obtained additional fields rmseGBPWLS and maeGBPWLS that determine the distance between the GBP estimate and WLS estimate.","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"evaluation = errorMetric(gbp, exact)","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The function returns the composite type ErrorMetricWiden with fields rmse, mae, wrss, rmseGBPWLS, maeGBPWLS.","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"","category":"page"},{"location":"man/utility/#Error-metrics","page":"Utility Functions","title":"Error metrics","text":"","category":"section"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The root mean square error, the mean absolute error and the weighted residual sum of squares are evaluated according to:","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"  beginaligned\n    textrmse = sqrt cfracsum_i=1^m leftz_i - h_i(hatmathbf x) right^2m quad\n    textmae = cfracsum_i=1^m leftz_i - h_i(hatmathbf x) rightm quad\n    textwrss = sum_i=1^m cfracleftz_i - h_i(hatmathbf x) right^2v_i\n  endaligned","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"where m denotes the number of observations, z_i is observation value, v_i is observation variance, and corresponding equation h_i(hatmathbf x) is evaluated at the point hatmathbf x obtained using the GBP or WLS algorithm. Note, wrss is the value of the objective function of the optimization problem we are solving.","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"&nbsp;","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"Fields rmseGBPWLS and maeGBPWLS determine distance beetwen the GBP estimate hatx_textgbpi and WLS estimate hatx_textwlsi, where root mean square error and mean absolute error are obtained using:","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"  beginaligned\n    textrmse = sqrt cfracsum_i=1^n lefthatx_textwlsi - hatx_textgbpi) right^2n quad\n    textmae = cfracsum_i=1^n lefthatx_textwlsi - hatx_textgbpi) rightn\n  endaligned","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"where n is the number of state variables.","category":"page"},{"location":"man/continuousTreeOutput/#outputTreeContinuous","page":"Output Data","title":"Output Data","text":"","category":"section"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The main inference results are kept in the composite type ContinuousTreeModel in the subtype ContinuousInference with fields:","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"fromFactor,\ntoVariable\nmeanFactorVariable,\nvarianceFactorVariable,\nfromVariable\ntoFactor\nmeanVariableFactor,\nvarianceVariableFactor,\nmean,\nvariance.","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The values of messages from factor nodes to variable nodes can be accessed using meanFactorVariable and varianceFactorVariable fields, while values of messages from variable nodes to factor nodes are stored in meanVariableFactor and varianceVariableFactor fields. These values correspond to edges defined by factor and variable nodes, with indexes preserved in fromFactor - toVariable and fromVariable - toFactor fields.","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"Fields mean and variance define state variable marginal distributions.","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The ContinuousInference field contains the GBP algorithm results. To describe the outputs, we will use the example shown below.","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"using FactorGraph\n\n#     x1   x2   x3   x4\nH = [1.0  0.0  0.0  0.0;  # f1\n    -5.0 -4.0  9.0  0.0;  # f2\n     2.2  0.0  0.0  0.5;  # f3\n     0.0  0.0  1.0  0.0;  # f4\n     0.0  0.0  0.0  0.5]  # f5\n\n#     f1   f2   f3   f4   f5\nz = [0.0; 1.7; 1.9; 0.8; 2.1]\n\n#       f1   f2   f3   f4   f5\nv = [1e-10; 0.1; 0.1; 0.1; 0.1]","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The factor graph construction and message initialization is accomplished using continuousTreeModel() function.","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"gbp = continuousTreeModel(H, z, v)","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/continuousTreeOutput/#Factor-graph-and-root-variable-node","page":"Output Data","title":"Factor graph and root variable node","text":"","category":"section"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The first step in solving/analysing the above system/system of equations is forming a factor graph, where set of variable nodes mathcalX = x_1 x_2 x_3 x_4  is defined by state variables. The set of equations denotes the set of factor nodes mathcalF = f_1 f_2 f_3 f_4 f_5.","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"<img src=\"../../assets/factorgraphtree.png\" class=\"center\"/>\n<figcaption>Figure 1: The tree factor graph with three variable nodes and three factor nodes.</figcaption>\n&nbsp;","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"Additionally, we include the virtual factor node f_v_1, where factor node f_v_1 is singly connected used when the variable node is not directly observed, hence having variance v_x_1 to infty or a priori given mean and variance of state variables. Further, the function continuousTreeModel() sets the first variable node x_1 as the root node.","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"To change defualt values of virtual factor nodes and defualt root variable node use:","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"gbp = continuousTreeModel(H, z, v; mean = 0.1, variance = 1e60, root = 3)","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/continuousTreeOutput/#Messages-initialization","page":"Output Data","title":"Messages initialization","text":"","category":"section"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The initialization step starts with messages from local factor nodes f_1 f_v_1 f_4 f_5  to variable nodes mathcalX.","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/continuousTreeOutput/#Forward-messages-from-the-leaf-nodes-to-the-root-node","page":"Output Data","title":"Forward messages from the leaf nodes to the root node","text":"","category":"section"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The GBP first forward recursion step starts by computing messages from leaf variable nodes x_2 x_4 to the incidence factor nodes f_2 f_3, using incoming messages from factor nodes f_v_1 f_5 .","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"forwardVariableFactor(gbp)\n\njulia> T = gbp.inference\njulia> [T.fromVariable  T.toFactor  T.meanVariableFactor T.varianceVariableFactor]\n5×4 Matrix{Float64}:\n 2.0  2.0  0.1  1.0e60\n 4.0  3.0  4.2  0.4\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The first row defines the message from variable node x_2 to factor node f_2, the second row keeps the message from variable node x_4 to factor node f_3. Zero rows are initialized for messages to be calculated in the next forward and backward steps.","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The second forward recursion step computes the message from factor node f_3 to the variable node x_1, using incoming message from variable node x_4.","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"forwardFactorVariable(gbp)\n\njulia> T = gbp.inference\njulia> [T.fromFactor T.toVariable T.meanFactorVariable T.varianceFactorVariable]\n5×4 Matrix{Float64}:\n 3.0  1.0  -0.0909091  0.0413223\n 0.0  0.0   0.0        0.0\n 0.0  0.0   0.0        0.0\n 0.0  0.0   0.0        0.0\n 0.0  0.0   0.0        0.0","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The first row defines the message from factor node f_3 to variable node x_1. Zero rows are initialized for messages to be calculated in the next forward and backward steps.","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The message passing steps from variable nodes to factor nodes and from factor nodes to variable nodes are then applied recursively until messages have been propagated along every link, and the root node has received messages from all of its neighbours. The FactorGraph keeps flag gbp.graph.forward to signal that moment. Therefore, a complete forward step can be done using:","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"while gbp.graph.forward\n    forwardVariableFactor(gbp)\n    forwardFactorVariable(gbp)\nend","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/continuousTreeOutput/#Backward-messages-from-the-root-node-to-the-leaf-nodes","page":"Output Data","title":"Backward messages from the root node to the leaf nodes","text":"","category":"section"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The GBP first backward recursion step starts by computing message from the root variable node x_3 to the factor node f_2, using incoming message from factor node f_4.","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"backwardVariableFactor(gbp)\n\njulia> T = gbp.inference\njulia> [T.fromVariable  T.toFactor  T.meanVariableFactor T.varianceVariableFactor]\n5×4 Matrix{Float64}:\n 2.0  2.0   0.1      1.0e60\n 4.0  3.0   4.2      0.4\n 1.0  2.0  -2.2e-10  1.0e-10\n 3.0  2.0   0.8      0.1\n 0.0  0.0   0.0      0.0","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The first three rows are obtained using forward steps. The fourth row defines the message from variable node x_3 to factor node f_2.","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The secand backward recursion step computes messages from factor node f_2 to variable nodes x_1 x_2.","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"backwardFactorVariable(gbp)\n\njulia> T = gbp.inference\njulia> [T.fromFactor T.toVariable T.meanFactorVariable T.varianceFactorVariable]\n5×4 Matrix{Float64}:\n 3.0  1.0  -0.0909091  0.0413223\n 2.0  3.0   0.233333   1.97531e59\n 2.0  1.0   1.02       6.4e59\n 2.0  2.0   1.375      0.5125\n 0.0  0.0   0.0        0.0","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The first two rows are obtained using forward steps. The third row defines the message from factor node f_2 to variable node x_1, the fourth row keeps the message from factor node f_2 to variable node x_2.","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"Thus, the backward recursion starts when the root node received messages from all of its neighbours. It can therefore send out messages to all of its neighbours. These in turn will then have received messages from all of their neighbours and so can send out messages along the links going away from the root, and so on. In this way, messages are passed outwards from the root all the way to the leaves. The FactorGraph keeps flag gbp.graph.backward to signal that moment. Therefore, a complete backward step can be done using:","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"while gbp.graph.backward\n    backwardVariableFactor(gbp)\n    backwardFactorVariable(gbp)\nend","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/continuousTreeOutput/#Marginals","page":"Output Data","title":"Marginals","text":"","category":"section"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"The marginal of variable nodes mathcalX can be obtained using messages from factor nodes mathcalF to variable nodes mathcalX. Note that the mean value of marginal is adopted as the estimated value of the state variable. Finally, we obtain:","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"marginal(gbp)\n\njulia> [gbp.inference.mean gbp.inference.variance]\n3×2 Matrix{Float64}:\n -2.2e-10  1.0e-10\n  1.375    0.5125\n  0.8      0.1\n  4.0      0.2","category":"page"},{"location":"man/continuousTreeOutput/","page":"Output Data","title":"Output Data","text":"Where rows correspond with mean and variance values of the state variables x_1 x_2 x_3 x_4 .","category":"page"},{"location":"man/discreteTreeModel/#graphicalTreeModelDiscrete","page":"Graphical Model","title":"Tree Graphical Model","text":"","category":"section"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"The FactorGraph supports the composite type DiscreteTreeModel based on the forward–backward message passing schedule, with three fields:","category":"page"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"DiscreteTreeGraph;\nDiscreteInference;\nDiscreteSystem.","category":"page"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"The subtype DiscreteTreeGraph describes the tree factor graph obtained based on the input data. The BP inference and marginal values are kept in the subtype DiscreteInference. The input system being solved is preserved in the subtype DiscreteSystem. Note that the function discreteTreeModel() returns the main composite type DiscreteTreeModel with all subtypes.","category":"page"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/discreteTreeModel/#Build-graphical-model","page":"Graphical Model","title":"Build graphical model","text":"","category":"section"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"Input arguments of the function discreteTreeModel() describe the tree graphical model, while the function returns DiscreteTreeModel type.","category":"page"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"Loads the system data passing arguments:","category":"page"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"bp = discreteTreeModel(probability, table)","category":"page"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/discreteTreeModel/#Virtual-factor-nodes","page":"Graphical Model","title":"Virtual factor nodes","text":"","category":"section"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"The function discreteTreeModel() receives arguments by keyword to set the message of the virtual factor nodes to initiate messages from leaf variable nodes.  This value is applied to all variable nodes and to all their possible states if the corresponding variable node does not have a singly connected factor node.","category":"page"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"bp = discreteTreeModel(DATA; message = value)","category":"page"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"Default setting of the mean value is message = 1.0.","category":"page"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/discreteTreeModel/#Root-variable-node","page":"Graphical Model","title":"Root variable node","text":"","category":"section"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"The function discreteTreeModel() receives argument by keyword to set the root variable node.","category":"page"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"bp = discreteTreeModel(DATA; root = index)","category":"page"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"Default setting of the root variable node is root = 1.","category":"page"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/discreteTreeModel/#Tree-factor-graph","page":"Graphical Model","title":"Tree factor graph","text":"","category":"section"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"Function checks whether the factor graph has a tree structure.","category":"page"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"tree = isTree(gbp)","category":"page"},{"location":"man/discreteTreeModel/","page":"Graphical Model","title":"Graphical Model","text":"The tree structure of tha factor graph is marked as tree = true, the opposite is tree = false. The function accepts the composite type DiscreteTreeModel.","category":"page"},{"location":"man/continuousModel/#graphicalModelContinuous","page":"Graphical Model","title":"Graphical Model","text":"","category":"section"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"The FactorGraph supports the composite type ContinuousModel based on the synchronous message passing schedule, with three fields:","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"ContinuousGraph;\nContinuousInference;\nContinuousSystem.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"The subtype ContinuousGraph describes the factor graph obtained based on the input data. The GBP inference and marginal values are kept in the subtype ContinuousInference. The system of the linear equations being solved is preserved in the subtype ContinuousSystem. Note that the function continuousModel() returns the main FactorGraph composite type ContinuousModel with all subtypes.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"In addition, we also provide several functions for factor graph manipulation.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/continuousModel/#Build-graphical-model","page":"Graphical Model","title":"Build graphical model","text":"","category":"section"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"Input arguments of the function continuousModel() describe the graphical model, while the function returns ContinuousModel type.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"Loads the system data passing arguments:","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"gbp = continuousModel(coefficient, observation, variances)","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/continuousModel/#Virtual-factor-nodes","page":"Graphical Model","title":"Virtual factor nodes","text":"","category":"section"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"The function continuousModel() receives arguments by keyword to set the mean and variance of the virtual factor nodes. We advise the reader to read the Section initialisation procedure which provides a detailed description of the virtual factor nodes.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"gbp = continuousModel(DATA; mean = value, variance = value)","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"Default setting of the mean value is mean = 0.0, while the default variance is equal to variance = 1e10.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/continuousModel/#Randomized-damping-parametars","page":"Graphical Model","title":"Randomized damping parametars","text":"","category":"section"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"The function continuousModel() receives arguments by keyword to set damping parametars. We advise the reader to read the section the GBP with randomized damping which provides a detailed description of the input parameters.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"gbp = continuousModel(DATA; prob = value, alpha = value)","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"The keyword prob represents the probability of the Bernoulli random variable, independently sampled for each mean value message from a factor node to a variable node, applied for randomised damping iteration scheme with value between 0 and 1. Default setting is set to prob = 0.6. The damped message is evaluated as a linear combination of the message from the previous and the current iteration, with weights alpha = value and 1 - alpha, applied for randomised damping iteration scheme where alpha is between 0 and 1. Default setting is set to alpha = 0.4.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"Using the function continuousModel(), the set of damp messages are fixed through GBP iterations. However, we provide the function that changes damp parameters prob and alpha on the fly:","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"damping!(gbp; prob = value, alpha = value)","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/continuousModel/#Freeze-factor-node,-variable-node-or-edge","page":"Graphical Model","title":"Freeze factor node, variable node or edge","text":"","category":"section"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"The functions freeze target factor or variable node, whereby all messages sent by the factor or variable node retain latest obtained values at the time of freezing.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"freezeFactor!(gbp; factor = index)","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"freezeVariable!(gbp; variable = index)","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"We provide functions that freeze the target edge. More precisely, the function freezes the message from variable node to factor node, or the message from factor node to variable node. Hence, the frozen message keeps the last value obtained at the time of freezing.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"freezeVariableFactor!(gbp; variable = index, factor = index)","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"freezeFactorVariable!(gbp; factor = index, variable = index)","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"The functions accept following parameters: composite type ContinuousModel; the factor node index corresponding to the row index of the coefficient matrix; and the variable node index corresponding to the column index of the coefficient matrix. Note that the singly connected factor nodes can not be frozen because they always send the same message.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/continuousModel/#Defreeze-factor-node,-variable-node-or-edge","page":"Graphical Model","title":"Defreeze factor node, variable node or edge","text":"","category":"section"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"The functions unfreeze the target frozen factor node or frozen variable node, allowing the factor or variable node to calculate outgoing messages.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"defreezeFactor!(gbp; factor = index)","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"defreezeVariable!(gbp; variable = index)","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"Also, we provide functions that unfreeze the target edge, starting the calculation of messages either from variable node to factor node or from factor node to variable node calculates.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"defreezeVariableFactor!(gbp; variable = index, factor = index)","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"defreezeFactorVariable!(gbp; factor = index, variable = index)","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"The functions accept following parameters: composite type ContinuousModel; the factor node index corresponding to the row index of the coefficient matrix; and the variable node index corresponding to the column index of the coefficient matrix. Since singly connected factors cannot be frozen, they cannot be unfreezed.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/continuousModel/#Hide-factor-node","page":"Graphical Model","title":"Hide factor node","text":"","category":"section"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"Utilising a hiding mechanism, the function softly deletes factor node. Hence, the function obliterates the target factor node from the graph during the calculation. Soft delete actually removes the node, while preserving node numbering and keeping the same dimensions of the internal variables.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"hideFactor!(gbp; factor = index)","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"If the function targets the singly connected factor node, the function obliterates the target factor only if there are two or more singly connected factor nodes at the same variable node. If there is only one singly connected factor node at the variable node, the function transforms the target factor node to the virtual factor node. Note that to maintain consistency, the function also affects ContinuousSystem.observation, ContinuousSystem.coefficient and ContinuousSystem.coefficientTranspose fields by setting non-zero elements to zero.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/continuousModel/#Add-factor-nodes","page":"Graphical Model","title":"Add factor nodes","text":"","category":"section"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"The function adds new factor nodes to the existing factor graph.","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"addFactors!(gbp; coefficient = matrix, observation = vector, variance = vector)","category":"page"},{"location":"man/continuousModel/","page":"Graphical Model","title":"Graphical Model","text":"The function supports addition of the multiple factor nodes to initial (existing) formation of the factor graph using the same input data format. The function accepts the following parameters: composite type ContinuousModel; the observation and variance vectors representing new observation values and variances, respectively. The keyword coefficient with corresponding coefficients defines the set of equations describing new factor nodes. Also, function initializes messages from variable nodes to a new factor node using results from the last GBP iteration. Note that the function also affects ContinuousSystem.observation, ContinuousSystem.variance, ContinuousSystem.coefficient and ContinuousSystem.coefficientTranspose fields.","category":"page"},{"location":"man/discreteTreeInference/#inferenceTreeDiscrete","page":"Inference","title":"Inference","text":"","category":"section"},{"location":"man/discreteTreeInference/","page":"Inference","title":"Inference","text":"To exchange information over the tree factor graph, the FactorGraph provides forward–backward BP algorithm. We advise the reader to read the Section forward–backward message passing schedule which provides a detailed description of the inference algorithm.","category":"page"},{"location":"man/discreteTreeInference/","page":"Inference","title":"Inference","text":"Each of the inference functions accepts only the composite type DiscreteTreeModel, i.e., an output variable of the function bp = discreteTreeModel().","category":"page"},{"location":"man/discreteTreeInference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/discreteTreeInference/#Forward-inference","page":"Inference","title":"Forward inference","text":"","category":"section"},{"location":"man/discreteTreeInference/","page":"Inference","title":"Inference","text":"The set of functions that can be used to preform forward message inference:","category":"page"},{"location":"man/discreteTreeInference/","page":"Inference","title":"Inference","text":"forwardVariableFactor(bp)\nforwardFactorVariable(bp)","category":"page"},{"location":"man/discreteTreeInference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/discreteTreeInference/#Backward-inference","page":"Inference","title":"Backward inference","text":"","category":"section"},{"location":"man/discreteTreeInference/","page":"Inference","title":"Inference","text":"The set of functions that can be used to preform backward message inference:","category":"page"},{"location":"man/discreteTreeInference/","page":"Inference","title":"Inference","text":"backwardVariableFactor(bp)\nbackwardFactorVariable(bp)","category":"page"},{"location":"man/discreteTreeInference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/discreteTreeInference/#Marginal-inference","page":"Inference","title":"Marginal inference","text":"","category":"section"},{"location":"man/discreteTreeInference/","page":"Inference","title":"Inference","text":"To compute normalized marginals the FactorGraph provides the function:","category":"page"},{"location":"man/discreteTreeInference/","page":"Inference","title":"Inference","text":"marginal(bp)","category":"page"},{"location":"man/discreteTreeInference/","page":"Inference","title":"Inference","text":"To compute unnormalized marginals the FactorGraph provides the function:","category":"page"},{"location":"man/discreteTreeInference/","page":"Inference","title":"Inference","text":"marginalUnnormalized(bp)","category":"page"},{"location":"man/discreteTreeInference/","page":"Inference","title":"Inference","text":"Same as before, functions accept the composite type DiscreteTreeModel.","category":"page"},{"location":"man/continuousInput/#inputdataContinuous","page":"Input Data","title":"Input Data","text":"","category":"section"},{"location":"man/continuousInput/","page":"Input Data","title":"Input Data","text":"The FactorGraph package requires knowledge about the joint probability density function g(mathcalX) of the set of random variables mathcalX = x_1dotsx_n that can be factorised proportionally  (propto) to a product of local functions:","category":"page"},{"location":"man/continuousInput/","page":"Input Data","title":"Input Data","text":"    g(mathcalX) propto prod_i=1^m psi_i(mathcalX_i)","category":"page"},{"location":"man/continuousInput/","page":"Input Data","title":"Input Data","text":"The FactorGraph package supports continuous random variables, where local function psi_i(mathcalX_i) is defined as a continuous Gaussian distribution:","category":"page"},{"location":"man/continuousInput/","page":"Input Data","title":"Input Data","text":"  mathcalN(z_imathcalX_iv_i) propto expBigg-cfracz_i-h_i(mathcalX_i)^22v_iBigg","category":"page"},{"location":"man/continuousInput/","page":"Input Data","title":"Input Data","text":"Hence, the local function is associated with observation z_i, variance  v_i, and linear equation h_i(mathcalX_i). To describe the joint probability density function g(mathcalX), it is enough to define the coefficient matrix containing coefficients of the equations, and vectors of observation and variance values. Then, the i-th row of the coefficient matrix, with consistent entries of observation and variance, defines the local function psi_i(mathcalX_i).","category":"page"},{"location":"man/continuousInput/","page":"Input Data","title":"Input Data","text":"Thus, the input data structure includes the coefficient variable which describes coefficients of the equations, while variables observation and variance represent observation and variance vectors, respectively. The functions continuousModel() and continuousTreeModel() accept variables coefficient, observation and variance.","category":"page"},{"location":"man/continuousInput/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/continuousInput/#Build-the-graphical-model","page":"Input Data","title":"Build the graphical model","text":"","category":"section"},{"location":"man/continuousInput/","page":"Input Data","title":"Input Data","text":"Let us observe the following joint probability density function:","category":"page"},{"location":"man/continuousInput/","page":"Input Data","title":"Input Data","text":"    g(mathcalX) propto  expBigg-cfrac25 - 02x_1^22cdot 11BiggexpBigg-cfrac06 - (21x_1 + 34x_2)^22cdot 35Bigg","category":"page"},{"location":"man/continuousInput/","page":"Input Data","title":"Input Data","text":"We can describe the joint probability density function using variables coefficient, observation and variance. Passing data directly via command-line support the following format, where the coefficient matrix can be defined as a full or sparse matrix:","category":"page"},{"location":"man/continuousInput/","page":"Input Data","title":"Input Data","text":"coefficient = zeros(2, 2)\ncoefficient[1, 1] = 0.2; coefficient[2, 1] = 2.1; coefficient[2, 2] = 3.4\nobservation = [2.5; 0.6]\nvariance = [1.1; 3.5]\n\ngbp = continuousModel(coefficient, observation, variance)","category":"page"},{"location":"man/continuousInput/","page":"Input Data","title":"Input Data","text":"Here, the variable gbp holds the main composite type related to the continuous model. In the case of a tree factor graph, when you want to use a forward-backward GBP algorithm, then the following command can be used:","category":"page"},{"location":"man/continuousInput/","page":"Input Data","title":"Input Data","text":"gbp = continuousTreeModel(coefficient, observation, variance)","category":"page"},{"location":"man/continuousTreeInference/#inferenceTreeContinuous","page":"Inference","title":"Inference","text":"","category":"section"},{"location":"man/continuousTreeInference/","page":"Inference","title":"Inference","text":"To exchange information over the tree factor graph, the FactorGraph provides forward–backward GBP algorithm. We advise the reader to read the Section forward–backward message passing schedule which provides a detailed description of the inference algorithm.","category":"page"},{"location":"man/continuousTreeInference/","page":"Inference","title":"Inference","text":"Each of the inference functions accepts only the composite type ContinuousTreeModel, i.e., an output variable of the function gbp = continuousTreeModel().","category":"page"},{"location":"man/continuousTreeInference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/continuousTreeInference/#Forward-inference","page":"Inference","title":"Forward inference","text":"","category":"section"},{"location":"man/continuousTreeInference/","page":"Inference","title":"Inference","text":"The set of functions that can be used to preform forward message inference:","category":"page"},{"location":"man/continuousTreeInference/","page":"Inference","title":"Inference","text":"forwardVariableFactor(gbp)\nforwardFactorVariable(gbp)","category":"page"},{"location":"man/continuousTreeInference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/continuousTreeInference/#Backward-inference","page":"Inference","title":"Backward inference","text":"","category":"section"},{"location":"man/continuousTreeInference/","page":"Inference","title":"Inference","text":"The set of functions that can be used to preform backward message inference:","category":"page"},{"location":"man/continuousTreeInference/","page":"Inference","title":"Inference","text":"backwardVariableFactor(gbp)\nbackwardFactorVariable(gbp)","category":"page"},{"location":"man/continuousTreeInference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/continuousTreeInference/#Marginal-inference","page":"Inference","title":"Marginal inference","text":"","category":"section"},{"location":"man/continuousTreeInference/","page":"Inference","title":"Inference","text":"To compute marginals the FactorGraph provides the function:","category":"page"},{"location":"man/continuousTreeInference/","page":"Inference","title":"Inference","text":"marginal(gbp)","category":"page"},{"location":"man/continuousTreeInference/","page":"Inference","title":"Inference","text":"Same as before, the function accepts the composite type ContinuousTreeModel.","category":"page"},{"location":"man/theoreticalInference/#inferenceFactorGraphs","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"","category":"section"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"In the standard setup, we observe the set of random variables mathcalX = x_1dotsx_n described via the joint probability density function g(mathcalX). Assuming the function g(mathcalX) can be factorised proportionally (propto) to a product of local functions:","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"    g(mathcalX) propto prod_i=1^m psi(mathcalX_i)","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"where mathcalX_i subseteq mathcalX. The first step is forming a factor graph, which is a bipartite graph that describes the structure of the factorisation. Factor graph allows a graph-based representation of probability density functions using variable and factor nodes connected by edges. In contrast to directed and undirected graphical models, factor graphs provide the details of the factorisation more explicitly. The factor graph structure comprises the set of factor nodes mathcalF=f_1dotsf_m, where each factor node  f_i represents local function psi(mathcalX_i), and the set of variable nodes mathcalX. The factor node f_i connects to the variable node x_s if and only if x_s in mathcalX_i.","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"The message passing algorithm on factor graphs proceeds by passing two types of messages along the edges of the factor graph:","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"a variable node x_s in mathcalX to a factor node f_i in mathcalF message mu_x_s to f_i(x_s), and\na factor node f_i in mathcalF to a variable node x_s in mathcalX message mu_f_i to x_s(x_s).","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"Both variable and factor nodes in a factor graph process the incoming messages and calculate outgoing messages, where an output message on any edge depends on incoming messages from all other edges. The messages represent \"beliefs\" about variable nodes, thus a message that arrives or departs a certain variable node is a function (distribution) of the random variable corresponding to the variable node.","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"Here we shall focus on the problem of evaluating local marginals over variable nodes mathcalX = x_1dotsx_n described via the joint probability density function g(mathcalX), which will lead us to the belief propagation (BP) algorithm, also known as the sum-product algorithm [1].","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"","category":"page"},{"location":"man/theoreticalInference/#variableFactorMessage","page":"Inference in Factor Graphs","title":"Message from a variable node to a factor node","text":"","category":"section"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"Consider a part of a factor graph with a group of factor nodes mathcalF_s=f_if_wf_W subseteq mathcalF that are neighbours of the variable node x_s in mathcalX. The message mu_x_s to f_i(x_s) from the variable node x_s to the factor node f_i is equal to the product of all incoming factor node to variable node messages arriving at all the other incident edges:","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"    mu_x_s to f_i(x_s) =prod_f_a in mathcalF_s setminus f_i mu_f_a to x_s(x_s)","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"where mathcalF_s setminus f_i represents the set of factor nodes incident to the variable node x_s, excluding the factor node f_i. Note that each message is a function of the variable x_s.","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"","category":"page"},{"location":"man/theoreticalInference/#factorVariableMessage","page":"Inference in Factor Graphs","title":"Message from a factor node to a variable node","text":"","category":"section"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"Consider a part of a factor graph that consists of a group of variable nodes mathcalX_i = x_s x_lx_L subseteq mathcal X that are neighbours of the factor node f_i in mathcalF. The message mu_f_i to x_s(x_s) from the factor node f_i to the variable node x_s is defined as a product of all incoming variable node to factor node messages arriving at other incident edges, multiplied by the function psi_i(mathcalX_i) associated to the factor node f_i, and marginalised over all of the variables associated with the incoming messages:","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"    mu_f_i to x_s(x_s)= sumlimits_x_ldotssumlimits_x_L psi_i(mathcalX_i)\n    prod_x_b in mathcalX_isetminus x_s mu_x_b to f_i(x_b)","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"where mathcalX_isetminus x_s is the set of variable nodes incident to the factor node f_i, excluding the variable node x_s. For continuous variables the summations are simply replaced by integrations:","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"    mu_f_i to x_s(x_s)= intlimits_x_ldotsintlimits_x_L psi_i(mathcalX_i)\n    prod_x_b in mathcalX_isetminus x_s bigmu_x_b to f_i(x_b) cdot mathrmdx_bbig","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"","category":"page"},{"location":"man/theoreticalInference/#marginal","page":"Inference in Factor Graphs","title":"Marginal inference","text":"","category":"section"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"The marginal of the variable node x_s is obtained as the product of all incoming messages into the variable node x_s:","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"    p(x_s) =prod_f_c in mathcalF_s mu_f_c to x_s(x_s)","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"where mathcalF_s is the set of factor nodes incident to the variable node x_s.","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"","category":"page"},{"location":"man/theoreticalInference/#MessagePassingSchedule","page":"Inference in Factor Graphs","title":"Message passing schedule","text":"","category":"section"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"The GBP or BP is an iterative algorithm, and requires a message-passing schedule. Typically, the message updating procedure can be implemented using:","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"synchronous schedule, or\nforward-backward schedule.","category":"page"},{"location":"man/theoreticalInference/#synchronousSchedule","page":"Inference in Factor Graphs","title":"Synchronous message passing schedule","text":"","category":"section"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"The scheduling where messages from variable to factor nodes, and messages from factor nodes to variable nodes, are updated in parallel in respective half-iterations, is known as synchronous scheduling. Synchronous scheduling updates all messages in a given iteration using the output of the previous iteration as an input. The synchronous scheduling allows inference for an arbitrary factor graph structure.","category":"page"},{"location":"man/theoreticalInference/#treeSchedule","page":"Inference in Factor Graphs","title":"Forward-backward message passing schedule","text":"","category":"section"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"The forward–backward schedule allows exact inference in tree factor graph. We start by viewing an arbitrary variable node as the root of the factor graph and initiating messages at the leaves of the tree factor graph using. The message passing steps from variable nodes to factor nodes and from factor nodes to variable nodes are then applied recursively until messages have been propagated along every link, and the root node has received messages from all of its neighbours. Each node can send a message towards the root once it has received messages from all of its other neighbours. This step is known as the forward recursion.","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"The backward recursion starts when the root node received messages from all of its neighbours. It can therefore send out messages to all of its neighbours. These in turn will then have received messages from all of their neighbours and so can send out messages along the links going away from the root, and so on. In this way, messages are passed outwards from the root all the way to the leaves.","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"By now, a message will have passed in both directions across every link in the graph, and every node will have received a message from all of its neighbours. Every variable node will have received messages from all of its neighbours, we can readily calculate the marginal distribution for every variable in the graph. The number of messages that have to be computed is given by twice the number of links in the graph and so involves only twice the computation involved in finding a single marginal [1].","category":"page"},{"location":"man/theoreticalInference/#initialisationProcedure","page":"Inference in Factor Graphs","title":"Initialisation procedure","text":"","category":"section"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"The Initialisation step starts with messages from singly connected factor nodes to variable nodes. Then, variable nodes forward the incoming messages received from singly connected factor nodes along remaining edges. To ensure this, we are using virtual factor nodes. Hence, the virtual factor node is a singly connected factor node used if the variable node is not directly observed. In a typical scenario, without prior knowledge, the variance of virtual factor nodes tend to infinity for continuous variables. Then, we also improve convergence performance using virtual factor nodes. For discrete variables messages from virtual factor nodes are set to unity.","category":"page"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"","category":"page"},{"location":"man/theoreticalInference/#refs","page":"Inference in Factor Graphs","title":"References","text":"","category":"section"},{"location":"man/theoreticalInference/","page":"Inference in Factor Graphs","title":"Inference in Factor Graphs","text":"[1] C. M. Bishop, Pattern Recognition and Machine Learning (Information Science and Statistics). Berlin, Heidelberg: Springer-Verlag, 2006.","category":"page"},{"location":"man/continuousInference/#inferenceContinuous","page":"Inference","title":"Inference","text":"","category":"section"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"We advise the reader to read the Section continuous Gaussian random variables which provides a detailed description of the inference algorithms. To exchange information over the factor graph, the FactorGraph provides three inference approaches:","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"vanilla GBP algorithm;\nbroadcast GBP algorithm;\nbroadcast GBP with Kahan–Babuška algorithm.","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"Each of the inference functions accepts only the composite type ContinuousModel, i.e., an output variable of the function gbp = continuousModel() and applies the synchronous message passing schedule.","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/continuousInference/#Message-inference","page":"Inference","title":"Message inference","text":"","category":"section"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"The set of functions that can be used to preform message inference:","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"messageFactorVariable(gbp); messageVariableFactor(gbp)\nmessageFactorVariableBroadcast(gbp); messageVariableFactorBroadcast(gbp)\nmessageFactorVariableKahan(gbp); messageVariableFactorKahan(gbp)","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/continuousInference/#Mean-inference","page":"Inference","title":"Mean inference","text":"","category":"section"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"The set of functions that can be used to preform only mean inference:","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"meanFactorVariable(gbp); meanVariableFactor(gbp)\nmeanFactorVariableBroadcast(gbp); meanVariableFactorBroadcast(gbp)\nmeanFactorVariableKahan(gbp); meanVariableFactorKahan(gbp)","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/continuousInference/#Variance-inference","page":"Inference","title":"Variance inference","text":"","category":"section"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"The set of functions that can be used to preform only variance inference:","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"varianceFactorVariable(gbp); varianceVariableFactor(gbp)\nvarianceFactorVariableBroadcast(gbp); varianceVariableFactorBroadcast(gbp)\nvarianceFactorVariableKahan(gbp); varianceVariableFactorKahan(gbp)","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/continuousInference/#Randomised-damping-inference","page":"Inference","title":"Randomised damping inference","text":"","category":"section"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"Additionaly, we provide the set of functions to preform damping inference:","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"messageDampFactorVariable(gbp); meanDampFactorVariable(gbp)\nmessageDampFactorVariableBroadcast(gbp); meanDampFactorVariableBroadcast(gbp)\nmessageDampFactorVariableKahan(gbp); meanDampFactorVariableKahan(gbp)","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/continuousInference/#Marginal-inference","page":"Inference","title":"Marginal inference","text":"","category":"section"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"To compute marginals the FactorGraph provides the function:","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"marginal(gbp)","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"Same as before, the function accepts only the composite type ContinuousModel.","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/continuousInference/#Dynamic-inference","page":"Inference","title":"Dynamic inference","text":"","category":"section"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"This framework is an extension to the real-time model that operates continuously and accepts asynchronous observation and variance values. More precisely, in each GBP iteration user can change the observation and variance values of the corresponding factor nodes and continue the GBP iteration process. We advise the reader to read the section dynamic GBP algorithm which provides a detailed description of the input parameters.","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"dynamicFactor!(gbp; factor = index, observation = value, variance = value)","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"The function accepts the composite type ContinuousModel and keywords factor, observation and variance, which defines the dynamic update scheme of the factor nodes. The factor node index corresponding to the row index of the coefficient matrix. Note that during each function call, ContinuousSystem.observation and ContinuousSystem.variance fields also change values according to the scheme.","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/continuousInference/#Dynamic-inference-with-variance-ageing","page":"Inference","title":"Dynamic inference with variance ageing","text":"","category":"section"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"The ageing framework represents an extension of the dynamic model and establishes a model for data arrival processes and for the process of data deterioration or ageing over time (or GBP iterations). We integrate these data regularly into the running instances of the GBP algorithm. We advise the reader to read the section ageing GBP algorithm which provides a detailed description of the input parameters.","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"ageingVariance!(gbp; factor = index, initial = value, limit = value,\n                model = value, a = value, b = value, tau = value)","category":"page"},{"location":"man/continuousInference/","page":"Inference","title":"Inference","text":"This function should be integrated into the iteration loop to ensure variance ageing over iterations. The function accepts the composite type ContinuousModel and the keywords factor, initial, limit, model, a, b and tau. The variance growth model can be linear model = 1, logarithmic model = 2 and exponential model = 3, where parameters a and b control the rate of the growth. The initial defines the initial value of the variance, while the variance upper limit value is defined according to limit. The ageing model increases the value of variance over iterations, thus the current iteration step should be forwarded using tau keyword. Also, during each function call, ContinuousSystem.variance field changes values according to the ageing model.","category":"page"},{"location":"man/discreteTreeOutput/#outputTreeDiscrete","page":"Output Data","title":"Output Data","text":"","category":"section"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The main inference results are kept in the composite type DiscreteTreeModel in the subtype DiscreteInference with fields:","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"fromFactor,\ntoVariable\nmessageFactorVariable,\nfromVariable\ntoFactor\nmessageVariableFactor,\nmarginal.","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The values of messages from factor nodes to variable nodes can be accessed using messageFactorVariable field, while values of messages from variable nodes to factor nodes are stored in messageVariableFactor field. These values correspond to edges defined by factor and variable nodes, with indexes preserved in fromFactor - toVariable and fromVariable - toFactor fields.","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"Fields marginal holds state variable normalized marginal distributions.","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The DiscreteInference field contains the BP algorithm results. To describe the outputs, we will use the example with four random variables, where random variables x_1 x_2 x_3 x_4  have possible states  4 3 1 2.","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"using FactorGraph\n\nprobability1 = [1]\ntable1 = [0.2; 0.3; 0.4; 0.1]\n\nprobability2 = [1; 2; 3]\ntable2 = zeros(4, 3, 1)\ntable2[1, 1, 1] = 0.2; table2[2, 1, 1] = 0.5; table2[3, 1, 1] = 0.3; table2[4, 1, 1] = 0.0\ntable2[1, 2, 1] = 0.1; table2[2, 2, 1] = 0.1; table2[3, 2, 1] = 0.7; table2[4, 2, 1] = 0.1\ntable2[1, 3, 1] = 0.5; table2[2, 3, 1] = 0.2; table2[3, 3, 1] = 0.1; table2[4, 3, 1] = 0.1\n\nprobability3 = [4; 2]\ntable3 = zeros(2, 3)\ntable3[1, 1] = 0.2; table3[2, 1] = 0.8\ntable3[1, 2] = 0.5; table3[2, 2] = 0.5\ntable3[1, 3] = 0.5; table3[2, 3] = 0.5\n\nprobability4 = [4]\ntable4 = [0.4; 0.6]","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The factor graph construction and message initialization is accomplished using discreteTreeModel() function.","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"probability = [probability1, probability2, probability3, probability4]\ntable = [table1, table2, table3, table4]\n\nbp = discreteTreeModel(probability, table)","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/discreteTreeOutput/#Factor-graph-and-root-variable-node","page":"Output Data","title":"Factor graph and root variable node","text":"","category":"section"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The first step in solving/analysing the above system/system of equations is forming a factor graph, where set of variable nodes mathcalX = x_1 x_2 x_3 x_4  is defined by discrete random variables. The set of conditional probability tables denotes the set of factor nodes mathcalF = f_1 f_2 f_3 f_4. Here, we leave the default setting for the root factor, or the function discreteTreeModel() sets the first variable node x_1 as the root node.","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"<img src=\"../../assets/factorgraphtreediscrete.png\" class=\"center\"/>\n<figcaption>Figure 1: The tree factor graph with three variable nodes and three factor nodes.</figcaption>\n&nbsp;","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"Additionally, we include the virtual factor node f_v_1, to initiate messages from leaf variable node. The function discreteTreeModel() sets all-ones table of the virtual factor node.","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/discreteTreeOutput/#Messages-initialization","page":"Output Data","title":"Messages initialization","text":"","category":"section"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The initialization step starts with messages from leaf factor nodes f_1 f_v_1 f_4 to variable nodes mathcalX.","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/discreteTreeOutput/#Forward-messages-from-the-leaf-nodes-to-the-root-node","page":"Output Data","title":"Forward messages from the leaf nodes to the root node","text":"","category":"section"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The BP first forward recursion step starts by computing messages from leaf variable nodes x_3 x_4 to the incidence factor nodes f_2 f_3, using incoming messages from factor nodes f_v_1 f_4 .","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"forwardVariableFactor(bp)\n\njulia> T = bp.inference\njulia> [T.fromVariable  T.toFactor  T.messageVariableFactor]\n5×3 Matrix{Any}:\n 3  2  [1.0]\n 4  3  [0.4, 0.6]\n 0  0  Float64[]\n 0  0  Float64[]\n 0  0  Float64[]","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The first row defines the message from variable node x_3 to factor node f_2, the second row keeps the message from variable node x_4 to factor node f_3. The rest of the rows are initialized for messages to be calculated in the next forward and backward steps.","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The second forward recursion step computes the message from factor node f_3 to the variable node x_2, using incoming message from variable node x_4.","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"forwardFactorVariable(bp)\n\njulia> T = bp.inference\njulia> [T.fromFactor T.toVariable T.messageFactorVariable]\n5×3 Matrix{Any}:\n 3  2  [0.56, 0.5, 0.5]\n 0  0  Float64[]\n 0  0  Float64[]\n 0  0  Float64[]\n 0  0  Float64[]","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The first row defines the message from factor node f_3 to variable node x_2. The rest of the rows are initialized for messages to be calculated in the next forward and backward steps.","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The message passing steps from variable nodes to factor nodes and from factor nodes to variable nodes are then applied recursively until messages have been propagated along every link, and the root node has received messages from all of its neighbours. The FactorGraph keeps flag bp.graph.forward to signal that moment. Therefore, a complete forward step can be done using:","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"while bp.graph.forward\n    forwardVariableFactor(bp)\n    forwardFactorVariable(bp)\nend","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/discreteTreeOutput/#Backward-messages-from-the-root-node-to-the-leaf-nodes","page":"Output Data","title":"Backward messages from the root node to the leaf nodes","text":"","category":"section"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The BP first backward recursion step starts by computing message from the root variable node x_1 to the factor node f_2, using incoming message from factor node f_1.","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"backwardVariableFactor(bp)\n\njulia> T = bp.inference\njulia> [T.fromVariable  T.toFactor  T.messageVariableFactor]\n5×3 Matrix{Any}:\n 3  2  [1.0]\n 4  3  [0.4, 0.6]\n 2  2  [0.56, 0.5, 0.5]\n 1  2  [0.2, 0.3, 0.4, 0.1]\n 0  0  Float64[]","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The first three rows are obtained using forward steps. The fourth row defines the message from variable node x_1 to factor node f_2.","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The secand backward recursion step computes messages from factor node f_2 to variable nodes x_2 x_3.","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"backwardFactorVariable(bp)\n\njulia> T = bp.inference\njulia> [T.fromFactor T.toVariable T.messageFactorVariable]\n5×3 Matrix{Any}:\n 3  2  [0.56, 0.5, 0.5]\n 2  1  [0.412, 0.43, 0.568, 0.1]\n 2  2  [0.31, 0.34, 0.21]\n 2  3  [0.4486]\n 0  0  Float64[]","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The first two rows are obtained using forward steps. The third row defines the message from factor node f_2 to variable node x_2, the fourth row keeps the message from factor node f_2 to variable node x_3.","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"Thus, the backward recursion starts when the root node received messages from all of its neighbours. It can therefore send out messages to all of its neighbours. These in turn will then have received messages from all of their neighbours and so can send out messages along the links going away from the root, and so on. In this way, messages are passed outwards from the root all the way to the leaves. The FactorGraph keeps flag gbp.graph.backward to signal that moment. Therefore, a complete backward step can be done using:","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"while bp.graph.backward\n    backwardVariableFactor(bp)\n    backwardFactorVariable(bp)\nend","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/discreteTreeOutput/#Marginals","page":"Output Data","title":"Marginals","text":"","category":"section"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"The normalized marginals of variable nodes mathcalX can be obtained using messages from factor nodes mathcalF to variable nodes mathcalX. Finally, we obtain:","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"marginal(bp)\n\njulia> bp.inference.marginal\n4-element Vector{Vector{Float64}}:\n [0.18368256798930002, 0.287561301827909, 0.5064645563976816, 0.022291573785109226]\n [0.38698172090949623, 0.37895675434685683, 0.23406152474364691]\n [1.0]\n [0.3004904146232724, 0.6995095853767277]","category":"page"},{"location":"man/discreteTreeOutput/","page":"Output Data","title":"Output Data","text":"Where rows correspond normalized marginal distributions of the state variables x_1 x_2 x_3 x_4 .","category":"page"},{"location":"#FactorGraph","page":"Home","title":"FactorGraph","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The FactorGraph package provides the set of different functions to perform inference over the factor graph with continuous or discrete random variables using the belief propagation (BP) algorithm, also known as the sum-product algorithm.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Continuous-framework","page":"Home","title":"Continuous framework","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In the case of continuous random variables described by Gaussian distributions, we are using the linear Gaussian belief propagation (GBP) algorithm to solve the inference problem. The linear GBP model requires the set of linear equations and provides the minimum mean squared error (MMSE) estimate of the state variables. To perform inference the FactorGraph package uses several algorithms based on the synchronous message passing schedule:","category":"page"},{"location":"","page":"Home","title":"Home","text":"vanilla GBP algorithm;\nbroadcast GBP algorithm;\nbroadcast GBP with Kahan–Babuška algorithm.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Within these algorithms, the packege provides several routines to allow dynamic GBP framework:","category":"page"},{"location":"","page":"Home","title":"Home","text":"dynamic GBP algorithm;\nageing GBP algorithm.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Finally, the package also includes a message passing algorithm that allows inference in the tree factor graph:","category":"page"},{"location":"","page":"Home","title":"Home","text":"forward–backward GBP algorithm.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Discrete-framework","page":"Home","title":"Discrete framework","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In the case of discrete random variables the package currently provides only the BP algorithm that allows exact inference in the tree factor graph:","category":"page"},{"location":"","page":"Home","title":"Home","text":"forward–backward BP algorithm.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Requirement","page":"Home","title":"Requirement","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"FactorGraph requires Julia 1.6 and higher.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the FactorGraph package, run the following command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add FactorGraph","category":"page"},{"location":"","page":"Home","title":"Home","text":"To use FactorGraph package, add the following code to your script, or alternatively run the same command in Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using FactorGraph","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Quick-start-whitin-continuous-framework","page":"Home","title":"Quick start whitin continuous framework","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The following examples are intended for a quick introduction to FactorGraph package within the continuous framework.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The broadcast GBP algorithm:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using FactorGraph\n\nH = [1.0 0.0 0.0; 1.5 0.0 2.0; 0.0 3.1 4.6] # coefficient matrix\nz = [0.5; 0.8; 4.1]                         # observation vector\nv = [0.1; 1.0; 1.0]                         # variance vector\n\ngbp = continuousModel(H, z, v)              # initialize the graphical model\nfor iteration = 1:50                        # the GBP inference\n    messageFactorVariableBroadcast(gbp)     # compute messages using the broadcast GBP\n    messageVariableFactorBroadcast(gbp)     # compute messages using the broadcast GBP\nend\nmarginal(gbp)                               # compute marginals","category":"page"},{"location":"","page":"Home","title":"Home","text":"The vanilla GBP algorithm in the dynamic framework:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using FactorGraph\n\nH = [1.0 0.0 0.0; 1.5 0.0 2.0; 0.0 3.1 4.6] # coefficient matrix\nz = [0.5; 0.8; 4.1]                         # observation vector\nv = [0.1; 1.0; 1.0]                         # variance vector\n\ngbp = continuousModel(H, z, v)              # initialize the graphical model\nfor iteration = 1:200                       # the GBP inference\n    messageFactorVariable(gbp)              # compute messages using the vanilla GBP\n    messageVariableFactor(gbp)              # compute messages using the vanilla GBP\nend\n\ndynamicFactor!(gbp;                         # integrate changes in the running GBP\n    factor = 1,\n    observation = 0.85,\n    variance = 1e-10)\nfor iteration = 201:400                     # continues the GBP inference\n    messageFactorVariable(gbp)              # compute messages using the vanilla GBP\n    messageVariableFactor(gbp)              # compute messages using the vanilla GBP\nend\nmarginal(gbp)                               # compute marginals","category":"page"},{"location":"","page":"Home","title":"Home","text":"The vanilla GBP algorithm in the dynamic ageing framework:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using FactorGraph\n\nH = [1.0 0.0 0.0; 1.5 0.0 2.0; 0.0 3.1 4.6] # coefficient matrix\nz = [0.5; 0.8; 4.1]                         # observation vector\nv = [0.1; 1.0; 1.0]                         # variance vector\n\ngbp = continuousModel(H, z, v)              # initialize the graphical model\nfor iteration = 1:200                       # the GBP inference\n    messageFactorVariable(gbp)              # compute messages using the vanilla GBP\n    messageVariableFactor(gbp)              # compute messages using the vanilla GBP\nend\n\nfor iteration = 1:400                       # continues the GBP inference\n    ageingVariance!(gbp;                    # integrate changes in the running GBP\n        factor = 3,\n        initial = 1,\n        limit = 50,\n        model = 1,\n        a = 0.05,\n        tau = iteration)\n    messageFactorVariable(gbp)              # compute messages using the vanilla GBP\n    messageVariableFactor(gbp)              # compute messages using the vanilla GBP\nend\nmarginal(gbp)                               # compute marginals","category":"page"},{"location":"","page":"Home","title":"Home","text":"The forward–backward GBP algorithm over the tree factor graph:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using FactorGraph\n\nH = [1 0 0 0 0; 6 8 2 0 0; 0 5 0 0 0;       # coefficient matrix\n     0 0 2 0 0; 0 0 3 8 2]\nz = [1; 2; 3; 4; 5]                         # observation vector\nv = [3; 4; 2; 5; 1]                         # variance vector\n\ngbp = continuousTreeModel(H, z, v)          # initialize the tree graphical model\nwhile gbp.graph.forward                     # inference from leaves to the root\n     forwardVariableFactor(gbp)             # compute forward messages\n     forwardFactorVariable(gbp)             # compute forward messages\nend\nwhile gbp.graph.backward                    # inference from the root to leaves\n     backwardVariableFactor(gbp)            # compute backward messages\n     backwardFactorVariable(gbp)            # compute backward messages\nend\nmarginal(gbp)                               # compute marginals","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Quick-start-whitin-discrete-framework","page":"Home","title":"Quick start whitin discrete framework","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Following example is intended for a quick introduction to FactorGraph package within the discrete framework.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The forward–backward BP algorithm over the tree factor graph:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using FactorGraph\n\nprobability1 = [1]\ntable1 = [0.2; 0.3; 0.4; 0.1]\n\nprobability2 = [1; 2; 3]\ntable2 = zeros(4, 3, 1)\ntable2[1, 1, 1] = 0.2; table2[2, 1, 1] = 0.5; table2[3, 1, 1] = 0.3; table2[4, 1, 1] = 0.0\ntable2[1, 2, 1] = 0.1; table2[2, 2, 1] = 0.1; table2[3, 2, 1] = 0.7; table2[4, 2, 1] = 0.1\ntable2[1, 3, 1] = 0.5; table2[2, 3, 1] = 0.2; table2[3, 3, 1] = 0.1; table2[4, 3, 1] = 0.1\n\nprobability = [probability1, probability2]\ntable = [table1, table2]\n\nbp = discreteTreeModel(probability, table)  # initialize the tree graphical model\nwhile bp.graph.forward                      # inference from leaves to the root\n    forwardVariableFactor(bp)               # compute forward messages\n    forwardFactorVariable(bp)               # compute forward messages\nend\nwhile bp.graph.backward                     # inference from the root to leaves\n    backwardVariableFactor(bp)              # compute backward messages\n    backwardFactorVariable(bp)              # compute backward messages\nend\nmarginal(bp)                                # compute normalized marginals","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#More-information","page":"Home","title":"More information","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"M. Cosovic and D. Vukobratovic, \"Distributed Gauss-Newton Method for State Estimation Using Belief Propagation,\" in IEEE Transactions on  Power Systems, vol. 34, no. 1, pp. 648-658, Jan. 2019. arxiv.org\nM. Cosovic, \"Design and Analysis of Distributed State Estimation Algorithms Based on Belief Propagation and Applications in Smart Grids.\" arXiv preprint arXiv:1811.08355 (2018). arxiv.org","category":"page"}]
}
