<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Theoretical Background · GaussBP</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="GaussBP logo"/></a><div class="docs-package-name"><span class="docs-autofit">GaussBP</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../input/">Input Data</a></li><li><a class="tocitem" href="../runsettings/">Run Settings</a></li><li><a class="tocitem" href="../output/">Output Data</a></li><li class="is-active"><a class="tocitem" href>Theoretical Background</a><ul class="internal"><li><a class="tocitem" href="#vanillaGBP"><span>Linear GBP Algorithm</span></a></li><li><a class="tocitem" href="#efficientGBP"><span>Computation-efficient GBP Algorithm</span></a></li><li><a class="tocitem" href="#kahanGBP"><span>The GBP and Kahan–Babuška Algorithm</span></a></li><li><a class="tocitem" href="#dynamicGBP"><span>The Dynamic GBP Algorithm</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Theoretical Background</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Theoretical Background</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/mcosovic/GaussBP.jl/blob/master/docs/src/man/theoretical.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="theoretical"><a class="docs-heading-anchor" href="#theoretical">Theoretical Background</a><a id="theoretical-1"></a><a class="docs-heading-anchor-permalink" href="#theoretical" title="Permalink"></a></h1><p>As an input, we observe a noisy linear system of equations with real coefficients and variables:</p><p class="math-container">\[        \mathbf{z}=\mathbf{h}(\mathbf{x})+\mathbf{u},\]</p><p>where <span>$\mathbf {x}=[x_1,\dots,x_{n}]^{{T}}$</span> is the vector of the state variables, <span>$\mathbf{h}(\mathbf{x})= [h_1(\mathbf{x})$</span>, <span>$\dots$</span>, <span>$h_k(\mathbf{x})]^{{T}}$</span> is the vector of observation or measurement functions,  <span>$\mathbf{z} = [z_1,\dots,z_m]^{{T}}$</span> is the vector of measurement values, and <span>$\mathbf{u} = [u_1,\dots,u_k]^{{T}}$</span> is the vector of uncorrelated measurement errors. The linear system of equations is an overdetermined <span>$m&gt;n$</span> arising in many technical fields, such as statistics, signal processing, and control theory. </p><p>Each observation is associated with measured value <span>$z_i$</span>, measurement error  <span>$u_i$</span>, and measurement function <span>$h_i(\mathbf{x})$</span>. Under the assumption that measurement errors <span>$u_i$</span> follow a zero-mean Gaussian distribution, the probability density function associated with the <span>$i$</span>-th measurement is proportional to:</p><p class="math-container">\[    \mathcal{N}(z_i|\mathbf{x},v_i) \propto \exp\Bigg\{-\cfrac{[z_i-h_i(\mathbf{x})]^2}{2v_i}\Bigg\},\]</p><p>where <span>$v_i$</span> is the measurement variance defined by the measurement error <span>$u_i$</span>, and the measurement function <span>$h_i(\mathbf{x})$</span> connects the vector of state variables <span>$\mathbf{x}$</span> to the value of the <span>$i$</span>-th measurement.</p><p>The goal is to determine state variables <span>$\mathbf{x}$</span> according to the noisy observed data <span>$\mathbf{z}$</span> and a prior knowledge: </p><p class="math-container">\[ 	p(\mathbf{x}|\mathbf{z})= \cfrac{p(\mathbf{z}|\mathbf{x})p(\mathbf{x})}{p(\mathbf{z})}.\]</p><p>Assuming that the prior probability distribution <span>$p(\mathbf{x})$</span> is uniform, and given that <span>$p(\mathbf{z})$</span> does not depend on <span>$\mathbf{x}$</span>, the maximum a posteriori solution reduces to the maximum likelihood solution, as given below:</p><p class="math-container">\[	\hat{\mathbf{x}}= \mathrm{arg}\max_{\mathbf{x}}p(\mathbf{x}|\mathbf{z})= \mathrm{arg}\max_{\mathbf{x}}p(\mathbf{z}|\mathbf{x})=
	\mathrm{arg}\max_{\mathbf{x}}\mathcal{L}(\mathbf{z}|\mathbf{x}).\]</p><p>One can find the solution via maximization of the likelihood function <span>$\mathcal{L}(\mathbf{z}|\mathbf{x})$</span>, which is defined via likelihoods of <span>$m$</span> independent measurements:  </p><p class="math-container">\[	\hat{\mathbf x}= \mathrm{arg} \max_{\mathbf{x}}\mathcal{L}(\mathbf{z}|\mathbf{x})= 
    \mathrm{arg} \max_{\mathbf{x}} \prod_{i=1}^m \mathcal{N}(z_i|\mathbf{x},v_i).\]</p><p>It can be shown that the maximum a posteriori solution can be obtained by solving the following optimization problem, known as the weighted least-squares (WLS) problem:</p><p class="math-container">\[	\hat{\mathbf x} = \mathrm{arg}\min_{\mathbf{x}} \sum_{i=1}^m  \cfrac{[z_i-h_i(\mathbf x)]^2}{v_i}.\]</p><p>The state estimate <span>$\hat{\mathbf x}$</span> representing the solution of the optimization problem is known as the WLS estimator. The maximum likelihood and WLS estimator are equivalent to the maximum a posteriori solution.</p><hr/><h2 id="vanillaGBP"><a class="docs-heading-anchor" href="#vanillaGBP">Linear GBP Algorithm</a><a id="vanillaGBP-1"></a><a class="docs-heading-anchor-permalink" href="#vanillaGBP" title="Permalink"></a></h2><p>In the standard setup, the goal of the belief propagation (BP) algorithm is to efficiently evaluate the marginals of a set of random variables <span>$\mathcal{X} = \{x_1,\dots,x_n\}$</span> described via the joint probability density function <span>$g(\mathcal{X})$</span>. Assuming the function <span>$g(\mathcal{X})$</span> can be factorised proportionally (<span>$\propto$</span>) to a product of local functions:</p><p class="math-container">\[    g(\mathcal{X}) \propto \prod_{i=1}^m \psi(\mathcal{X}_i),\]</p><p>where <span>$\mathcal{X}_i \subseteq \mathcal{X}$</span>. The first step is forming a factor graph, which is a bipartite graph that describes the structure of the factorisation. Factor graph allows a graph-based representation of probability density functions using variable and factor nodes connected by edges. In contrast to directed and undirected graphical models, factor graphs provide the details of the factorisation in more explicit way. The factor graph structure comprises the set of factor nodes <span>$\mathcal{F}=\{f_1,\dots,f_m\}$</span>, where each factor node  <span>$f_i$</span> represents local function <span>$\psi(\mathcal{X}_i)$</span>, and the set of variable nodes <span>$\mathcal{X}$</span>. The factor node <span>$f_i$</span> connects to the variable node <span>$x_s$</span> if and only if <span>$x_s \in \mathcal{X}_i$</span>. </p><p>The BP algorithm on factor graphs proceeds by passing two types of messages along the edges of the factor graph: </p><ul><li>a variable node <span>$x_s \in \mathcal{X}$</span> to a factor node <span>$f_i \in \mathcal{F}$</span> message <span>$\mu_{x_s \to f_i}(x_s)$</span>, and  </li><li>a factor node <span>$f_i \in \mathcal{F}$</span> to a variable node <span>$x_s \in \mathcal{X}$</span> message <span>$\mu_{f_i \to x_s}(x_s)$</span>.</li></ul><p>Both variable and factor nodes in a factor graph process the incoming messages and calculate outgoing messages, where an output message on any edge depends on incoming messages from all other edges. The BP messages represent ``beliefs&quot; about variable nodes, thus a message that arrives or departs a certain variable node is a function (distribution) of the random variable corresponding to the variable node.</p><p>The Gaussian belief propagation (GBP) represents a class of the BP, where local function <span>$\psi(\mathcal{X}_i)$</span> is defined as a continuous Gaussian distribution:</p><p class="math-container">\[    \mathcal{N}(z_i|\mathcal{X}_i,v_i) \propto \exp\Bigg\{-\cfrac{[z_i-h(\mathcal{X}_i)]^2}{2v_i}\Bigg\},\]</p><p>where <span>$v_i$</span> is the variance, and the function <span>$h(\mathcal{X}_i)$</span> connects the set of state variables <span>$\mathcal{X}_i$</span> to the known <span>$z_i$</span> value. The \emph{linear}-GBP model implies the linear function <span>$h(\mathcal{X}_i)$</span>. If the linear-GBP algorithm converges, it will converge to a fixed point representing a true means \cite{bickson}, regardless of the structure of the factor graph. Unlike means, the variances of the linear-GBP algorithm may not converge to correct values for graphical models with loops, while for models without loops (i.e., tree factor graph) variances will have exact values.</p><p>Under the <strong>native GBP algorithm</strong> , we imply the algorithm in which messages are calculated as described below.</p><h4 id="Message-from-a-variable-node-to-a-factor-node"><a class="docs-heading-anchor" href="#Message-from-a-variable-node-to-a-factor-node">Message from a variable node to a factor node</a><a id="Message-from-a-variable-node-to-a-factor-node-1"></a><a class="docs-heading-anchor-permalink" href="#Message-from-a-variable-node-to-a-factor-node" title="Permalink"></a></h4><p>Consider a part of a factor graph with a group of factor nodes <span>$\mathcal{F}_s=\{f_i,f_w,...,f_W\}$</span> <span>$\subseteq$</span> <span>$\mathcal{F}$</span> that are neighbours of the variable node <span>$x_s \in \mathcal{X}$</span>. The message <span>$\mu_{x_s \to f_i}(x_s)$</span> from the variable node <span>$x_s$</span> to the factor node <span>$f_i$</span> is equal to the product of all incoming factor node to variable node messages arriving at all the other incident edges: </p><p class="math-container">\[    \mu_{x_s \to f_i}(x_s) =\prod_{f_a \in \mathcal{F}_s \setminus f_i} \mu_{f_a \to x_s}(x_s),\]</p><p>where <span>$\mathcal{F}_s \setminus f_i$</span> represents the set of factor nodes incident to the variable node <span>$x_s$</span>, excluding the factor node <span>$f_i$</span>. Note that each message is a function of the variable <span>$x_s$</span>.</p><p>Let us assume that the incoming messages <span>$\mu_{f_w \to x_s}(x_s)$</span>, <span>$\dots$</span>, <span>$\mu_{f_W \to x_s}(x_s)$</span> into the variable node <span>$x_s$</span> are Gaussian and represented by their mean-variance pairs <span>$(z_{f_w \to x_s},v_{f_w \to x_s})$</span>, <span>$\dots$</span>, <span>$(z_{f_W \to x_s},v_{f_W \to x_s})$</span>. Note that these messages carry beliefs about the variable node <span>$x_s$</span> provided by its neighbouring factor nodes <span>$\mathcal{F}_s\setminus f_i$</span>. It can be shown that the message <span>$\mu_{x_s \to f_i}(x_s)$</span> from the variable node <span>$x_s$</span> to the factor node <span>$f_i$</span> is proportional to:  </p><p class="math-container">\[	\mu_{x_s \to f_i}(x_s) \propto \mathcal{N}(x_s|z_{x_s \to f_i}, v_{x_s \to f_i}),		\]</p><p>with mean <span>$z_{x_s \to f_i}$</span> and variance <span>$v_{x_s \to f_i}$</span> obtained as: </p><p class="math-container">\[    z_{x_s \to f_i} = \Bigg( \sum_{f_a \in \mathcal{F}_s\setminus f_i} \cfrac{z_{f_a \to x_s}}{v_{f_a \to x_s}}\Bigg) v_{x_s \to f_i} \\
	\cfrac{1}{v_{x_s \to f_i}} = \sum_{f_a \in \mathcal{F}_s\setminus f_i} \cfrac{1}{v_{f_a \to x_s}}.\]</p><p>After the variable node <span>$x_s$</span> receives the messages from all of the neighbouring factor nodes from the set <span>$\mathcal{F}_s\setminus f_i$</span>, it evaluates the message <span>$\mu_{x_s \to f_i}(x_s)$</span>, and sends it to the factor node <span>$f_i$</span>. </p><h4 id="Message-from-a-factor-node-to-a-variable-node"><a class="docs-heading-anchor" href="#Message-from-a-factor-node-to-a-variable-node">Message from a factor node to a variable node</a><a id="Message-from-a-factor-node-to-a-variable-node-1"></a><a class="docs-heading-anchor-permalink" href="#Message-from-a-factor-node-to-a-variable-node" title="Permalink"></a></h4><p>Consider a part of a factor graph that consists of a group of variable nodes <span>$\mathcal{X}_i = \{x_s, x_l,...,x_L\}$</span> <span>$\subseteq$</span> <span>$\mathcal X$</span> that are neighbours of the factor node <span>$f_i$</span> <span>$\in$</span> <span>$\mathcal{F}$</span>. The message <span>$\mu_{f_i \to x_s}(x_s)$</span> from the factor node <span>$f_i$</span> to the variable node <span>$x_s$</span> is defined as a product of all incoming variable node to factor node messages arriving at other incident edges, multiplied by the function <span>$\psi_i(\mathcal{X}_i)$</span> associated to the factor node <span>$f_i$</span>, and marginalised over all of the variables associated with the incoming messages:</p><p class="math-container">\[    \mu_{f_i \to x_s}(x_s)= \int\limits_{x_l}\dots\int\limits_{x_L} \psi_i(\mathcal{X}_i)
	\prod_{x_b \in \mathcal{X}_i\setminus x_s} \big[\mu_{x_b \to f_i}(x_b) \cdot \mathrm{d}x_b\big], \]</p><p>where <span>$\mathcal{X}_i\setminus x_s$</span> is the set of variable nodes incident to the factor node <span>$f_i$</span>, excluding the variable node <span>$x_s$</span>.</p><p>Due to linearity of measurement functions <span>$h_i(\mathcal{X}_i)$</span>, closed form expressions for these messages is easy to obtain and follow a Gaussian form:</p><p class="math-container">\[	\mu_{f_i \to x_s}(x_s) \propto \mathcal{N}(x_s|z_{f_i \to x_s},v_{f_i \to x_s}).\]</p><p>The message <span>$\mu_{f_i \to x_s}(x_s)$</span> can be computed only when all other incoming messages (variable to factor node messages) are known. Let us assume that the messages into factor nodes are Gaussian, denoted by: </p><p class="math-container">\[		\mu_{x_l \to f_i}(x_l) \propto \mathcal{N}(x_l|z_{x_l \to f_i}, v_{x_l \to f_i})\\
		\vdots\\
		\mu_{x_L \to f_i}(x_L) \propto \mathcal{N}(x_L|z_{x_L \to f_i}, v_{x_L \to f_i}).\]</p><p>The Gaussian function associated with the factor node <span>$f_i$</span> is:</p><p class="math-container">\[	\mathcal{N}(z_i|\mathcal{X}_i, v_i) \propto \exp\Bigg\{-\cfrac{[z_i-h_i(\mathcal{X}_i)]^2} {2v_i}\Bigg\}.\]</p><p>The model contains only linear functions which we represent in a general form as:</p><p class="math-container">\[	h_i(\mathcal{X}_i) = C_{x_s} x_s + \sum_{x_b \in \mathcal{X}_i\setminus x_s} C_{x_b} x_b,\]</p><p>where <span>$\mathcal{X}_i\setminus x_s$</span> is the set of variable nodes incident to the factor node <span>$f_i$</span>, excluding the variable node <span>$x_s$</span>. </p><p>It can be shown that the message <span>$\mu_{f_i \to x_s}(x_s)$</span> from the factor node <span>$f_i$</span> to the variable node <span>$x_s$</span> is represented by the Gaussian function \eqref{BP<em>Gauss</em>fv}, with mean <span>$z_{f_i \to x_s}$</span> and variance <span>$v_{f_i \to x_s}$</span> obtained as: </p><p class="math-container">\[		z_{f_i \to x_s} = \cfrac{1}{C_{x_s}} \Bigg(z_i - \sum_{x_b \in \mathcal{X}_i \setminus x_s} 
        C_{x_b} z_{x_b \to f_i} \Bigg)\\
        v_{f_i \to x_s} = \cfrac{1}{C_{x_s}^2} \Bigg( v_i + \sum_{x_b \in \mathcal{X}_i \setminus x_s} C_{x_b}^2 v_{x_b \to f_i}  \Bigg).\]</p><p>To summarise, after the factor node <span>$f_i$</span> receives the messages from all of the neighbouring variable nodes from the set <span>$\mathcal{X}_i\setminus x_s$</span>, it evaluates the message <span>$\mu_{f_i \to x_s}(x_s)$</span>, and sends it to the variable node <span>$x_s$</span>. </p><h4 id="Marginal-inference"><a class="docs-heading-anchor" href="#Marginal-inference">Marginal inference</a><a id="Marginal-inference-1"></a><a class="docs-heading-anchor-permalink" href="#Marginal-inference" title="Permalink"></a></h4><p>The marginal of the variable node <span>$x_s$</span> is obtained as the product of all incoming messages into the variable node <span>$x_s$</span>:</p><p class="math-container">\[    p(x_s) =\prod_{f_c \in \mathcal{F}_s} \mu_{f_c \to x_s}(x_s),\]</p><p>where <span>$\mathcal{F}_s$</span> is the set of factor nodes incident to the variable node <span>$x_s$</span>. It can be shown that the marginal of the state variable <span>$x_s$</span> is represented by: </p><p class="math-container">\[    p(x_s) \propto \mathcal{N}(x_s|\hat x_s,v_{x_s}),\]</p><p>with the mean value <span>$\hat x_s$</span> and variance <span>$v_{x_s}$</span>:		</p><p class="math-container">\[    \hat x_s = \Bigg( \sum_{f_c \in \mathcal{F}_s} \cfrac{z_{f_c \to x_s}}{v_{f_c \to x_s}}\Bigg) v_{x_s} \\
	\cfrac{1}{v_{x_s}} = \sum_{f_c \in \mathcal{F}_s} \cfrac{1}{v_{f_c \to x_s}}.\]</p><p>Finally, the mean-value <span>$\hat x_s$</span> is adopted as the estimated value of the state variable <span>$x_s$</span>. </p><h2 id="efficientGBP"><a class="docs-heading-anchor" href="#efficientGBP">Computation-efficient GBP Algorithm</a><a id="efficientGBP-1"></a><a class="docs-heading-anchor-permalink" href="#efficientGBP" title="Permalink"></a></h2><p>We can make a substantial improvement to the vanilla GBP algorithm&#39;s complexity by reducing the number of calculations per outgoing messages. We achieve this reduction by summarisation of all incoming messages for each variable and factor node instead of summarising all incoming messages per each outgoing message. This simple trick, allow a single variable or factor node to share these summations across all outgoing messages, hence calculating these summations only once. As a result, each outgoing message involves a constant number of operations improving the worst-case running complexity to <span>$\mathcal{O}(nm)$</span>. In this framework, we calculate the message from the variable node to the factor node as:</p><p class="math-container">\[        z_{x_s \to f_i} = \Bigg(\alpha_{x_s} - \cfrac{z_{f_i \to x_s}}{v_{f_i \to x_s}}\Bigg) v_{x_s \to f_i} \\
		\cfrac{1}{v_{x_s \to f_i}} = \beta_{x_s} - \cfrac{1}{v_{f_i \to x_s}},\]</p><p>where:</p><p class="math-container">\[    \alpha_{x_s} = \sum_{f_a \in \mathcal{F}_s} \cfrac{z_{f_a \to x_s}}{v_{f_a \to x_s}};  \quad
	\beta_{x_s} = \sum_{f_a \in \mathcal{F}_s} \cfrac{1}{v_{f_a \to x_s}}. \]</p><p>Likewise, the message from the factor node to the variable node is:</p><p class="math-container">\[	z_{f_i \to x_s} = \cfrac{1}{C_{x_s}} \left(z_i - \alpha_{f_i} \right) + z_{x_s \to f_i} \\
    v_{f_i \to x_s} = \cfrac{1}{C_{x_s}^2} \left( v_i +  \beta_{f_i}  \right) - v_{x_s \to f_i},\]</p><p>where:</p><p class="math-container">\[    \alpha_{f_i} = \sum_{x_b \in \mathcal{X}_i} C_{x_b} z_{x_b \to f_i};  \quad
	\beta_{f_i} = \sum_{x_b \in \mathcal{X}_i} C_{x_b}^2 v_{x_b \to f_i}. \]</p><h2 id="kahanGBP"><a class="docs-heading-anchor" href="#kahanGBP">The GBP and Kahan–Babuška Algorithm</a><a id="kahanGBP-1"></a><a class="docs-heading-anchor-permalink" href="#kahanGBP" title="Permalink"></a></h2><p>The major drawback of the computation-efficient GBP algorithm is sensitivity to numerical errors because of the summation of floating-point numbers, due to possible significant differences in the values of incoming means and variances. However, this limitation can be alleviated with a compensated summation algorithm, such as the Kahan summation or the improved Kahan–Babuška algorithm. These algorithms increase the complexity of the operations by a constant factor, which means the time complexity of the worst-case remains unaffected. More precisely, we do summation that exist in the messages as:  </p><pre><code class="language-julia-repl">function kahan(summands, total, epsilon)
    t = total + summands
    if abs(total) &gt;= abs(summands)
        epsilon += (total - t) + summands
    else
        epsilon += (summands - t) + total
    end
    total = t
    
    return total, epsilon
end</code></pre><h2 id="dynamicGBP"><a class="docs-heading-anchor" href="#dynamicGBP">The Dynamic GBP Algorithm</a><a id="dynamicGBP-1"></a><a class="docs-heading-anchor-permalink" href="#dynamicGBP" title="Permalink"></a></h2><p>To recall, each factor node is associated with the measurement value <span>$z_i$</span> and the measurement variance  <span>$v_i$</span>. The dynamic framework allows the update of these values in any GBP iteration. This framework is an extension to the real-time model that operates continuously and accepts asynchronous measurements from different measurement subsystems. Such measurements are continuously integrated into the running instances of the GBP algorithm. Hence, the GBP algorithm can update the state estimate vector in a time-continuous process.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../output/">« Output Data</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 26 July 2021 12:14">Monday 26 July 2021</span>. Using Julia version 1.6.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
