<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Theoretical Background · GaussBP</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="GaussBP logo"/></a><div class="docs-package-name"><span class="docs-autofit">GaussBP</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../input/">Input Data</a></li><li><a class="tocitem" href="../graphicalmodel/">Graphical Model</a></li><li><a class="tocitem" href="../inference/">Inference</a></li><li><a class="tocitem" href="../utility/">Utility Functions</a></li><li><a class="tocitem" href="../output/">Output Data</a></li><li class="is-active"><a class="tocitem" href>Theoretical Background</a><ul class="internal"><li><a class="tocitem" href="#vanillaGBP"><span>Linear GBP Algorithm</span></a></li><li><a class="tocitem" href="#efficientGBP"><span>Computation-efficient GBP Algorithm</span></a></li><li><a class="tocitem" href="#kahanGBP"><span>The GBP and Kahan–Babuška Algorithm</span></a></li><li><a class="tocitem" href="#schedule"><span>Message Passing Schedule</span></a></li><li><a class="tocitem" href="#dampGBP"><span>The GBP with Randomized Damping</span></a></li><li><a class="tocitem" href="#dynamicGBP"><span>The Dynamic GBP Algorithm</span></a></li><li><a class="tocitem" href="#ageingGBP"><span>The Ageing GBP Algorithm</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Theoretical Background</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Theoretical Background</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/mcosovic/GaussBP.jl/blob/master/docs/src/man/theoretical.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="theoretical"><a class="docs-heading-anchor" href="#theoretical">Theoretical Background</a><a id="theoretical-1"></a><a class="docs-heading-anchor-permalink" href="#theoretical" title="Permalink"></a></h1><p>As an input, we observe a noisy linear system of equations with real coefficients and variables:</p><p class="math-container">\[        \mathbf{z}=\mathbf{h}(\mathbf{x})+\mathbf{u},\]</p><p>where <span>$\mathbf {x}=[x_1,\dots,x_{n}]^{{T}}$</span> is the vector of the state variables, <span>$\mathbf{h}(\mathbf{x})= [h_1(\mathbf{x})$</span>, <span>$\dots$</span>, <span>$h_k(\mathbf{x})]^{{T}}$</span> is the vector of observation or measurement functions,  <span>$\mathbf{z} = [z_1,\dots,z_m]^{{T}}$</span> is the vector of measurement values, and <span>$\mathbf{u} = [u_1,\dots,u_k]^{{T}}$</span> is the vector of uncorrelated measurement errors. The linear system of equations is an overdetermined <span>$m&gt;n$</span> arising in many technical fields, such as statistics, signal processing, and control theory.</p><p>Each observation is associated with measured value <span>$z_i$</span>, measurement error  <span>$u_i$</span>, and measurement function <span>$h_i(\mathbf{x})$</span>. Under the assumption that measurement errors <span>$u_i$</span> follow a zero-mean Gaussian distribution, the probability density function associated with the <span>$i$</span>-th measurement is proportional to:</p><p class="math-container">\[    \mathcal{N}(z_i|\mathbf{x},v_i) \propto \exp\Bigg\{-\cfrac{[z_i-h_i(\mathbf{x})]^2}{2v_i}\Bigg\},\]</p><p>where <span>$v_i$</span> is the measurement variance defined by the measurement error <span>$u_i$</span>, and the measurement function <span>$h_i(\mathbf{x})$</span> connects the vector of state variables <span>$\mathbf{x}$</span> to the value of the <span>$i$</span>-th measurement.</p><p>The goal is to determine state variables <span>$\mathbf{x}$</span> according to the noisy observed data <span>$\mathbf{z}$</span> and a prior knowledge:</p><p class="math-container">\[    p(\mathbf{x}|\mathbf{z})= \cfrac{p(\mathbf{z}|\mathbf{x})p(\mathbf{x})}{p(\mathbf{z})}.\]</p><p>Assuming that the prior probability distribution <span>$p(\mathbf{x})$</span> is uniform, and given that <span>$p(\mathbf{z})$</span> does not depend on <span>$\mathbf{x}$</span>, the maximum a posteriori solution reduces to the maximum likelihood solution, as given below:</p><p class="math-container">\[    \hat{\mathbf{x}}= \mathrm{arg}\max_{\mathbf{x}}p(\mathbf{x}|\mathbf{z})= \mathrm{arg}\max_{\mathbf{x}}p(\mathbf{z}|\mathbf{x})=
    \mathrm{arg}\max_{\mathbf{x}}\mathcal{L}(\mathbf{z}|\mathbf{x}).\]</p><p>One can find the solution via maximization of the likelihood function <span>$\mathcal{L}(\mathbf{z}|\mathbf{x})$</span>, which is defined via likelihoods of <span>$m$</span> independent measurements:</p><p class="math-container">\[    \hat{\mathbf x}= \mathrm{arg} \max_{\mathbf{x}}\mathcal{L}(\mathbf{z}|\mathbf{x})=
    \mathrm{arg} \max_{\mathbf{x}} \prod_{i=1}^m \mathcal{N}(z_i|\mathbf{x},v_i).\]</p><p>It can be shown that the maximum a posteriori solution can be obtained by solving the following optimization problem, known as the weighted least-squares (WLS) problem:</p><p class="math-container">\[    \hat{\mathbf x} = \mathrm{arg}\min_{\mathbf{x}} \sum_{i=1}^m  \cfrac{[z_i-h_i(\mathbf x)]^2}{v_i}.\]</p><p>The state estimate <span>$\hat{\mathbf x}$</span> representing the solution of the optimization problem is known as the WLS estimator. The maximum likelihood and WLS estimator are equivalent to the maximum a posteriori solution.</p><hr/><h2 id="vanillaGBP"><a class="docs-heading-anchor" href="#vanillaGBP">Linear GBP Algorithm</a><a id="vanillaGBP-1"></a><a class="docs-heading-anchor-permalink" href="#vanillaGBP" title="Permalink"></a></h2><p>In the standard setup, the goal of the belief propagation (BP) algorithm is to efficiently evaluate the marginals of a set of random variables <span>$\mathcal{X} = \{x_1,\dots,x_n\}$</span> described via the joint probability density function <span>$g(\mathcal{X})$</span>. Assuming the function <span>$g(\mathcal{X})$</span> can be factorised proportionally (<span>$\propto$</span>) to a product of local functions:</p><p class="math-container">\[    g(\mathcal{X}) \propto \prod_{i=1}^m \psi(\mathcal{X}_i),\]</p><p>where <span>$\mathcal{X}_i \subseteq \mathcal{X}$</span>. The first step is forming a factor graph, which is a bipartite graph that describes the structure of the factorisation. Factor graph allows a graph-based representation of probability density functions using variable and factor nodes connected by edges. In contrast to directed and undirected graphical models, factor graphs provide the details of the factorisation more explicitly. The factor graph structure comprises the set of factor nodes <span>$\mathcal{F}=\{f_1,\dots,f_m\}$</span>, where each factor node  <span>$f_i$</span> represents local function <span>$\psi(\mathcal{X}_i)$</span>, and the set of variable nodes <span>$\mathcal{X}$</span>. The factor node <span>$f_i$</span> connects to the variable node <span>$x_s$</span> if and only if <span>$x_s \in \mathcal{X}_i$</span>.</p><p>The BP algorithm on factor graphs proceeds by passing two types of messages along the edges of the factor graph:</p><ul><li>a variable node <span>$x_s \in \mathcal{X}$</span> to a factor node <span>$f_i \in \mathcal{F}$</span> message <span>$\mu_{x_s \to f_i}(x_s)$</span>, and</li><li>a factor node <span>$f_i \in \mathcal{F}$</span> to a variable node <span>$x_s \in \mathcal{X}$</span> message <span>$\mu_{f_i \to x_s}(x_s)$</span>.</li></ul><p>Both variable and factor nodes in a factor graph process the incoming messages and calculate outgoing messages, where an output message on any edge depends on incoming messages from all other edges. The BP messages represent ``beliefs&quot; about variable nodes, thus a message that arrives or departs a certain variable node is a function (distribution) of the random variable corresponding to the variable node.</p><p>The Gaussian belief propagation (GBP) represents a class of the BP, where local function <span>$\psi(\mathcal{X}_i)$</span> is defined as a continuous Gaussian distribution:</p><p class="math-container">\[    \mathcal{N}(z_i|\mathcal{X}_i,v_i) \propto \exp\Bigg\{-\cfrac{[z_i-h(\mathcal{X}_i)]^2}{2v_i}\Bigg\},\]</p><p>where <span>$v_i$</span> is the variance, and the function <span>$h(\mathcal{X}_i)$</span> connects the set of state variables <span>$\mathcal{X}_i$</span> to the known <span>$z_i$</span> value. The \emph{linear}-GBP model implies the linear function <span>$h(\mathcal{X}_i)$</span>. If the linear-GBP algorithm converges, it will converge to a fixed point representing a true means \cite{bickson}, regardless of the structure of the factor graph. Unlike means, the variances of the linear-GBP algorithm may not converge to correct values for graphical models with loops, while for models without loops (i.e., tree factor graph) variances will have exact values.</p><p>Under the <strong>native GBP algorithm</strong> , we imply the algorithm in which messages are calculated as described below.</p><h4 id="Message-from-a-variable-node-to-a-factor-node"><a class="docs-heading-anchor" href="#Message-from-a-variable-node-to-a-factor-node">Message from a variable node to a factor node</a><a id="Message-from-a-variable-node-to-a-factor-node-1"></a><a class="docs-heading-anchor-permalink" href="#Message-from-a-variable-node-to-a-factor-node" title="Permalink"></a></h4><p>Consider a part of a factor graph with a group of factor nodes <span>$\mathcal{F}_s=\{f_i,f_w,...,f_W\}$</span> <span>$\subseteq$</span> <span>$\mathcal{F}$</span> that are neighbours of the variable node <span>$x_s \in \mathcal{X}$</span>. The message <span>$\mu_{x_s \to f_i}(x_s)$</span> from the variable node <span>$x_s$</span> to the factor node <span>$f_i$</span> is equal to the product of all incoming factor node to variable node messages arriving at all the other incident edges:</p><p class="math-container">\[    \mu_{x_s \to f_i}(x_s) =\prod_{f_a \in \mathcal{F}_s \setminus f_i} \mu_{f_a \to x_s}(x_s),\]</p><p>where <span>$\mathcal{F}_s \setminus f_i$</span> represents the set of factor nodes incident to the variable node <span>$x_s$</span>, excluding the factor node <span>$f_i$</span>. Note that each message is a function of the variable <span>$x_s$</span>.</p><p>Let us assume that the incoming messages <span>$\mu_{f_w \to x_s}(x_s)$</span>, <span>$\dots$</span>, <span>$\mu_{f_W \to x_s}(x_s)$</span> into the variable node <span>$x_s$</span> are Gaussian and represented by their mean-variance pairs <span>$(z_{f_w \to x_s},v_{f_w \to x_s})$</span>, <span>$\dots$</span>, <span>$(z_{f_W \to x_s},v_{f_W \to x_s})$</span>. Note that these messages carry beliefs about the variable node <span>$x_s$</span> provided by its neighbouring factor nodes <span>$\mathcal{F}_s\setminus f_i$</span>. It can be shown that the message <span>$\mu_{x_s \to f_i}(x_s)$</span> from the variable node <span>$x_s$</span> to the factor node <span>$f_i$</span> is proportional to:</p><p class="math-container">\[    \mu_{x_s \to f_i}(x_s) \propto \mathcal{N}(x_s|z_{x_s \to f_i}, v_{x_s \to f_i}),\]</p><p>with mean <span>$z_{x_s \to f_i}$</span> and variance <span>$v_{x_s \to f_i}$</span> obtained as:</p><p class="math-container">\[    z_{x_s \to f_i} = \Bigg( \sum_{f_a \in \mathcal{F}_s\setminus f_i} \cfrac{z_{f_a \to x_s}}{v_{f_a \to x_s}}\Bigg) v_{x_s \to f_i} \\
    \cfrac{1}{v_{x_s \to f_i}} = \sum_{f_a \in \mathcal{F}_s\setminus f_i} \cfrac{1}{v_{f_a \to x_s}}.\]</p><p>After the variable node <span>$x_s$</span> receives the messages from all of the neighbouring factor nodes from the set <span>$\mathcal{F}_s\setminus f_i$</span>, it evaluates the message <span>$\mu_{x_s \to f_i}(x_s)$</span>, and sends it to the factor node <span>$f_i$</span>.</p><h4 id="Message-from-a-factor-node-to-a-variable-node"><a class="docs-heading-anchor" href="#Message-from-a-factor-node-to-a-variable-node">Message from a factor node to a variable node</a><a id="Message-from-a-factor-node-to-a-variable-node-1"></a><a class="docs-heading-anchor-permalink" href="#Message-from-a-factor-node-to-a-variable-node" title="Permalink"></a></h4><p>Consider a part of a factor graph that consists of a group of variable nodes <span>$\mathcal{X}_i = \{x_s, x_l,...,x_L\}$</span> <span>$\subseteq$</span> <span>$\mathcal X$</span> that are neighbours of the factor node <span>$f_i$</span> <span>$\in$</span> <span>$\mathcal{F}$</span>. The message <span>$\mu_{f_i \to x_s}(x_s)$</span> from the factor node <span>$f_i$</span> to the variable node <span>$x_s$</span> is defined as a product of all incoming variable node to factor node messages arriving at other incident edges, multiplied by the function <span>$\psi_i(\mathcal{X}_i)$</span> associated to the factor node <span>$f_i$</span>, and marginalised over all of the variables associated with the incoming messages:</p><p class="math-container">\[    \mu_{f_i \to x_s}(x_s)= \int\limits_{x_l}\dots\int\limits_{x_L} \psi_i(\mathcal{X}_i)
    \prod_{x_b \in \mathcal{X}_i\setminus x_s} \big[\mu_{x_b \to f_i}(x_b) \cdot \mathrm{d}x_b\big],\]</p><p>where <span>$\mathcal{X}_i\setminus x_s$</span> is the set of variable nodes incident to the factor node <span>$f_i$</span>, excluding the variable node <span>$x_s$</span>.</p><p>Due to linearity of measurement functions <span>$h_i(\mathcal{X}_i)$</span>, closed form expressions for these messages is easy to obtain and follow a Gaussian form:</p><p class="math-container">\[    \mu_{f_i \to x_s}(x_s) \propto \mathcal{N}(x_s|z_{f_i \to x_s},v_{f_i \to x_s}).\]</p><p>The message <span>$\mu_{f_i \to x_s}(x_s)$</span> can be computed only when all other incoming messages (variable to factor node messages) are known. Let us assume that the messages into factor nodes are Gaussian, denoted by:</p><p class="math-container">\[        \mu_{x_l \to f_i}(x_l) \propto \mathcal{N}(x_l|z_{x_l \to f_i}, v_{x_l \to f_i})\\
        \vdots\\
        \mu_{x_L \to f_i}(x_L) \propto \mathcal{N}(x_L|z_{x_L \to f_i}, v_{x_L \to f_i}).\]</p><p>The Gaussian function associated with the factor node <span>$f_i$</span> is:</p><p class="math-container">\[    \mathcal{N}(z_i|\mathcal{X}_i, v_i) \propto \exp\Bigg\{-\cfrac{[z_i-h_i(\mathcal{X}_i)]^2} {2v_i}\Bigg\}.\]</p><p>The model contains only linear functions which we represent in a general form as:</p><p class="math-container">\[    h_i(\mathcal{X}_i) = C_{x_s} x_s + \sum_{x_b \in \mathcal{X}_i\setminus x_s} C_{x_b} x_b,\]</p><p>where <span>$\mathcal{X}_i\setminus x_s$</span> is the set of variable nodes incident to the factor node <span>$f_i$</span>, excluding the variable node <span>$x_s$</span>.</p><p>It can be shown that the message <span>$\mu_{f_i \to x_s}(x_s)$</span> from the factor node <span>$f_i$</span> to the variable node <span>$x_s$</span> is represented by the Gaussian function \eqref{BP<em>Gauss</em>fv}, with mean <span>$z_{f_i \to x_s}$</span> and variance <span>$v_{f_i \to x_s}$</span> obtained as:</p><p class="math-container">\[        z_{f_i \to x_s} = \cfrac{1}{C_{x_s}} \Bigg(z_i - \sum_{x_b \in \mathcal{X}_i \setminus x_s}
        C_{x_b} z_{x_b \to f_i} \Bigg)\\
        v_{f_i \to x_s} = \cfrac{1}{C_{x_s}^2} \Bigg( v_i + \sum_{x_b \in \mathcal{X}_i \setminus x_s} C_{x_b}^2 v_{x_b \to f_i}  \Bigg).\]</p><p>To summarise, after the factor node <span>$f_i$</span> receives the messages from all of the neighbouring variable nodes from the set <span>$\mathcal{X}_i\setminus x_s$</span>, it evaluates the message <span>$\mu_{f_i \to x_s}(x_s)$</span>, and sends it to the variable node <span>$x_s$</span>.</p><h4 id="Marginal-inference"><a class="docs-heading-anchor" href="#Marginal-inference">Marginal inference</a><a id="Marginal-inference-1"></a><a class="docs-heading-anchor-permalink" href="#Marginal-inference" title="Permalink"></a></h4><p>The marginal of the variable node <span>$x_s$</span> is obtained as the product of all incoming messages into the variable node <span>$x_s$</span>:</p><p class="math-container">\[    p(x_s) =\prod_{f_c \in \mathcal{F}_s} \mu_{f_c \to x_s}(x_s),\]</p><p>where <span>$\mathcal{F}_s$</span> is the set of factor nodes incident to the variable node <span>$x_s$</span>. It can be shown that the marginal of the state variable <span>$x_s$</span> is represented by:</p><p class="math-container">\[    p(x_s) \propto \mathcal{N}(x_s|\hat x_s,v_{x_s}),\]</p><p>with the mean value <span>$\hat x_s$</span> and variance <span>$v_{x_s}$</span>:</p><p class="math-container">\[    \hat x_s = \Bigg( \sum_{f_c \in \mathcal{F}_s} \cfrac{z_{f_c \to x_s}}{v_{f_c \to x_s}}\Bigg) v_{x_s} \\
    \cfrac{1}{v_{x_s}} = \sum_{f_c \in \mathcal{F}_s} \cfrac{1}{v_{f_c \to x_s}}.\]</p><p>Finally, the mean-value <span>$\hat x_s$</span> is adopted as the estimated value of the state variable <span>$x_s$</span>.</p><hr/><h2 id="efficientGBP"><a class="docs-heading-anchor" href="#efficientGBP">Computation-efficient GBP Algorithm</a><a id="efficientGBP-1"></a><a class="docs-heading-anchor-permalink" href="#efficientGBP" title="Permalink"></a></h2><p>We can make a substantial improvement to the vanilla GBP algorithm&#39;s complexity by reducing the number of calculations per outgoing messages. We achieve this reduction by summarisation of all incoming messages for each variable and factor node instead of summarising all incoming messages per each outgoing message. This simple trick, allow a single variable or factor node to share these summations across all outgoing messages, hence calculating these summations only once. As a result, each outgoing message involves a constant number of operations improving the worst-case running complexity to <span>$\mathcal{O}(nm)$</span>. In this framework, we calculate the message from the variable node to the factor node as:</p><p class="math-container">\[        z_{x_s \to f_i} = \Bigg(\alpha_{x_s} - \cfrac{z_{f_i \to x_s}}{v_{f_i \to x_s}}\Bigg) v_{x_s \to f_i} \\
        \cfrac{1}{v_{x_s \to f_i}} = \beta_{x_s} - \cfrac{1}{v_{f_i \to x_s}},\]</p><p>where:</p><p class="math-container">\[    \alpha_{x_s} = \sum_{f_a \in \mathcal{F}_s} \cfrac{z_{f_a \to x_s}}{v_{f_a \to x_s}};  \quad
    \beta_{x_s} = \sum_{f_a \in \mathcal{F}_s} \cfrac{1}{v_{f_a \to x_s}}.\]</p><p>Likewise, the message from the factor node to the variable node is:</p><p class="math-container">\[    z_{f_i \to x_s} = \cfrac{1}{C_{x_s}} \left(z_i - \alpha_{f_i} \right) + z_{x_s \to f_i} \\
    v_{f_i \to x_s} = \cfrac{1}{C_{x_s}^2} \left( v_i +  \beta_{f_i}  \right) - v_{x_s \to f_i},\]</p><p>where:</p><p class="math-container">\[    \alpha_{f_i} = \sum_{x_b \in \mathcal{X}_i} C_{x_b} z_{x_b \to f_i};  \quad
    \beta_{f_i} = \sum_{x_b \in \mathcal{X}_i} C_{x_b}^2 v_{x_b \to f_i}.\]</p><hr/><h2 id="kahanGBP"><a class="docs-heading-anchor" href="#kahanGBP">The GBP and Kahan–Babuška Algorithm</a><a id="kahanGBP-1"></a><a class="docs-heading-anchor-permalink" href="#kahanGBP" title="Permalink"></a></h2><p>The major drawback of the computation-efficient GBP algorithm is sensitivity to numerical errors because of the summation of floating-point numbers, due to possible significant differences in the values of incoming means and variances. However, this limitation can be alleviated with a compensated summation algorithm, such as the Kahan summation or the improved Kahan–Babuška algorithm. These algorithms increase the complexity of the operations by a constant factor, which means the time complexity of the worst-case remains unaffected. More precisely, we do summation that exists in the messages as:</p><pre><code class="language-julia-repl">function kahan(summands, total, epsilon)
    t = total + summands
    if abs(total) &gt;= abs(summands)
        epsilon += (total - t) + summands
    else
        epsilon += (summands - t) + total
    end
    total = t

    return total, epsilon
end</code></pre><hr/><h2 id="schedule"><a class="docs-heading-anchor" href="#schedule">Message Passing Schedule</a><a id="schedule-1"></a><a class="docs-heading-anchor-permalink" href="#schedule" title="Permalink"></a></h2><p>We are employing a loopy GBP since the corresponding factor graph usually contains cycles. Loopy GBP is an iterative algorithm, and requires a message-passing schedule. Typically, the scheduling where messages from variable to factor nodes, and messages from factor nodes to variable nodes, are updated in parallel in respective half-iterations, is known as synchronous scheduling. Synchronous scheduling updates all messages in a given iteration using the output of the previous iteration as an input.</p><p>The initialization step starts with messages from singly connected factor nodes to variable nodes. Then, variable nodes forward the incoming messages received from singly connected factor nodes along remaining edges. To ensure this, we are using virtual factor nodes. Hence, the virtual factor node is a singly connected factor node used if the variable node is not directly observed. In a typical scenario, without prior knowledge, the variance of virtual factor nodes tend to infinity. We also improve convergence performance using virtual factor nodes.</p><hr/><h2 id="dampGBP"><a class="docs-heading-anchor" href="#dampGBP">The GBP with Randomized Damping</a><a id="dampGBP-1"></a><a class="docs-heading-anchor-permalink" href="#dampGBP" title="Permalink"></a></h2><p>We propose a randomized damping approach, where each mean value message from factor node to a variable node is damped independently with probability <span>$p$</span>, otherwise, the message is calculated as in the standard the GBP algorithm. The damped message is evaluated as a linear combination of the message from the previous and the current iteration, with weights <span>$\alpha$</span> and <span>$1 - \alpha$</span>, respectively. More, precisly, the proposed randomized damping scheduling updates of selected factor to variable node means in every iteration by combining them with their values from the previous iteration using convergence parameters <span>$p$</span> and <span>$\alpha$</span>:</p><p class="math-container">\[    z_{f_{i} \rightarrow x_{s}}^{(\tau)}=\left(1-q_{i s}\right) \cdot z_{f_{i} \rightarrow x_{s}}^{(\tau)}+q_{i s} \cdot\left(\alpha \cdot z_{f_{x} \rightarrow x_{s}}^{(\tau-1)}+(1-\alpha) \cdot z_{f_{i} \rightarrow x_{a}}^{(\tau)}\right),\]</p><p>where <span>$q_{i s} \sim \operatorname{Ber}(p) \in\{0,1\}$</span> is independently sampled with probability <span>$p$</span> for the mean from factor node <span>$f_i$</span> to the variable node <span>$x_s$</span>.</p><p>The randomised damping parameter pairs lead to a trade-off between the number of non-converging simulations and the rate of convergence. In general, we observe a large number of non-converging simulations for the selection of <code>prob</code> and <code>alpha</code> for which only a small fraction of messages are combined with their values in a previous iteration, and that is a case for <code>prob</code> close to 0 or <code>alpha</code> close to 1.</p><hr/><h2 id="dynamicGBP"><a class="docs-heading-anchor" href="#dynamicGBP">The Dynamic GBP Algorithm</a><a id="dynamicGBP-1"></a><a class="docs-heading-anchor-permalink" href="#dynamicGBP" title="Permalink"></a></h2><p>To recall, each factor node is associated with the measurement value <span>$z_i$</span> and the measurement variance  <span>$v_i$</span>. The dynamic framework allows the update of these values in any GBP iteration <span>$\tau$</span>. This framework is an extension to the real-time model that operates continuously and accepts asynchronous measurements from different measurement subsystems. Such measurements are continuously integrated into the running instances of the GBP algorithm. Hence, the GBP algorithm can update the state estimate vector in a time-continuous process.</p><p>Additionally, this framework allows for the artificial addition and removal of factor nodes. Then, the initial factor graph, described with the Jacobian matrix, should include all possible measurements. Measurements that are not active are then taken into account via extremely large values of variances (e.g., <span>$10^{60}$</span>). Consequently, estimates will have a unique solution according to measurement variances whose values are much smaller than <span>$10^{60}$</span>.</p><p>GaussBP uses the above data according to the Table.</p><table><tr><th style="text-align: center">Column</th><th style="text-align: left">Description</th></tr><tr><td style="text-align: center">1</td><td style="text-align: left">factor node index corresponding to the row number of the jacobian matrix</td></tr><tr><td style="text-align: center">2</td><td style="text-align: left">new observation value</td></tr><tr><td style="text-align: center">3</td><td style="text-align: left">new variance value</td></tr></table><hr/><h2 id="ageingGBP"><a class="docs-heading-anchor" href="#ageingGBP">The Ageing GBP Algorithm</a><a id="ageingGBP-1"></a><a class="docs-heading-anchor-permalink" href="#ageingGBP" title="Permalink"></a></h2><p>The ageing framework represents an extension of the dynamic model and establishes a model for measurement arrival processes and for the process of measurement deterioration or ageing over time (or GBP iterations). We integrate these measurements regularly into the running instances of the GBP algorithm.</p><p>Let <span>$\alpha$</span> denote iteration number when the factor node <span>$f_i$</span> receives the new measurement value <span>$z_i$</span> with the predefined variance <span>$v_{i}$</span>. After iteration instant <span>$\alpha$</span>, the ageing model increases variance value over iterations <span>$v_i(\tau)$</span>. More precisely, we associate the Gaussian distribution <span>$\mathcal{N}(z_i|\mathcal{X}_i, v_i(\tau))$</span> to the corresponding factor node <span>$f_i$</span>, where the variance <span>$v_i(\tau)$</span> increases its value starting from the predefined variance <span>$v_i(\tau) = v_i$</span>. Finally, in practice ageing model requires defining a limit from above <span>$\bar {v}_i$</span> of a function <span>$v_i(\tau)$</span>, instead of allowing variance to take on extremely large values.</p><p>Depending on the measurements arriving dynamic, an adaptive mechanism for increasing the variance over the time <span>$v_i(t)$</span> can be derived. The logarithmic growth model represents a promising solution for systems with a high sampling rate of the measurements, where a rapid increase in variance is required:</p><p class="math-container">\[    v_i(\tau) = \begin{cases}
      a \, \text{log} \left(\cfrac{\tau - \rho + 1 + b}{1 + b} \right ) + v_i, &amp; \alpha \leq \tau \leq \theta \\
      \bar {v}_i, &amp; \tau \geq \theta,
  \end{cases}\]</p><p>where <span>$a$</span> and <span>$b$</span> control the rate of growth.  In contrast, the exponential growth model corresponds to systems with a low sampling rate of the measurements:</p><p class="math-container">\[    v_i(\tau) = \begin{cases}
      v_i(1+b)^{a(\tau - \rho)}, &amp; \alpha \leq \tau \leq \theta \\
      \bar {v}_i, &amp; \tau \geq \theta.
  \end{cases}\]</p><p>Finally, the linear growth model can be observed as a compromise between logarithmic and exponential growth models:</p><p class="math-container">\[    v_i(\tau) = \begin{cases}
      a(\tau-\rho) + v_i, &amp; \alpha \leq \tau \leq \theta \\
      \bar {v}_i, &amp; \tau \geq \theta.
  \end{cases}\]</p><p>The ageing model uses the above data according to the Table.</p><table><tr><th style="text-align: center">Column</th><th style="text-align: center">Label</th><th style="text-align: left">Description</th></tr><tr><td style="text-align: center">1</td><td style="text-align: center"><span>$i$</span></td><td style="text-align: left">factor node index corresponding to the row number of the jacobian matrix</td></tr><tr><td style="text-align: center">2</td><td style="text-align: center"><span>$z_i$</span></td><td style="text-align: left">new observation value</td></tr><tr><td style="text-align: center">3</td><td style="text-align: center"><span>$v_i$</span></td><td style="text-align: left">new variance value</td></tr><tr><td style="text-align: center">4</td><td style="text-align: center">-</td><td style="text-align: left">the growth model, where linear = 1, logarithmic = 2, exponential = 3</td></tr><tr><td style="text-align: center">5</td><td style="text-align: center"><span>$a$</span></td><td style="text-align: left">the parameter that controls the rate of the growth</td></tr><tr><td style="text-align: center">6</td><td style="text-align: center"><span>$b$</span></td><td style="text-align: left">the parameter that controls the rate of the growth</td></tr><tr><td style="text-align: center">7</td><td style="text-align: center"><span>$\bar {v}_i$</span></td><td style="text-align: left">the variance upper limit value</td></tr></table></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../output/">« Output Data</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 23 August 2021 08:33">Monday 23 August 2021</span>. Using Julia version 1.6.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
