var documenterSearchIndex = {"docs":
[{"location":"man/input/#inputdata","page":"Input Data","title":"Input Data","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The GaussBP package supports HDF5 and XLSX input files or passing data directly via command-line arguments. The basic input data structure describing a linear system of equations includes the jacobian matrix containing coefficients of the equations, while vectors observation and variance represent measurement values and measurement variances, respectively. The functions graphicalModel() and graphicalModelTree() accept jacobian, observation and variance variables to form appropriate probabilistic graphical model. Note that, with large-scale systems, we strongly recommend using the HDF5 file data format.","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/input/#HDF5","page":"Input Data","title":"HDF5","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The HDF5 input file must contain the following elements:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"coefficient data model.h5/jacobian;\nmeasurement values model.h5/observation;\nmeasurement variances model.h5/variance.","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The type and structure of the input data must be:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"jacobian::Array{Float64, 2} = [row column coefficient];\nobservation::Array{Float64, 1};\nvariance::Array{Float64, 1}.","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/input/#XLSX","page":"Input Data","title":"XLSX","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The XLSX input file must contain the following sheets:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"coefficient data sheet: jacobian;\nmeasurement data sheet: measurement.","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The type and structure of the input data must be:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"jacobian - row | column | coefficient;\nmeasurement -  observation | variance.","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/input/#Passing-arguments","page":"Input Data","title":"Passing arguments","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The structure of the arguments must be:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"gbp = graphicalModel(jacobian, observation, variance).","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The type and structure of the arguments must be:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"jacobian::Union{Array{Float64, 2}, SparseMatrixCSC{Float64, Int64}};\nobservation::Array{Float64, 1};\nvariance::Array{Float64, 1};","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/input/#Data-structure","page":"Input Data","title":"Data structure","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The GaussBP package uses jacobian input data format for all analyses. Jacobian input data contains coefficients of the linear system of the equations. The structure of the jacobian variable being loaded from HDF5 or XLSX input files is given below:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"column 1: row indices of the corresponding jacobian matrix;\ncolumn 2: column indices of the corresponding jacobian matrix;\ncolumn 3: coefficient values of the corresponding jacobian matrix.","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"Passing data directly via command-line arguments allows the use of a sparse or full matrix to describe the jacobian variable.","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The observation and variance input data are used for all analyses available in the GaussBP package and contains measurement mean and variance values.","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/input/#Use-cases","page":"Input Data","title":"Use cases","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The pre-defined data are located in the src/example as the .h5 or .xlsx files:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"data33_14.xlsx;\ndata33_14.h5;\ndata897_300.h5;\ndata3119_1354.h5;\ndata5997_2000.h5;\ndata7149_2000.h5;\ndata29997_10000.h5;\ndata283803_70000.h5.\ndata12_11.h5;\ndata13_11.xlsx.","category":"page"},{"location":"man/graphicalmodel/#graphicalModel","page":"Graphical Model","title":"Graphical Model","text":"","category":"section"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"The GaussBP supports the composite type GraphicalModel related with the synchronous message passing schedule, with three fields:","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"FactorGraphTree;\nInference;\nSystemModel.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"The subtype FactorGraph describes the factor graph obtained based on the input data. The GBP inference and marginal values are kept in the subtype Inference. The system of the linear equations being solved is preserved in the subtype SystemModel. Note that the function graphicalModel() returns the main GaussBP composite type GraphicalModel with all subtypes.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"In addition, we also provide several functions for factor graph manipulation.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/graphicalmodel/#Build-graphical-model","page":"Graphical Model","title":"Build graphical model","text":"","category":"section"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"Input arguments DATA of the function graphicalModel() describe the graphical model, while the function returns GraphicalModel type.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"Loads the system data using h5-file from the package:","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"gbp = graphicalModel(\"data33_14.h5\")","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"Loads the system data using xlsx-file from the package:","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"gbp = graphicalModel(\"data33_14.xlsx\")","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"Loads the system data from a custom path:","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"gbp = graphicalModel(\"C:/name.h5\")","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"Loads the system data passing arguments directly:","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"gbp = graphicalModel(jacobian, observation, variances)","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/graphicalmodel/#Virtual-factor-nodes","page":"Graphical Model","title":"Virtual factor nodes","text":"","category":"section"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"The GBP function graphicalModel() receives arguments by keyword to set the mean and variance of the virtual factor nodes. We advise the reader to read the section message passing schedule which provides a detailed description of the virtual factor nodes.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"gbp = graphicalModel(DATA; mean = value, variance = value)","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"Default setting of the mean value is mean = 0.0, while the default variance is equal to variance = 1e10.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/graphicalmodel/#Randomized-damping-parametars","page":"Graphical Model","title":"Randomized damping parametars","text":"","category":"section"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"The GBP function graphicalModel() receives arguments by keyword to set damping parametars. We advise the reader to read the section the GBP with randomized damping which provides a detailed description of the input parameters.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"gbp = graphicalModel(DATA; prob = value, alpha = value)","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"The keyword prob represents the probability of the Bernoulli random variable, independently sampled for each mean value message from a factor node to a variable node, applied for randomised damping iteration scheme with value between 0 and 1. Default setting is set to prob = 0.6. The damped message is evaluated as a linear combination of the message from the previous and the current iteration, with weights alpha = value and 1 - alpha, applied for randomised damping iteration scheme where alpha is between 0 and 1. Default setting is set to alpha = 0.4.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"Using the function graphicalModel(), the set of damp messages are fixed through GBP iterations. However, we provide the function that changes damp parameters prob and alpha on the fly:","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"damping!(gbp; prob = value, alpha = value)","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/graphicalmodel/#Freeze-factor-node,-variable-node-or-edge","page":"Graphical Model","title":"Freeze factor node, variable node or edge","text":"","category":"section"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"The functions freeze target factor or variable node, whereby all messages sent by the factor or variable node retain latest obtained values at the time of freezing.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"freezeFactor!(gbp; factor = index)","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"freezeVariable!(gbp; variable = index)","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"We provide functions that freeze the target edge. More precisely, the function freezes the message from variable node to factor node, or the message from factor node to variable node. Hence, the frozen message keeps the last value obtained at the time of freezing.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"freezeVariableFactor!(gbp; variable = index, factor = index)","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"freezeFactorVariable!(gbp; factor = index, variable = index)","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"The functions accept following parameters: composite type GraphicalModel; the factor node index corresponding to the row index of the Jacobian matrix; and the variable node index corresponding to the column index of the Jacobian matrix. Note that the singly connected factor nodes can not be frozen because they always send the same message.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/graphicalmodel/#Defreeze-factor-node,-variable-node-or-edge","page":"Graphical Model","title":"Defreeze factor node, variable node or edge","text":"","category":"section"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"The functions unfreeze the target frozen factor node or frozen variable node, allowing the factor or variable node to calculate outgoing messages.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"defreezeFactor!(gbp; factor = index)","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"defreezeVariable!(gbp; variable = index)","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"Also, we provide functions that unfreeze the target edge, starting the calculation of messages either from variable node to factor node or from factor node to variable node calculates.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"defreezeVariableFactor!(gbp; variable = index, factor = index)","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"defreezeFactorVariable!(gbp; factor = index, variable = index)","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"The functions accept following parameters: composite type GraphicalModel; the factor node index corresponding to the row index of the Jacobian matrix; and the variable node index corresponding to the column index of the Jacobian matrix. Since singly connected factors cannot be frozen, they cannot be unfreezed.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/graphicalmodel/#Hide-factor-node","page":"Graphical Model","title":"Hide factor node","text":"","category":"section"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"Utilising a hiding mechanism, the function softly deletes factor node. Hence, the function obliterates the target factor node from the graph during the calculation. Soft delete actually removes the node, while preserving node numbering and keeping the same dimensions of the internal variables.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"hideFactor!(gbp; factor = index)","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"If the function targets the singly connected factor node, the function obliterates the target factor only if there are two or more singly connected factor nodes at the same variable node. If there is only one singly connected factor node at the variable node, the function transforms the target factor node to the virtual factor node. Note that to maintain consistency, the function also affects SystemModel.observation, SystemModel.jacobian and SystemModel.jacobianTranspose fields by setting non-zero elements to zero.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/graphicalmodel/#Add-factor-nodes","page":"Graphical Model","title":"Add factor nodes","text":"","category":"section"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"The function adds new factor nodes to the existing factor graph.","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"addFactors!(gbp; mean = vector, variance = vector, jacobian = matrix)","category":"page"},{"location":"man/graphicalmodel/","page":"Graphical Model","title":"Graphical Model","text":"The function supports addition of the multiple factor nodes to initial (existing) formation of the factor graph using the same input data format. The function accepts the following parameters: composite type GraphicalModel; the mean and variance vectors representing new measurement values and variances, respectively. The keyword jacobian with corresponding coefficients defines the set of equations describing new factor nodes. Also, function initializes messages from variable nodes to a new factor node using results from the last GBP iteration. Note that the function also affects SystemModel.observation, SystemModel.variance, SystemModel.jacobian and SystemModel.jacobianTranspose fields.","category":"page"},{"location":"man/outputtree/#outputtree","page":"Output Data","title":"Output Data","text":"","category":"section"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The main inference results are kept in the composite type GraphicalModelTree in the subtype Inference with fields:","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"fromFactor,\ntoVariable\nmeanFactorVariable,\nvarianceFactorVariable,\nfromVariable\ntoFactor\nmeanVariableFactor,\nvarianceVariableFactor,\nmean,\nvariance.","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The values of messages from factor nodes to variable nodes can be accessed using meanFactorVariable and varianceFactorVariable fields, while values of messages from variable nodes to factor nodes are stored in meanVariableFactor and varianceVariableFactor fields. These values correspond to edges defined by factor and variable nodes, with indexes preserved in fromFactor - toVariable and fromVariable - toFactor fields.","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"Fields mean and variance define state variable marginal distributions.","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The Inference field contains the GBP algorithm results. To describe the outputs, we will use the example shown below.","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"using GaussBP\n\n#     x1   x2   x3   x4\nH = [1.0  0.0  0.0  0.0;  # f1\n    -5.0 -4.0  9.0  0.0;  # f2\n     2.2  0.0  0.0  0.5;  # f3\n     0.0  0.0  1.0  0.0;  # f4\n     0.0  0.0  0.0  0.5]  # f5\n\n#     f1   f2   f3   f4   f5\nz = [0.0; 1.7; 1.9; 0.8; 2.1]\n\n#       f1   f2   f3   f4   f5\nv = [1e-10; 0.1; 0.1; 0.1; 0.1]","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The factor graph construction and message initialization is accomplished using graphicalModelTree() function.","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"gbp = graphicalModelTree(H, z, v)","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/outputtree/#Factor-graph-and-root-variable-node","page":"Output Data","title":"Factor graph and root variable node","text":"","category":"section"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The first step in solving/analysing the above system/system of equations is forming a factor graph, where set of variable nodes mathcalX = x_1 x_2 x_3 x_4  is defined by state variables. The set of equations denotes the set of factor nodes mathcalF = f_1 f_2 f_3 f_4 f_5.","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"<img src=\"../../assets/factorgraphtree.png\" class=\"center\"/>\n<figcaption>Figure 1: The tree factor graph with three variable nodes and three factor nodes.</figcaption>\n&nbsp;","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"Additionally, we include the virtual factor node f_v_1, where factor node f_v_1 is singly connected used when the variable node is not directly measured, hence having variance v_x_1 to infty or a priori given mean and variance of state variables. Further, the function graphicalModelTree() sets the first variable node x_1 as the root node.","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"To change defualt values of virtual factor nodes and defualt root variable node use:","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"gbp = graphicalModelTree(H, z, v; mean = 0.1, variance = 1e60, root = 3)","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/outputtree/#Messages-initialization","page":"Output Data","title":"Messages initialization","text":"","category":"section"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The initialization step starts with messages from local factor nodes f_1 f_v_1 f_4 f_5  to variable nodes mathcalX.","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/outputtree/#Forward-messages-from-the-leaf-nodes-to-the-root-node","page":"Output Data","title":"Forward messages from the leaf nodes to the root node","text":"","category":"section"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The GBP first forward recursion step starts by computing messages from leaf variable nodes x_2 x_4 to the incidence factor nodes f_2 f_3, using incoming messages from factor nodes f_v_1 f_5 .","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"forwardVariableFactor(gbp)\n\njulia> T = gbp.inference\njulia> [T.fromVariable  T.toFactor  T.meanVariableFactor T.varianceVariableFactor]\n5×4 Matrix{Float64}:\n 2.0  2.0  0.1  1.0e60\n 4.0  3.0  4.2  0.4\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The first row defines the message from variable node x_2 to factor node f_2, the second row keeps the message from variable node x_4 to factor node f_3. Zero rows are initialized for messages to be calculated in the next forward and backward steps.","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The second forward recursion step computes messages from factor nodes f_3 to the variable node x_1, using incoming message from variable node x_4.","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"forwardFactorVariable(gbp)\n\njulia> T = gbp.inference\njulia> [T.fromFactor T.toVariable T.meanFactorVariable T.varianceFactorVariable]\n5×4 Matrix{Float64}:\n 3.0  1.0  -0.0909091  0.0413223\n 0.0  0.0   0.0        0.0\n 0.0  0.0   0.0        0.0\n 0.0  0.0   0.0        0.0\n 0.0  0.0   0.0        0.0","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The first row defines the message from factor node f_3 to variable node x_1. Zero rows are initialized for messages to be calculated in the next forward and backward steps.","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The message passing steps from variable nodes to factor nodes and from factor nodes to variable nodes are then applied recursively until messages have been propagated along every link, and the root node has received messages from all of its neighbours. The GaussBP keeps flag gbp.graph.forward to signal that moment. Therefore, a complete forward step can be done using:","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"while gbp.graph.forward\n    forwardVariableFactor(gbp)\n    forwardFactorVariable(gbp)\nend","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/outputtree/#Backward-messages-the-root-node-to-the-leaf-nodes","page":"Output Data","title":"Backward messages the root node to the leaf nodes","text":"","category":"section"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The GBP first backward recursion step starts by computing messages from root variable nodes x_3 to the factor node f_2, using incoming messages from factor node f_4.","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"backwardVariableFactor(gbp)\n\njulia> T = gbp.inference\njulia> [T.fromVariable  T.toFactor  T.meanVariableFactor T.varianceVariableFactor]\n5×4 Matrix{Float64}:\n 2.0  2.0   0.1      1.0e60\n 4.0  3.0   4.2      0.4\n 1.0  2.0  -2.2e-10  1.0e-10\n 3.0  2.0   0.8      0.1\n 0.0  0.0   0.0      0.0","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The first three rows are obtained using forward steps. The fourth row defines the message from variable node x_3 to factor node f_2.","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The secand backward recursion step computes message from factor nodes f_2 to the variable nodes x_1 x_2, using incoming message from variable node x_3.","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"backwardFactorVariable(gbp)\n\njulia> T = gbp.inference\njulia> [T.fromFactor T.toVariable T.meanFactorVariable T.varianceFactorVariable]\n5×4 Matrix{Float64}:\n 3.0  1.0  -0.0909091  0.0413223\n 2.0  3.0   0.233333   1.97531e59\n 2.0  1.0   1.02       6.4e59\n 2.0  2.0   1.375      0.5125\n 0.0  0.0   0.0        0.0","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The first two rows are obtained using forward steps. The third row defines the message from factor node f_2 to variable node x_1, the fourth row keeps the message from factor node f_2 to variable node x_2","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"Thus, the backward recursion starts when the root node received messages from all of its neighbours. It can therefore send out messages to all of its neighbours. These in turn will then have received messages from all of their neighbours and so can send out messages along the links going away from the root, and so on. In this way, messages are passed outwards from the root all the way to the leaves. The GaussBP keeps flag gbp.graph.backward to signal that moment. Therefore, a complete forward step can be done using:","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"while gbp.graph.backward\n    backwardVariableFactor(gbp)\n    backwardFactorVariable(gbp)\nend","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/outputtree/#Marginals","page":"Output Data","title":"Marginals","text":"","category":"section"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"The marginal of variable nodes mathcalX can be obtained using messages from factor nodes mathcalF to variable nodes mathcalX. Note that the mean value of marginal is adopted as the estimated value of the state variable. Finally, we obtain:","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"marginal(gbp)\n\njulia> [gbp.inference.mean gbp.inference.variance]\n3×2 Matrix{Float64}:\n -2.2e-10  1.0e-10\n  1.375    0.5125\n  0.8      0.1\n  4.0      0.2","category":"page"},{"location":"man/outputtree/","page":"Output Data","title":"Output Data","text":"Where rows correspond with mean and variance values of the state variables x_1 x_2 x_3 x_4 .","category":"page"},{"location":"man/inference/#vanilla","page":"Inference","title":"Inference","text":"","category":"section"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"We advise the reader to read the theoretical background which provides a detailed description of the inference algorithms. To exchange information over the factor graph, the GaussBP provides three inference approaches:","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"vanilla GBP algorithm,\ncomputation-efficient GBP algorithm,\ncomputation-efficient kahan–babuška GBP algorithm.","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"Each of the inference functions accepts only the composite type GraphicalModel, i.e., an output variable of the function gbp = graphicalModel() and applies the synchronous message passing schedule.","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/inference/#Message-inference","page":"Inference","title":"Message inference","text":"","category":"section"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"The set of functions that can be used to preform message inference:","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"messageFactorVariableVanilla(gbp); messageVariableFactorVanilla(gbp)\nmessageFactorVariableEfficient(gbp); messageVariableFactorEfficient(gbp)\nmessageFactorVariableKahan(gbp); messageVariableFactorKahan(gbp)","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/inference/#Mean-inference","page":"Inference","title":"Mean inference","text":"","category":"section"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"The set of functions that can be used to preform only mean inference:","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"meanFactorVariableVanilla(gbp); meanVariableFactorVanilla(gbp)\nmeanFactorVariableEfficient(gbp); meanVariableFactorEfficient(gbp)\nmeanFactorVariableKahan(gbp); meanVariableFactorKahan(gbp)","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/inference/#Variance-inference","page":"Inference","title":"Variance inference","text":"","category":"section"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"The set of functions that can be used to preform only variance inference:","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"varianceFactorVariableVanilla(gbp); varianceVariableFactorVanilla(gbp)\nvarianceFactorVariableEfficient(gbp); varianceVariableFactorEfficient(gbp)\nvarianceFactorVariableKahan(gbp); varianceVariableFactorKahan(gbp)","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/inference/#Randomised-damping-inference","page":"Inference","title":"Randomised damping inference","text":"","category":"section"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"Additionaly, we provide the set of functions to preform damping inference:","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"messageDampFactorVariableVanilla(gbp); meanDampFactorVariableVanilla(gbp)\nmessageDampFactorVariableEfficient(gbp); meanDampFactorVariableEfficient(gbp)\nmessageDampFactorVariableKahan(gbp); meanDampFactorVariableKahan(gbp)","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/inference/#Marginal-inference","page":"Inference","title":"Marginal inference","text":"","category":"section"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"To compute marginals the GaussBP provides the function:","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"marginal(gbp)","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"Same as before, the function accepts only the composite type GraphicalModel.","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/inference/#Dynamic-inference","page":"Inference","title":"Dynamic inference","text":"","category":"section"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"This framework is an extension to the real-time model that operates continuously and accepts asynchronous measurement mean and variance values. More precisely, in each GBP iteration user can change the mean and variance values of the corresponding factor nodes and continue the GBP iteration process. We advise the reader to read the section dynamic GBP algorithm which provides a detailed description of the input parameters.","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"dynamicFactor!(gbp; factor = index, mean = value, variance = value)","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"The function accepts the composite type GraphicalModel and keywords factor, mean and variance, which defines the dynamic update scheme of the factor nodes. The factor node index corresponding to the row index of the jacobian matrix. Note that during each function call, SystemModel.observation and SystemModel.variance fields also change values according to the scheme.","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/inference/#Dynamic-inference-with-variance-ageing","page":"Inference","title":"Dynamic inference with variance ageing","text":"","category":"section"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"The ageing framework represents an extension of the dynamic model and establishes a model for measurement arrival processes and for the process of measurement deterioration or ageing over time (or GBP iterations). We integrate these measurements regularly into the running instances of the GBP algorithm. We advise the reader to read the section ageing GBP algorithm which provides a detailed description of the input parameters.","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"ageingVariance!(gbp; factor = index, initial = value, limit = value,\n                model = value, a = value, b = value, tau = value)","category":"page"},{"location":"man/inference/","page":"Inference","title":"Inference","text":"This function should be integrated into the iteration loop to ensure variance ageing over iterations. The function accepts the composite type GraphicalModel and the keywords factor, initial, limit, model, a, b and tau. The variance growth model can be linear model = 1, logarithmic model = 2 and exponential model = 3, where parameters a and b control the rate of the growth. The initial defines the initial value of the variance, while the variance upper limit value is defined according to limit. The ageing model increases the value of variance over iterations, thus the current iteration step should be forwarded using tau keyword. Also, during each function call, SystemModel.variance field changes values according to the ageing model.","category":"page"},{"location":"man/utility/#utilityfunction","page":"Utility Functions","title":"Utility Functions","text":"","category":"section"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The GaussBP provides several utility functions to evaluate and compare obtained results.","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"","category":"page"},{"location":"man/utility/#The-WLS-results","page":"Utility Functions","title":"The WLS results","text":"","category":"section"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The function provides the estimate obtained by the WLS method and root mean square error (RMSE), the mean absolute error (MAE) and the weighted residual sum of squares (WRSS) error metrics evaluated according to the WLS solutions. These results can be used to compare results obtained by the GBP algorithm.","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"exact = wls(gbp)","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The function returns the composite type WeightedLeastSquares with fields estimate, rmse, mae, wrss. Note that results are obtained according to variables SystemModel.jacobian, SystemModel.observation and SystemModel.variance.","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"","category":"page"},{"location":"man/utility/#The-GBP-error-metrics","page":"Utility Functions","title":"The GBP error metrics","text":"","category":"section"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The package provides the function to obtain RMSE, MAE, and WRSS error metrics of the GBP algorithm.","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"evaluation = errorMetric(gbp)","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The function returns the composite type ErrorMetric with fields rmse, mae, wrss. Further, passing the composite type WeightedLeastSquares, we obtained additional fields rmseGBPWLS and maeGBPWLS that determine the distance between the GBP estimate and WLS estimate.","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"evaluation = errorMetric(gbp, exact)","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The function returns the composite type ErrorMetricWiden with fields rmse, mae, wrss, rmseGBPWLS, maeGBPWLS.","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"","category":"page"},{"location":"man/utility/#Display-results","page":"Utility Functions","title":"Display results","text":"","category":"section"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The function shows data display in the Julia REPL, and can provide different views depending on the input variables.","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The following function can be used to show GBP results:","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"displayData(gbp)","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"To show the GBP results and error metric use:","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"displayData(gbp, evaluation)","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"To show the GBP and WLS results use:","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"displayData(gbp, exact)","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"To show the GBP and WLS results, and error metrics use:","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"displayData(gbp, exact, evaluation)","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"","category":"page"},{"location":"man/utility/#Error-metrics","page":"Utility Functions","title":"Error metrics","text":"","category":"section"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"The root mean square error, the mean absolute error and the weighted residual sum of squares are evaluated according to:","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"  beginaligned\n    textrmse = sqrt cfracsum_i=1^m leftz_i - h_i(hatmathbf x) right^2m quad\n    textmae = cfracsum_i=1^m leftz_i - h_i(hatmathbf x) rightm quad\n    textwrss = sum_i=1^m cfracleftz_i - h_i(hatmathbf x) right^2v_i\n  endaligned","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"where m denotes the number of observations, z_i is observation value, v_i is observation variance, and corresponding equation h_i(hatmathbf x) is evaluated at the point hatmathbf x obtained using the GBP or WLS algorithm. Note, wrss is the value of the objective function of the optimization problem we are solving.","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"&nbsp;","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"Fields rmseGBPWLS and maeGBPWLS determine distance beetwen the GBP estimate hatx_textgbpi and WLS estimate hatx_textwlsi, where root mean square error and mean absolute error are obtained using:","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"  beginaligned\n    textrmse = sqrt cfracsum_i=1^n lefthatx_textwlsi - hatx_textgbpi) right^2n quad\n    textmae = cfracsum_i=1^n lefthatx_textwlsi - hatx_textgbpi) rightn\n  endaligned","category":"page"},{"location":"man/utility/","page":"Utility Functions","title":"Utility Functions","text":"where n is the number of state variables.","category":"page"},{"location":"man/output/#output","page":"Output Data","title":"Output Data","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The main inference results are kept in the composite type GraphicalModel in the subtype Inference with fields:","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"fromFactor,\ntoVariable\nmeanFactorVariable,\nvarianceFactorVariable,\nfromVariable\ntoFactor\nmeanVariableFactor,\nvarianceVariableFactor,\nmean,\nvariance.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The values of messages from factor nodes to variable nodes can be accessed using meanFactorVariable and varianceFactorVariable fields, while values of messages from variable nodes to factor nodes are stored in meanVariableFactor and varianceVariableFactor fields. These values correspond to edges defined by factor and variable nodes, with indexes preserved in fromFactor - toVariable and fromVariable - toFactor fields.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Fields mean and variance define state variable marginal distributions.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The Inference field contains the GBP algorithm results. To describe the outputs, we will use the example shown below.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"using GaussBP\n\n#     x1   x2   x3\nH = [1.0  0.0  0.0;  # f1\n     2.0 -2.0  0.0;  # f2\n    -5.0 -4.0  9.0;  # f3\n     0.0  0.0  1.0]  # f4\n\n#     f1   f2   f3   f4\nz = [0.0; 1.7; 1.9; 0.2]\n\n#       f1   f2   f3    f4\nv = [1e-10; 0.1; 0.1; 1e-2]","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The factor graph construction and message initialization is accomplished using graphicalModel() function.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"gbp = graphicalModel(H, z, v)","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/output/#Factor-graph","page":"Output Data","title":"Factor graph","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The first step in solving/analysing the above system/system of equations is forming a factor graph, where set of variable nodes mathcalX = x_1 x_2 x_3  is defined by state variables. The set of equations denotes the set of factor nodes mathcalF = f_1 f_2 f_3 f_4 .","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"<img src=\"../../assets/factorgraph.png\" class=\"center\"/>\n<figcaption>Figure 1: The factor graph with three variable nodes and four factor nodes.</figcaption>\n&nbsp;","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Additionaly, we include the virtual factor node f_v_1, where factor node f_v_1 is a singly connected factor node used when the variable node is not directly measured, hence having variance v_x_1 to infty or a priori given mean and variance of state variables. To change defualt values of virtual factor nodes use:","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"gbp = graphicalModel(H, z, v; mean = 0.1, variance = 1e60)","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/output/#Messages-initialization","page":"Output Data","title":"Messages initialization","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The initialization step starts with messages from local factor nodes f_1 f_v_1 f_4  to variable nodes mathcalX. Then, variable nodes mathcalX forward the incoming messages received from local factor nodes along remaining edges defined by f_2 f_3 and mathcalX.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/output/#Messages-from-factor-nodes-to-variable-nodes","page":"Output Data","title":"Messages from factor nodes to variable nodes","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The GBP iterations computing messages from indirect factor nodes f_2 f_3 to variable nodes mathcalX, using incoming messages from variable nodes mathcalX to indirect factor nodes f_2 f_3 obtained in the previus step.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"messageFactorVariableVanilla(gbp)\n\njulia> T = gbp.inference\njulia> [T.fromFactor T.toVariable T.meanFactorVariable T.varianceFactorVariable]\n5×4 Matrix{Float64}:\n 2.0  1.0   0.95      1.0e60\n 3.0  1.0  -0.1       6.4e59\n 2.0  2.0  -0.85      0.025\n 3.0  2.0  -0.025     0.056875\n 3.0  3.0   0.255556  1.97531e59","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The first row defines the message from factor node f_2 to variable node x_1, the second row keeps the message from factor node f_3 to variable node x_1, etc.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/output/#Messages-from-variable-nodes-to-factor-nodes","page":"Output Data","title":"Messages from variable nodes to factor nodes","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Next, the algorithm proceeds with computing messages from variable nodes mathcalX to indirect factor nodes f_1 f_2, using incoming messages from factor nodes mathcalF to variable nodes mathcalX.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"messageVariableFactorVanilla(gbp)\n\njulia> T = gbp.inference\njulia> [T.fromVariable T.toFactor T.meanVariableFactor T.varianceVariableFactor]\n5×4 Matrix{Float64}:\n 1.0  2.0  -1.5625e-71  1.0e-10\n 2.0  2.0  -0.025       0.056875\n 1.0  3.0   9.5e-71     1.0e-10\n 2.0  3.0  -0.85        0.025\n 3.0  3.0   0.2         0.01","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The first row defines the message from variable node x_1 to factor node f_2, the second row keeps the message from variable node x_2 to factor node f_2, etc.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/output/#Marginals","page":"Output Data","title":"Marginals","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The marginal of variable nodes mathcalX can be obtained using messages from factor nodes mathcalF to variable nodes mathcalX. Note that the mean value of marginal is adopted as the estimated value of the state variable. Thus, after 100 iterations, we obtain:","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"marginal(gbp)\n\njulia> [gbp.inference.mean gbp.inference.variance]\n3×2 Matrix{Float64}:\n  2.26718e-9  1.0e-10\n -0.598092    0.0173664\n -0.0267176   0.00381679","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Where rows correspond with mean and variance values of the state variables x_1 x_2 x_3 .","category":"page"},{"location":"man/graphicalmodeltree/#graphicalModel","page":"Graphical Model","title":"Graphical Model Tree","text":"","category":"section"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"The GaussBP supports the composite type GraphicalModelTree related with the forward–backward message passing, with three fields:","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"FactorGraphTree;\nInference;\nSystemModel.","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"The subtype FactorGraphTree describes the tree factor graph obtained based on the input data. The GBP inference and marginal values are kept in the subtype Inference. The system of the linear equations being solved is preserved in the subtype SystemModel. Note that the function graphicalModelTree() returns the main composite type GraphicalModelTree with all subtypes.","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/graphicalmodeltree/#Build-graphical-model","page":"Graphical Model","title":"Build graphical model","text":"","category":"section"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"Input arguments DATA of the function graphicalModelTree() describe the tree graphical model, while the function returns GraphicalModelTree type.","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"Loads the system data using h5-file from the package:","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"gbp = graphicalModelTree(\"data12_11.h5\")","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"Loads the system data using xlsx-file from the package:","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"gbp = graphicalModelTree(\"data13_11.xlsx\")","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"Loads the system data from a custom path:","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"gbp = graphicalModelTree(\"C:/name.h5\")","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"Loads the system data passing arguments directly:","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"gbp = graphicalModelTree(jacobian, observation, variances)","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/graphicalmodeltree/#Virtual-factor-nodes","page":"Graphical Model","title":"Virtual factor nodes","text":"","category":"section"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"The GBP function graphicalModelTree() receives arguments by keyword to set the mean and variance of the virtual factor nodes to initiate messages from the leaves variable nodes if the corresponding variable node does not have a singly connected factor node.","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"gbp = graphicalModelTree(DATA; mean = value, variance = value)","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"Default setting of the mean value is mean = 0.0, while the default variance is equal to variance = 1e10.","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/graphicalmodeltree/#Root-variable-node","page":"Graphical Model","title":"Root variable node","text":"","category":"section"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"The GBP function graphicalModelTree() receives argument by keyword to set the root variable node.","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"gbp = graphicalModelTree(DATA; root = index)","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"Default setting of the root variable node is root = 1.","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"","category":"page"},{"location":"man/graphicalmodeltree/#Tree-factor-graph","page":"Graphical Model","title":"Tree factor graph","text":"","category":"section"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"Function checks whether the factor graph has a tree structure.","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"tree = isTree(gbp)","category":"page"},{"location":"man/graphicalmodeltree/","page":"Graphical Model","title":"Graphical Model","text":"The tree structure of tha factor graph is marked as tree = true, the opposite is tree = false. The function accepts the composite type GraphicalModelTree, as well as the type GraphicalModel.","category":"page"},{"location":"man/theoretical/#theoretical","page":"Theoretical Background","title":"Theoretical Background","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"As an input, we observe a noisy linear system of equations with real coefficients and variables:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"        mathbfz=mathbfh(mathbfx)+mathbfu","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathbf x=x_1dotsx_n^T is the vector of the state variables, mathbfh(mathbfx)= h_1(mathbfx), dots, h_k(mathbfx)^T is the vector of observation or measurement functions,  mathbfz = z_1dotsz_m^T is the vector of measurement values, and mathbfu = u_1dotsu_k^T is the vector of uncorrelated measurement errors. The linear system of equations is an overdetermined mn arising in many technical fields, such as statistics, signal processing, and control theory.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Each observation is associated with measured value z_i, measurement error  u_i, and measurement function h_i(mathbfx). Under the assumption that measurement errors u_i follow a zero-mean Gaussian distribution, the probability density function associated with the i-th measurement is proportional to:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    mathcalN(z_imathbfxv_i) propto expBigg-cfracz_i-h_i(mathbfx)^22v_iBigg","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where v_i is the measurement variance defined by the measurement error u_i, and the measurement function h_i(mathbfx) connects the vector of state variables mathbfx to the value of the i-th measurement.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The goal is to determine state variables mathbfx according to the noisy observed data mathbfz and a prior knowledge:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    p(mathbfxmathbfz)= cfracp(mathbfzmathbfx)p(mathbfx)p(mathbfz)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Assuming that the prior probability distribution p(mathbfx) is uniform, and given that p(mathbfz) does not depend on mathbfx, the maximum a posteriori solution reduces to the maximum likelihood solution, as given below:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    hatmathbfx= mathrmargmax_mathbfxp(mathbfxmathbfz)= mathrmargmax_mathbfxp(mathbfzmathbfx)=\n    mathrmargmax_mathbfxmathcalL(mathbfzmathbfx)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"One can find the solution via maximization of the likelihood function mathcalL(mathbfzmathbfx), which is defined via likelihoods of m independent measurements:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    hatmathbf x= mathrmarg max_mathbfxmathcalL(mathbfzmathbfx)=\n    mathrmarg max_mathbfx prod_i=1^m mathcalN(z_imathbfxv_i)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"It can be shown that the maximum a posteriori solution can be obtained by solving the following optimization problem, known as the weighted least-squares (WLS) problem:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    hatmathbf x = mathrmargmin_mathbfx sum_i=1^m  cfracz_i-h_i(mathbf x)^2v_i","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The state estimate hatmathbf x representing the solution of the optimization problem is known as the WLS estimator. The maximum likelihood and WLS estimator are equivalent to the maximum a posteriori solution.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"","category":"page"},{"location":"man/theoretical/#vanillaGBP","page":"Theoretical Background","title":"Linear GBP Algorithm","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"In the standard setup, the goal of the belief propagation (BP) algorithm is to efficiently evaluate the marginals of a set of random variables mathcalX = x_1dotsx_n described via the joint probability density function g(mathcalX). Assuming the function g(mathcalX) can be factorised proportionally (propto) to a product of local functions:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    g(mathcalX) propto prod_i=1^m psi(mathcalX_i)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathcalX_i subseteq mathcalX. The first step is forming a factor graph, which is a bipartite graph that describes the structure of the factorisation. Factor graph allows a graph-based representation of probability density functions using variable and factor nodes connected by edges. In contrast to directed and undirected graphical models, factor graphs provide the details of the factorisation more explicitly. The factor graph structure comprises the set of factor nodes mathcalF=f_1dotsf_m, where each factor node  f_i represents local function psi(mathcalX_i), and the set of variable nodes mathcalX. The factor node f_i connects to the variable node x_s if and only if x_s in mathcalX_i.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The BP algorithm on factor graphs proceeds by passing two types of messages along the edges of the factor graph:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"a variable node x_s in mathcalX to a factor node f_i in mathcalF message mu_x_s to f_i(x_s), and\na factor node f_i in mathcalF to a variable node x_s in mathcalX message mu_f_i to x_s(x_s).","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Both variable and factor nodes in a factor graph process the incoming messages and calculate outgoing messages, where an output message on any edge depends on incoming messages from all other edges. The BP messages represent ``beliefs\" about variable nodes, thus a message that arrives or departs a certain variable node is a function (distribution) of the random variable corresponding to the variable node.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The Gaussian belief propagation (GBP) represents a class of the BP, where local function psi(mathcalX_i) is defined as a continuous Gaussian distribution:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    mathcalN(z_imathcalX_iv_i) propto expBigg-cfracz_i-h(mathcalX_i)^22v_iBigg","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where v_i is the variance, and the function h(mathcalX_i) connects the set of state variables mathcalX_i to the known z_i value. The \\emph{linear}-GBP model implies the linear function h(mathcalX_i). If the linear-GBP algorithm converges, it will converge to a fixed point representing a true means \\cite{bickson}, regardless of the structure of the factor graph. Unlike means, the variances of the linear-GBP algorithm may not converge to correct values for graphical models with loops, while for models without loops (i.e., tree factor graph) variances will have exact values.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Under the native GBP algorithm , we imply the algorithm in which messages are calculated as described below.","category":"page"},{"location":"man/theoretical/#Message-from-a-variable-node-to-a-factor-node","page":"Theoretical Background","title":"Message from a variable node to a factor node","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Consider a part of a factor graph with a group of factor nodes mathcalF_s=f_if_wf_W subseteq mathcalF that are neighbours of the variable node x_s in mathcalX. The message mu_x_s to f_i(x_s) from the variable node x_s to the factor node f_i is equal to the product of all incoming factor node to variable node messages arriving at all the other incident edges:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    mu_x_s to f_i(x_s) =prod_f_a in mathcalF_s setminus f_i mu_f_a to x_s(x_s)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathcalF_s setminus f_i represents the set of factor nodes incident to the variable node x_s, excluding the factor node f_i. Note that each message is a function of the variable x_s.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Let us assume that the incoming messages mu_f_w to x_s(x_s), dots, mu_f_W to x_s(x_s) into the variable node x_s are Gaussian and represented by their mean-variance pairs (z_f_w to x_sv_f_w to x_s), dots, (z_f_W to x_sv_f_W to x_s). Note that these messages carry beliefs about the variable node x_s provided by its neighbouring factor nodes mathcalF_ssetminus f_i. It can be shown that the message mu_x_s to f_i(x_s) from the variable node x_s to the factor node f_i is proportional to:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    mu_x_s to f_i(x_s) propto mathcalN(x_sz_x_s to f_i v_x_s to f_i)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"with mean z_x_s to f_i and variance v_x_s to f_i obtained as:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    z_x_s to f_i = Bigg( sum_f_a in mathcalF_ssetminus f_i cfracz_f_a to x_sv_f_a to x_sBigg) v_x_s to f_i \n    cfrac1v_x_s to f_i = sum_f_a in mathcalF_ssetminus f_i cfrac1v_f_a to x_s","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"After the variable node x_s receives the messages from all of the neighbouring factor nodes from the set mathcalF_ssetminus f_i, it evaluates the message mu_x_s to f_i(x_s), and sends it to the factor node f_i.","category":"page"},{"location":"man/theoretical/#Message-from-a-factor-node-to-a-variable-node","page":"Theoretical Background","title":"Message from a factor node to a variable node","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Consider a part of a factor graph that consists of a group of variable nodes mathcalX_i = x_s x_lx_L subseteq mathcal X that are neighbours of the factor node f_i in mathcalF. The message mu_f_i to x_s(x_s) from the factor node f_i to the variable node x_s is defined as a product of all incoming variable node to factor node messages arriving at other incident edges, multiplied by the function psi_i(mathcalX_i) associated to the factor node f_i, and marginalised over all of the variables associated with the incoming messages:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    mu_f_i to x_s(x_s)= intlimits_x_ldotsintlimits_x_L psi_i(mathcalX_i)\n    prod_x_b in mathcalX_isetminus x_s bigmu_x_b to f_i(x_b) cdot mathrmdx_bbig","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathcalX_isetminus x_s is the set of variable nodes incident to the factor node f_i, excluding the variable node x_s.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Due to linearity of measurement functions h_i(mathcalX_i), closed form expressions for these messages is easy to obtain and follow a Gaussian form:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    mu_f_i to x_s(x_s) propto mathcalN(x_sz_f_i to x_sv_f_i to x_s)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The message mu_f_i to x_s(x_s) can be computed only when all other incoming messages (variable to factor node messages) are known. Let us assume that the messages into factor nodes are Gaussian, denoted by:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"        mu_x_l to f_i(x_l) propto mathcalN(x_lz_x_l to f_i v_x_l to f_i)\n        vdots\n        mu_x_L to f_i(x_L) propto mathcalN(x_Lz_x_L to f_i v_x_L to f_i)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The Gaussian function associated with the factor node f_i is:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    mathcalN(z_imathcalX_i v_i) propto expBigg-cfracz_i-h_i(mathcalX_i)^2 2v_iBigg","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The model contains only linear functions which we represent in a general form as:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    h_i(mathcalX_i) = C_x_s x_s + sum_x_b in mathcalX_isetminus x_s C_x_b x_b","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathcalX_isetminus x_s is the set of variable nodes incident to the factor node f_i, excluding the variable node x_s.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"It can be shown that the message mu_f_i to x_s(x_s) from the factor node f_i to the variable node x_s is represented by the Gaussian function \\eqref{BPGaussfv}, with mean z_f_i to x_s and variance v_f_i to x_s obtained as:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"        z_f_i to x_s = cfrac1C_x_s Bigg(z_i - sum_x_b in mathcalX_i setminus x_s\n        C_x_b z_x_b to f_i Bigg)\n        v_f_i to x_s = cfrac1C_x_s^2 Bigg( v_i + sum_x_b in mathcalX_i setminus x_s C_x_b^2 v_x_b to f_i  Bigg)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"To summarise, after the factor node f_i receives the messages from all of the neighbouring variable nodes from the set mathcalX_isetminus x_s, it evaluates the message mu_f_i to x_s(x_s), and sends it to the variable node x_s.","category":"page"},{"location":"man/theoretical/#Marginal-inference","page":"Theoretical Background","title":"Marginal inference","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The marginal of the variable node x_s is obtained as the product of all incoming messages into the variable node x_s:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    p(x_s) =prod_f_c in mathcalF_s mu_f_c to x_s(x_s)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathcalF_s is the set of factor nodes incident to the variable node x_s. It can be shown that the marginal of the state variable x_s is represented by:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    p(x_s) propto mathcalN(x_shat x_sv_x_s)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"with the mean value hat x_s and variance v_x_s:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    hat x_s = Bigg( sum_f_c in mathcalF_s cfracz_f_c to x_sv_f_c to x_sBigg) v_x_s \n    cfrac1v_x_s = sum_f_c in mathcalF_s cfrac1v_f_c to x_s","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Finally, the mean-value hat x_s is adopted as the estimated value of the state variable x_s.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"","category":"page"},{"location":"man/theoretical/#efficientGBP","page":"Theoretical Background","title":"Computation-efficient GBP Algorithm","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"We can make a substantial improvement to the vanilla GBP algorithm's complexity by reducing the number of calculations per outgoing messages. We achieve this reduction by summarisation of all incoming messages for each variable and factor node instead of summarising all incoming messages per each outgoing message. This simple trick, allow a single variable or factor node to share these summations across all outgoing messages, hence calculating these summations only once. As a result, each outgoing message involves a constant number of operations improving the worst-case running complexity to mathcalO(nm). In this framework, we calculate the message from the variable node to the factor node as:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"        z_x_s to f_i = Bigg(alpha_x_s - cfracz_f_i to x_sv_f_i to x_sBigg) v_x_s to f_i \n        cfrac1v_x_s to f_i = beta_x_s - cfrac1v_f_i to x_s","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    alpha_x_s = sum_f_a in mathcalF_s cfracz_f_a to x_sv_f_a to x_s  quad\n    beta_x_s = sum_f_a in mathcalF_s cfrac1v_f_a to x_s","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Likewise, the message from the factor node to the variable node is:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    z_f_i to x_s = cfrac1C_x_s left(z_i - alpha_f_i right) + z_x_s to f_i \n    v_f_i to x_s = cfrac1C_x_s^2 left( v_i +  beta_f_i  right) - v_x_s to f_i","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    alpha_f_i = sum_x_b in mathcalX_i C_x_b z_x_b to f_i  quad\n    beta_f_i = sum_x_b in mathcalX_i C_x_b^2 v_x_b to f_i","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"","category":"page"},{"location":"man/theoretical/#kahanGBP","page":"Theoretical Background","title":"The GBP and Kahan–Babuška Algorithm","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The major drawback of the computation-efficient GBP algorithm is sensitivity to numerical errors because of the summation of floating-point numbers, due to possible significant differences in the values of incoming means and variances. However, this limitation can be alleviated with a compensated summation algorithm, such as the Kahan summation or the improved Kahan–Babuška algorithm. These algorithms increase the complexity of the operations by a constant factor, which means the time complexity of the worst-case remains unaffected. More precisely, we do summation that exists in the messages as:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"function kahan(summands, total, epsilon)\n    t = total + summands\n    if abs(total) >= abs(summands)\n        epsilon += (total - t) + summands\n    else\n        epsilon += (summands - t) + total\n    end\n    total = t\n\n    return total, epsilon\nend","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"","category":"page"},{"location":"man/theoretical/#synchronous","page":"Theoretical Background","title":"Synchronous Message Passing Schedule","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The GBP is an iterative algorithm, and requires a message-passing schedule. Typically, the scheduling where messages from variable to factor nodes, and messages from factor nodes to variable nodes, are updated in parallel in respective half-iterations, is known as synchronous scheduling. Synchronous scheduling updates all messages in a given iteration using the output of the previous iteration as an input.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The initialization step starts with messages from singly connected factor nodes to variable nodes. Then, variable nodes forward the incoming messages received from singly connected factor nodes along remaining edges. To ensure this, we are using virtual factor nodes. Hence, the virtual factor node is a singly connected factor node used if the variable node is not directly observed. In a typical scenario, without prior knowledge, the variance of virtual factor nodes tend to infinity. We also improve convergence performance using virtual factor nodes.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"","category":"page"},{"location":"man/theoretical/#dampGBP","page":"Theoretical Background","title":"The GBP with Randomized Damping","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"We propose a randomized damping approach, where each mean value message from factor node to a variable node is damped independently with probability p, otherwise, the message is calculated as in the standard the GBP algorithm. The damped message is evaluated as a linear combination of the message from the previous and the current iteration, with weights alpha and 1 - alpha, respectively. More, precisly, the proposed randomized damping scheduling updates of selected factor to variable node means in every iteration by combining them with their values from the previous iteration using convergence parameters p and alpha:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    z_f_i rightarrow x_s^(tau)=left(1-q_i sright) cdot z_f_i rightarrow x_s^(tau)+q_i s cdotleft(alpha cdot z_f_x rightarrow x_s^(tau-1)+(1-alpha) cdot z_f_i rightarrow x_a^(tau)right)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where q_i s sim operatornameBer(p) in01 is independently sampled with probability p for the mean from factor node f_i to the variable node x_s.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The randomised damping parameter pairs lead to a trade-off between the number of non-converging simulations and the rate of convergence. In general, we observe a large number of non-converging simulations for the selection of prob and alpha for which only a small fraction of messages are combined with their values in a previous iteration, and that is a case for prob close to 0 or alpha close to 1.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"","category":"page"},{"location":"man/theoretical/#dynamicGBP","page":"Theoretical Background","title":"The Dynamic GBP Algorithm","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"To recall, each factor node is associated with the measurement value z_i and the measurement variance  v_i. The dynamic framework allows the update of these values in any GBP iteration tau. This framework is an extension to the real-time model that operates continuously and accepts asynchronous measurements from different measurement subsystems. Such measurements are continuously integrated into the running instances of the GBP algorithm. Hence, the GBP algorithm can update the state estimate vector in a time-continuous process.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Additionally, this framework allows for the artificial addition and removal of factor nodes. Then, the initial factor graph, described with the Jacobian matrix, should include all possible measurements. Measurements that are not active are then taken into account via extremely large values of variances (e.g., 10^60). Consequently, estimates will have a unique solution according to measurement variances whose values are much smaller than 10^60.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"","category":"page"},{"location":"man/theoretical/#ageingGBP","page":"Theoretical Background","title":"The Ageing GBP Algorithm","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The ageing framework represents an extension of the dynamic model and establishes a model for measurement arrival processes and for the process of measurement deterioration or ageing over time (or GBP iterations). We integrate these measurements regularly into the running instances of the GBP algorithm.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Let us assume that factor node f_i receives the new variance v_i. After that moment, the ageing model increases variance value over iterations v_i(tau). More precisely, we associate the Gaussian distribution mathcalN(z_imathcalX_i v_i(tau)) to the corresponding factor node f_i, where the variance v_i(tau) increases its value starting from the predefined variance v_i(tau) = v_i. Finally, in practice ageing model requires defining a limit from above bar v_i of a function v_i(tau), instead of allowing variance to take on extremely large values.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Depending on the measurements arriving dynamic, an adaptive mechanism for increasing the variance over iterations v_i(tau) can be derived. The logarithmic growth model represents a promising solution for systems with a high sampling rate of the measurements, where a rapid increase in variance is required:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    v_i(tau) = begincases\n      a  textlog left(cfractau + 1 + b1 + b right ) + v_i  1 leq tau leq theta \n      bar v_i  tau geq theta\n  endcases","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where a and b control the rate of growth.  In contrast, the exponential growth model corresponds to systems with a low sampling rate of the measurements:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    v_i(tau) = begincases\n      v_i(1+b)^atau  1 leq tau leq theta \n      bar v_i  tau geq theta\n  endcases","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Finally, the linear growth model can be observed as a compromise between logarithmic and exponential growth models:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    v_i(tau) = begincases\n      atau + v_i  1 leq tau leq theta \n      bar v_i  tau geq theta\n  endcases","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"","category":"page"},{"location":"man/theoretical/#treeGBP","page":"Theoretical Background","title":"The Forward-Backward Algorithm","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The forward–backward algorithm allows exact inference in tree factor graph. We start by viewing an arbitrary variable node as the root of the factor graph and initiating messages at the leaves of the tree factor graph using. The message passing steps from variable nodes to factor nodes and from factor nodes to variable nodes are then applied recursively until messages have been propagated along every link, and the root node has received messages from all of its neighbours. Each node can send a message towards the root once it has received messages from all of its other neighbours. This step is known as the forward recursion.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The backward recursion starts when the root node received messages from all of its neighbours. It can therefore send out messages to all of its neighbours. These in turn will then have received messages from all of their neighbours and so can send out messages along the links going away from the root, and so on. In this way, messages are passed outwards from the root all the way to the leaves.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"By now, a message will have passed in both directions across every link in the graph, and every node will have received a message from all of its neighbours. Every variable node will have received messages from all of its neighbours, we can readily calculate the marginal distribution for every variable in the graph. The number of messages that have to be computed is given by twice the number of links in the graph and so involves only twice the computation involved in finding a single marginal [1].","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"","category":"page"},{"location":"man/theoretical/#refs","page":"Theoretical Background","title":"References","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"[1] C. M. Bishop, Pattern Recognition and Machine Learning (Information Science and Statistics). Berlin, Heidelberg: Springer-Verlag, 2006.","category":"page"},{"location":"#GaussBP","page":"Home","title":"GaussBP","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The GaussBP package provides the set of different functions to perform inference over the factor graph in a static or dynamic framework using the linear Gaussian belief propagation (GBP) algorithm. The linear GBP model requires the set of linear equations and provides the minimum mean squared error (MMSE) estimate of the state variables.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The software package includes algorithms based on the synchronous message passing schedule:","category":"page"},{"location":"","page":"Home","title":"Home","text":"vanilla GBP algorithm;\ncomputation-efficient GBP algorithm;\ncomputation-efficient GBP algorithm with Kahan–Babuška algorithm;\ndynamic GBP algorithm;\nageing GBP algorithm.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The software package also includes a message passing algorithm that allows exact inference in tree factor graph:","category":"page"},{"location":"","page":"Home","title":"Home","text":"forward–backward algorithm.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Requirement","page":"Home","title":"Requirement","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GaussBP requires Julia 1.6 and higher.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the GaussBP package, run the following command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add GaussBP","category":"page"},{"location":"","page":"Home","title":"Home","text":"To use GaussBP package, add the following code to your script, or alternatively run the same command in Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GaussBP","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Quick-start","page":"Home","title":"Quick start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Following examples are intended for a quick introduction to GaussBP package.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Synchronous message passing schedule using the native GBP algorithm.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GaussBP\n\ngbp = graphicalModel(\"data33_14.h5\")        # initialize the graphical model using HDF5 input\nfor iteration = 1:200                       # the GBP inference\n    messageFactorVariableVanilla(gbp)       # compute messages using the native GBP\n    messageVariableFactorVanilla(gbp)       # compute messages using the native GBP\nend\nmarginal(gbp)                               # compute marginals\ndisplayData(gbp)                            # show results","category":"page"},{"location":"","page":"Home","title":"Home","text":"Synchronous message passing schedule using the efficient GBP algorithm.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GaussBP\n\nH = [1.0 0.0 0.0; 1.5 0.0 2.0; 0.0 3.1 4.6] # jacobian matrix\nz = [0.5; 0.8; 4.1]                         # observation vector\nv = [0.1; 1.0; 1.0]                         # variance vector\n\ngbp = graphicalModel(H, z, v)               # initialize the graphical model via arguments\nfor iteration = 1:50                        # the GBP inference\n    messageFactorVariableEfficient(gbp)     # compute messages using the efficient GBP\n    messageVariableFactorEfficient(gbp)     # compute messages using the efficient GBP\nend\nmarginal(gbp)                               # compute marginals","category":"page"},{"location":"","page":"Home","title":"Home","text":"Synchronous message passing schedule using the GBP and Kahan-Babuska algorithm with the plotting of the marginal mean through iteration.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GaussBP\nusing Plots\n\ngbp = graphicalModel(\"data33_14.h5\")        # initialize the graphical model\nx6 = []                                     # save the state variable marginal\nfor iteration = 1:50                        # the GBP inference\n    messageFactorVariableKahan(gbp)         # compute messages using the GBP with Kahan-Babuska\n    messageVariableFactorKahan(gbp)         # compute messages using the GBP with Kahan-Babuska\n    marginal(gbp)                           # compute marginals\n    push!(x6, gbp.inference.mean[6])        # save state variable marginal\nend\nplot(collect(1:50), x6)                     # show plot","category":"page"},{"location":"","page":"Home","title":"Home","text":"Synchronous message passing schedule using the native GBP algorithm in the dynamic framework.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GaussBP\n\nH = [1.0 0.0 0.0; 1.5 0.0 2.0; 0.0 3.1 4.6] # jacobian matrix\nz = [0.5; 0.8; 4.1]                         # observation vector\nv = [0.1; 1.0; 1.0]                         # variance vector\n\ngbp = graphicalModel(H, z, v)               # initialize the graphical model\nfor iteration = 1:200                       # the GBP inference\n    messageFactorVariableVanilla(gbp)       # compute messages using the native GBP\n    messageVariableFactorVanilla(gbp)       # compute messages using the native GBP\nend\n\ndynamicFactor!(gbp;                         # integrate changes in the running GBP\n    factor = 1,\n    mean = 0.85,\n    variance = 1e-10)\nfor iteration = 201:400                     # continues the GBP inference\n    messageFactorVariableVanilla(gbp)       # compute messages using the native GBP\n    messageVariableFactorVanilla(gbp)       # compute messages using the native GBP\nend\nmarginal(gbp)                               # compute marginals\ndisplayData(gbp)                            # show results","category":"page"},{"location":"","page":"Home","title":"Home","text":"Synchronous message passing schedule using the native GBP algorithm in the dynamic ageing framework.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GaussBP\n\nH = [1.0 0.0 0.0; 1.5 0.0 2.0; 0.0 3.1 4.6] # jacobian matrix\nz = [0.5; 0.8; 4.1]                         # observation vector\nv = [0.1; 1.0; 1.0]                         # variance vector\n\ngbp = graphicalModel(H, z, v)               # initialize the graphical model\nfor iteration = 1:200                       # the GBP inference\n    messageFactorVariableVanilla(gbp)       # compute messages using the native GBP\n    messageVariableFactorVanilla(gbp)       # compute messages using the native GBP\nend\n\nfor iteration = 1:400                       # continues the GBP inference\n    ageingVariance!(gbp;                    # integrate changes in the running GBP\n        factor = 4,\n        initial = 1,\n        limit = 50,\n        model = 1,\n        a = 0.05,\n        tau = iteration)\n    messageFactorVariableVanilla(gbp)       # compute messages using the native GBP\n    messageVariableFactorVanilla(gbp)       # compute messages using the native GBP\nend\nmarginal(gbp)                               # compute marginals\ndisplayData(gbp)                            # show results","category":"page"},{"location":"","page":"Home","title":"Home","text":"Forward–backward algorithm over the tree factor graph.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GaussBP\n\nH = [1 0 0 0 0; 6 8 2 0 0; 0 5 0 0 0;       # jacobian matrix\n     0 0 2 0 0; 0 0 3 8 2]\nz = [1; 2; 3; 4; 5]                         # observation vector\nv = [3; 4; 2; 5; 1]                         # variance vector\n\ngbp = graphicalModelTree(H, z, v; root = 1) # initialize the tree graphical model\nwhile gbp.graph.forward                     # inference from leaves to the root\n     forwardVariableFactor(gbp)             # compute forward messages\n     forwardFactorVariable(gbp)             # compute forward messages\nend\nwhile gbp.graph.backward                    # inference from the root to leaves\n     backwardVariableFactor(gbp)            # compute backward messages\n     backwardFactorVariable(gbp)            # compute backward messages\nend\nmarginal(gbp)                               # compute marginals\ndisplayData(gbp)                            # show results\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#More-information","page":"Home","title":"More information","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"M. Cosovic and D. Vukobratovic, \"Distributed Gauss-Newton Method for State Estimation Using Belief Propagation,\" in IEEE Transactions on  Power Systems, vol. 34, no. 1, pp. 648-658, Jan. 2019. arxiv.org\nM. Cosovic, \"Design and Analysis of Distributed State Estimation Algorithms Based on Belief Propagation and Applications in Smart Grids.\" arXiv preprint arXiv:1811.08355 (2018). arxiv.org","category":"page"},{"location":"man/inferencetree/#vanilla","page":"Inference","title":"Inference","text":"","category":"section"},{"location":"man/inferencetree/","page":"Inference","title":"Inference","text":"To exchange information over the tree factor graph, the GaussBP provides forward–backward algorithm. We advise the reader to read the forward–backward algorithm which provides a detailed description of the inference algorithm.","category":"page"},{"location":"man/inferencetree/","page":"Inference","title":"Inference","text":"Each of the inference functions accepts only the composite type GraphicalModelTree, i.e., an output variable of the function gbp = graphicalModelTree().","category":"page"},{"location":"man/inferencetree/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/inferencetree/#Forward-inference","page":"Inference","title":"Forward inference","text":"","category":"section"},{"location":"man/inferencetree/","page":"Inference","title":"Inference","text":"The set of functions that can be used to preform forward message inference:","category":"page"},{"location":"man/inferencetree/","page":"Inference","title":"Inference","text":"forwardVariableFactor(gbp)\nforwardFactorVariable(gbp)","category":"page"},{"location":"man/inferencetree/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/inferencetree/#Backward-inference","page":"Inference","title":"Backward inference","text":"","category":"section"},{"location":"man/inferencetree/","page":"Inference","title":"Inference","text":"The set of functions that can be used to preform backward message inference:","category":"page"},{"location":"man/inferencetree/","page":"Inference","title":"Inference","text":"backwardVariableFactor(gbp)\nbackwardFactorVariable(gbp)","category":"page"},{"location":"man/inferencetree/","page":"Inference","title":"Inference","text":"","category":"page"},{"location":"man/inferencetree/#Marginal-inference","page":"Inference","title":"Marginal inference","text":"","category":"section"},{"location":"man/inferencetree/","page":"Inference","title":"Inference","text":"To compute marginals the GaussBP provides the function:","category":"page"},{"location":"man/inferencetree/","page":"Inference","title":"Inference","text":"marginalTree(gbp)","category":"page"},{"location":"man/inferencetree/","page":"Inference","title":"Inference","text":"Same as before, the function accepts only the composite type GraphicalModelTree.","category":"page"}]
}
