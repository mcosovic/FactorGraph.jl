var documenterSearchIndex = {"docs":
[{"location":"man/input/#inputdata","page":"Input Data","title":"Input Data","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The package GaussBP supports HDF5 and XLSX input files, or passing data directly via command line arguments. The basic input data structure describing a linear system of equations includes the jacobian matrix that contains coefficients of the equations, while vectors observation and variance represent measurement values and measurement variances, respectively. Note that, with large-scale systems, we strongly recommend using the HDF5 data format.  ","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"Running the GBP algorithm in a dynamic framework requires a variable dynamic, which defines the dynamic update scheme of the factor nodes.","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/input/#HDF5","page":"Input Data","title":"HDF5","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The HDF5 input file must contain the following elements:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"coefficient data model.h5/jacobian; \nmeasurement values model.h5/observation;\nmeasurement variances model.h5/variance. ","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The input data for the dynamic GBP algorithm must contain the additional variable:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"dynamic data model.h5/dynamic.","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The type and structure of the input data must be:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"jacobian::Array{Float64, 2} = [row column coefficient];\nobservation::Array{Float64, 1};\nvariance::Array{Float64, 1};\ndynamic::Array{Float64, 2} = [iteration factor observation variance].","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/input/#XLSX","page":"Input Data","title":"XLSX","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The XLSX input file must contain the following sheets:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"coefficient data sheet: jacobian; \nmeasurement data sheet: measurement.","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The input data for the dynamic GBP algorithm must contain the additional sheet:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"dynamic data sheet: dynamic.","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The type and structure of the input data must be:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"jacobian - row | column | coefficient;\nmeasurement -  observation | variance;\ndynamic - iteration | factor | observation | variance.","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/input/#Passing-arguments","page":"Input Data","title":"Passing arguments","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The structure of the arguments should be:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"gbp(jacobian, observation, variance)","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The structure for the dynamic GBP algorithm must contain the additional argument:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"gbp(jacobian, observation, variance, dynamic)","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The type and structure of the arguments must be:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"jacobian::Union{Array{Float64, 2}, SparseMatrixCSC{Float64, Int64}}\nobservation::Array{Float64, 1}\nvariance::Array{Float64, 1} \ndynamic::Array{Float64, 2}","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/input/#Data-Structure","page":"Input Data","title":"Data Structure","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The jacobian input data is used for all analysis available in the GaussBP package and contains coefficients of the linear system of the equations.   ","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"Column Description\n1 row indices of the corresponding jacobian matrix\n2 column indices of the corresponding jacobian matrix\n3 coefficient values of the corresponding jacobian matrix","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"&nbsp;","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The dynamic input data is used only for the dynamic GBP algorithm and contains factor nodes update scheme. ","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"Column Description\n1 the iteration number when the factor node receives new observation and variance value\n2 factor node index corresponding to the row number of the jacobian matrix\n3 new observation value\n4 new variance value","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/input/#Use-Cases","page":"Input Data","title":"Use Cases","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The pre-defined data are located in the src/example as the .h5 or .xlsx files.","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"Case Variables Equations\ndata33_14.xlsx 14 33\ndata33_14.h5 33 14\ndata897_300.h5 300 897\ndata3119_1354.h5 1354 3119\ndata5997_2000.h5 2000 5997\ndata7149_2000.h5 2000 7149\ndata29997_10000.h5 10000 29997\ndata283803_70000.h5 70000 283803\ndataDynamic33_14.xlsx 14 33\ndataDynamic33_14.h5 14 33","category":"page"},{"location":"man/runsettings/#runpf","page":"Run Settings","title":"Run Settings","text":"","category":"section"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"Input arguments of the function gbp() describe the GBP algorithm settings. The order of inputs and their appearance is arbitrary, with only DATA input required. Still, for methodological reasons, the syntax examples follow a certain order.","category":"page"},{"location":"man/runsettings/#Syntax","page":"Run Settings","title":"Syntax","text":"","category":"section"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"gbp(DATA)\ngbp(DATA; ALGORITHM)\ngbp(DATA; ALGORITHM, ITERATION)\ngbp(DATA; ALGORITHM, ITERATION, INITIAL)\ngbp(DATA; ALGORITHM, ITERATION, INITIAL, OUT)","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"&nbsp;","category":"page"},{"location":"man/runsettings/#Description","page":"Run Settings","title":"Description","text":"","category":"section"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"gbp(DATA) runs the GBP algorithm using DATA input \ngbp(DATA; ALGORITHM) selects the type of the GBP algorithm \ngbp(DATA; ALGORITHM, ITERATION) sets the iteration scheme\ngbp(DATA; ALGORITHM, ITERATION, INITIAL) sets initial value of messages\ngbp(DATA; ALGORITHM, ITERATION, INITIAL, OUT) controls output variable structure and display ","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"&nbsp;","category":"page"},{"location":"man/runsettings/#Output","page":"Run Settings","title":"Output","text":"","category":"section"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"results, system = gbp() returns results and input system data","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"&nbsp;","category":"page"},{"location":"man/runsettings/#Examples","page":"Run Settings","title":"Examples","text":"","category":"section"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"results, system = gbp(\"data897_300.h5\"; algorithm = efficient, out = terminal)","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"results, system = gbp(\"data897_300.h5\"; algorithm = kahan, out = [evaluation, terminal])","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"results, system = gbp(\"data33_14.xlsx\"; variance = 1e60, out = [iteration, evaluation])","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"","category":"page"},{"location":"man/runsettings/#Variable-Arguments","page":"Run Settings","title":"Variable Arguments","text":"","category":"section"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"The GBP function gbp() receives the variable argument DATA. ","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"DATA input system data\n \nExample \"data33_14.h5\"\nDescription loads the system data using h5-file from the package\n \nExample \"data33_14.xlsx\"\nDescription loads the system data using xlsx-file from the package\n \nExample \"C:/name.h5\"\nDescription loads the system data using h5-file from a custom path\n \nExample \"C:/name.xlsx\"\nDescription loads the system data using xlsx-file from a custom path\n \nExample jacobian, observation, variances, dynamic\nDescription loads the system data passing arguments directly","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"","category":"page"},{"location":"man/runsettings/#Keyword-Arguments","page":"Run Settings","title":"Keyword Arguments","text":"","category":"section"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"The GBP function gbp() receives a group of arguments by keyword: ALGORITHM, ITERATION, INITIAL, OUT.","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"ALGORITHM selects the type of the algorithm\n \nCommand algorithm = vanilla\nDescription runs the solver using the native GBP algorithm, default ALGORITHM setting\n \nCommand algorithm = efficient\nDescription runs the solver using the computation-efficient GBP algorithm\n \nCommand algorithm = kahan\nDescription runs the solver using the computation-efficient GBP algorithm with compensated summation\n \nCommand algorithm = vanillaDynamic\nDescription runs the dynamic solver using the native GBP algorithm\n \nCommand algorithm = efficientDynamic\nDescription runs the dynamic solver using the computation-efficient GBP algorithm\n \nCommand algorithm = kahanDynamic\nDescription runs the dynamic solver using the computation-efficient GBP algorithm with compensated summation","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"&nbsp;","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"ITERATION sets the iteration scheme\n \nCommand max = value\nDescription specifies the maximum number of iterations, default setting: max = 30\n \nCommand bump = value\nDescription specifies the iteration when to suspend the computation of variances (in a usual scenario, variances converge much faster than means) default setting: bump = max\n \nCommand damp = value\nDescription specifies the iteration where applied randomised damping, default setting: damp = max\n \nCommand prob = value\nDescription a Bernoulli random variable with probability prob = value independently sampled for each mean value message from a factor node to a variable node, applied for randomised damping iteration scheme with value between 0 and 1, default setting: prob = 0.6\n \nCommand alpha = value\nDescription the damped message is evaluated as a linear combination of the message from the previous and the current iteration, with weights alpha = value and 1 - alpha, applied for randomised damping iteration scheme where alpha is between 0 and 1, default setting: alpha = 0.4","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"note: Randomised Damping\nWe provide an improved GBP algorithm that applies synchronous scheduling with randomised damping. The randomised damping parameter pairs lead to a trade-off between the number of non-converging simulations and the rate of convergence. In general, for the selection of prob and alpha for which only a small fraction of messages are combined with their values in a previous iteration, and that is a case for prob close to 0 or alpha close to 1, we observe a large number of non-converging simulations.","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"&nbsp;","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"INITIAL sets initial message values\n \nCommand mean = value\nDescription the initial mean value of messages from variable nodes to factor nodes\n \nCommand variance = value\nDescription the initial variance value of messages from variable nodes to factor nodes","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"note: Initial Message Values\nThe initial message values are only used across links incident with variable nodes if those are not directly observed. Otherwise, the initial messages are equal to the observed values.","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"&nbsp;","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"OUT controls output variable structure and display\n \nCommand out = iteration\nDescription saves means and variances of the marginals through iterations\n \nCommand out = evaluation\nDescription computes error metrics of the GBP algorithm, note that if combined evaluation with iteration (out = [iteration, evaluation]) computes error metrics through iterations\n \nCommand out = wls\nDescription computes the solution using the WLS method and error metrics of the GBP algorithm according to the WLS solution, for the combination of wls with iterate (out = [iteration, wls]) error metrics are computed through iterations\n \nCommand out = terminal\nDescription shows data display in the Julia REPL","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"note: OUT\nThe keyword out accepts any subset of commands iteration, evaluation, wls, terminal. ","category":"page"},{"location":"man/output/#outputdata","page":"Output Data","title":"Output Data","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The function gbp() returns a struct variable results with fields gbp and wls. ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The variable results.gbp contains the following fields:","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"mean, variance, iteration, rmse, mae, wrss ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"These variables save the GBP algorithm results. Values of the error metrics rmse, mae and wrss are available only if we use the command out = evaluation. ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The variable results.wls contains the following fields:","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"mean rmse, mae, wrss, rmseGBPWLS, maeGBPWLS ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Those variables save the WLS results, which can be used to compare the results obtained by the GBP algorithm. However, values are only available if we use the command out = wls.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Finally, a struct variable system contains the input data.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/output/#gbpresults","page":"Output Data","title":"The GBP Results","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The struct variable results.gbp contains the GBP algorithm results. To describe the outputs, we will use the example shown below. ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"using GaussBP\n\nH = [1.5 2.0; \n     3.1 4.6]\nz = [0.8; 4.1]\nv = [1.0; 1.0]  \n\nd = [5 2 2.4  1.5;\n     8 1 0.85 0.9]","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"&nbsp;","category":"page"},{"location":"man/output/#Compute-means-and-variances","page":"Output Data","title":"Compute means and variances","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The mean and variance vectors contain mean and variance values of the marginals obtained using the GBP algorithm after the algorithm reaches the maximum number of iterations, while variable iteration save the maximum number of iterations.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"results, ~ = gbp(H, z, v; algorithm = vanilla)\n\njulia> results.gbp.mean\n2-element Vector{Float64}:\n -5.109180124020112\n  4.148396436639841\n\njulia> results.gbp.variance\n2-element Vector{Float64}:\n 4035.0842444764703\n 4033.696884882772\n\njulia> results.gbp.iteration\n30 ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"&nbsp;","category":"page"},{"location":"man/output/#Compute-means,-variances-and-error-metrics","page":"Output Data","title":"Compute means, variances and error metrics","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"In addition, using the command out = evaluation, except mean, variance and iteration variables, we obtained root mean square error rmse, mean absolute error mae, and weighted residual sum of squares wrss of the GBP algorithm after the algorithm reaches the maximum number of iterations. ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"results, ~ = gbp(H, z, v; algorithm = vanilla, out = evaluation)\n\njulia> results.gbp.rmse\n1-element Vector{Float64}:\n 0.616577078168593\n\njulia> results.gbp.mae\n1-element Vector{Float64}:\n 0.511406044334784\n\njulia> results.gbp.wrss\n1-element Vector{Float64}:\n 1.022812088669568","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"&nbsp;","category":"page"},{"location":"man/output/#Compute-means-and-variances-through-GBP-iterations","page":"Output Data","title":"Compute means and variances through GBP iterations","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Using the keyword out = iteration, fields mean and variance are given as matrices, where each column contains mean and variance values in the corresponding iteration according to the vector iteration.   ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"results, ~ = gbp(H, z, v; algorithm = vanilla, max = 5, out = iteration)\n\njulia> results.gbp.mean\n2×5 Matrix{Float64}:\n 0.885904  -0.108862   0.643748  -0.413415  0.206589\n 0.671831   0.0883903  0.822573   0.335673  1.0947\n\njulia> results.gbp.variance\n2×5 Matrix{Float64}:\n 98361.6  48877.4  94041.0  45748.4  86286.4\n 25127.9  48877.3  24024.2  45748.0  22043.2\n\njulia> results.gbp.iteration\n5-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5 ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"&nbsp;","category":"page"},{"location":"man/output/#Compute-means,-variances-and-error-metrics-through-GBP-iterations","page":"Output Data","title":"Compute means, variances and error metrics through GBP iterations","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Further, fields rmse, mae and wrss become vectors if we use the command out = [iteration, evaluation], where each error value corresponds to the iteration according to the vector iteration.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"results, ~ = gbp(H, z, v; algorithm = vanilla, max = 5, out = [iteration, evaluation])\n\njulia> results.gbp.rmse\n5-element Vector{Float64}:\n 1.8058973401950722\n 2.904011755263201\n 1.7463457881882882\n 2.764690391451094\n 1.6388328035960575\n\njulia> results.gbp.mae\n5-element Vector{Float64}:\n 1.8046205188319902\n 2.4086945100967365\n 1.745111115124653\n 2.293135757520099\n 1.6376741235455516\n\njulia> results.gbp.wrss\n5-element Vector{Float64}:\n -3.6092410376639803\n  4.817389020193473\n -3.490222230249306\n  4.586271515040198\n -3.2753482470911033","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"&nbsp;","category":"page"},{"location":"man/output/#Compute-means,-variances-and-error-metrics-in-the-dynamic-framework","page":"Output Data","title":"Compute means, variances and error metrics in the dynamic framework","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The dynamic framework allows computing only means and variances, and error metrics if we use the command out = evaluation. In the dynamic framework, means, variances and/or error metrics are evaluated before each new measurement update. Thus, fields mean and variance are given as matrices, where each column contains mean and variance values in the corresponding iteration according to the vector iteration. Note that according to the variable d, updates occur in the fifth and eighth iteration. ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"results, ~ = gbp(H, z, v, d; algorithm = vanillaDynamic)\n\njulia> results.gbp.mean\n2×3 Matrix{Float64}:\n -0.413415  -0.161671  -1.25493\n  0.335673   0.944561   1.2827\n\njulia> results.gbp.variance\n2×3 Matrix{Float64}:\n 45748.4  76467.6  4035.08\n 45748.0  19535.0  4033.72\n\njulia> results.gbp.iteration\n3-element Vector{Int64}:\n  4\n  7\n 30","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"&nbsp;","category":"page"},{"location":"man/output/#Compute-means,-variances-and-error-metrics-in-the-dynamic-framework-through-iteration","page":"Output Data","title":"Compute means, variances and error metrics in the dynamic framework through iteration","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Similar to before, using the keyword out = iteration, fields mean and variance are given as matrices, where each column contains mean and variance values in the corresponding iteration according to the vector iteration. In addition, using the keyword out = [iteration evaluation] error metrics can be also evaluated through iterations. ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"results, ~ = gbp(H, z, v, d; algorithm = vanillaDynamic, max = 5, out = iteration)\n\nulia> results.gbp.mean\n2×5 Matrix{Float64}:\n 0.885904  -0.108862   0.643748  -0.413415  0.0664998\n 0.671831   0.0883903  0.822573   0.335673  0.819545\n\njulia> results.gbp.variance\n2×5 Matrix{Float64}:\n 98361.6  48877.4  94041.0  45748.4  86286.4\n 25127.9  48877.3  24024.2  45748.0  22043.2\n\njulia> results.gbp.iteration\n5-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/output/#The-WLS-Results","page":"Output Data","title":"The WLS Results","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The struct variable results.wls contains results obtained using the WLS, which can be used to compare results obtained by the GBP algorithm. To describe the outputs, we will use the example given in the GBP algorithm results section. ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"&nbsp;","category":"page"},{"location":"man/output/#Compute-WLS-solution-and-error-metrics","page":"Output Data","title":"Compute WLS solution and error metrics","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Using the command out = wls, we obtained error metrics rmse, mae and wrss according to the WLS solution mean. Fields rmseGBPWLS and maeGBPWLS determine distances between the GBP estimate and WLS estimate after the GBP algorithm reaches the maximum number of iterations.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"results, ~ = gbp(H, z, v, d; algorithm = vanilla, out = wls)\n\njulia> results.wls.mean\n2-element Vector{Float64}:\n -6.457142857142001\n  5.242857142856556\n\njulia> results.wls.rmse\n1-element Vector{Float64}:\n 8.537275777709492e-14\n\njulia> results.wls.mae\n1-element Vector{Float64}:\n 7.938094626069869e-14\n\njulia> results.wls.wrss\n1-element Vector{Float64}:\n -6.283862319378386e-14\n\njulia> results.wls.rmseGBPWLS\n1-element Vector{Float64}:\n 0.17925300226918328\n\njulia> results.wls.maeGBPWLS\n1-element Vector{Float64}:\n 1.2212117196693022","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"&nbsp;","category":"page"},{"location":"man/output/#Compute-WLS-solution-and-error-metrics-through-GBP-iterations","page":"Output Data","title":"Compute WLS solution and error metrics through GBP iterations","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Using command out = [iteration, wls], except variables mean, rmse, mae and wrss, we obtained fields rmseGBPWLS and maeGBPWLS for the each GBP iteration, where each element is related to the elements of the vector results.gbp.iteration","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"results, ~ = gbp(H, z, v, d; algorithm = vanilla, max = 5, out = [iteration, wls])\n\njulia> results.wls.rmseGBPWLS\n5-element Vector{Float64}:\n 1.9601144215518003\n 0.8441540294119615\n 1.895475083793785\n 0.8036572271155497\n 1.7787820268143777\n\njulia> results.wls.maeGBPWLS\n5-element Vector{Float64}:\n 5.957036597700661\n 5.751373926050387\n 5.760587282671439\n 5.475456017760832\n 5.405943343568386\n\njulia> results.gbp.iteration\n5-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"&nbsp;","category":"page"},{"location":"man/output/#Compute-WLS-solution-and-error-metrics-in-the-dynamic-framework","page":"Output Data","title":"Compute WLS solution and error metrics in the dynamic framework","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"In the dynamic framework, means and error metrics are evaluated before each new measurement update. Thus, field mean is given as matrix, where each column contains mean values in the corresponding iteration according to the vector results.gbp.iteration. Note that according to the variable d, updates occur in the fifth and eighth iteration. Also, error metrics are evaluated at the same iterations. ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"results, ~ = gbp(H, z, v, d; algorithm = vanillaDynamic, out = wls)\n\njulia> results.wls.mean\n2×3 Matrix{Float64}:\n -6.45714  -1.6  -1.27143\n  5.24286   1.6   1.37857\n\njulia> results.wls.rmse\n3-element Vector{Float64}:\n 8.537275777709492e-14\n 1.3892435022089475e-14\n 2.0123079445926824e-14\n\njulia> results.wls.mae\n3-element Vector{Float64}:\n 7.938094626069869e-14\n 1.3655743202889425e-14\n 2.0039525594484076e-14\n\njulia> results.wls.wrss\n3-element Vector{Float64}:\n -6.283862319378386e-14\n -8.807769328692908e-15\n  1.2163110025337826e-14\n\njulia> results.wls.rmseGBPWLS\n3-element Vector{Float64}:\n 0.8036572271155497\n 0.553587197227041\n 0.05612434414868057\n\njulia> results.wls.maeGBPWLS\n3-element Vector{Float64}:\n 5.475456017760832\n 1.0468839767712772\n 0.05618486344011808\n\njulia> results.gbp.iteration\n3-element Vector{Int64}:\n  4\n  7\n 30","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"&nbsp;","category":"page"},{"location":"man/output/#Compute-means,-variances-and-error-metrics-in-the-dynamic-framework-through-iteration-2","page":"Output Data","title":"Compute means, variances and error metrics in the dynamic framework through iteration","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Using command out = [iteration, wls], variables mean, rmse, mae and wrss are still evaluated before each new measurement update. Further, variables rmseGBPWLS and maeGBPWLS are obtained for each GBP iteration, where each element is related to the elements of the vector results.gbp.iteration.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"results, ~ = gbp(H, z, v, d; algorithm = vanillaDynamic, max = 5, out = [iteration, wls])\n\njulia> results.wls.rmseGBPWLS\n5-element Vector{Float64}:\n 1.1014847586826113\n 0.014475633457227476\n 1.0368454209245965\n 0.8036572271155497\n 0.6265283007716769\n\njulia> results.wls.maeGBPWLS\n5-element Vector{Float64}:\n 0.7788673422381023\n 0.09862607394889142\n 0.7331604281780024\n 5.475456017760832\n 1.2234773564418984\n\njulia> results.gbp.iteration\n5-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/output/#Error-Metrics","page":"Output Data","title":"Error Metrics","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The root mean square error, the mean absolute error and the weighted residual sum of squares are evaluated according to:","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"  beginaligned\n    textrmse = sqrt cfracsum_i=1^m leftz_i - h_i(hatmathbf x) right^2m quad\n    textmae = cfracsum_i=1^m leftz_i - h_i(hatmathbf x) rightm quad\n    textwrss = sum_i=1^m cfracleftz_i - h_i(hatmathbf x) right^2v_i \n  endaligned","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"where m denotes the number of observations, z_i is observation value, v_i is observation variance, and corresponding equation h_i(hatmathbf x) is evaluated at the point hatmathbf x obtained using the GBP or WLS algorithm. Note, wrss is the value of the objective function of the optimization problem we are solving.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"&nbsp;","category":"page"},{"location":"man/output/#The-WLS-and-GBP-error-metrics","page":"Output Data","title":"The WLS and GBP error metrics","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Fields rmseGBPWLS and maeGBPWLS determine distance beetwen the GBP estimate hatx_textgbpi and WLS estimate hatx_textwlsi, where root mean square error and mean absolute error are obtained using:","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"  beginaligned\n    textrmse = sqrt cfracsum_i=1^n lefthatx_textwlsi - hatx_textgbpi) right^2n quad\n    textmae = cfracsum_i=1^n lefthatx_textwlsi - hatx_textgbpi) rightn\n  endaligned","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"where n is the number of state variabels.","category":"page"},{"location":"man/theoretical/#theoretical","page":"Theoretical Background","title":"Theoretical Background","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"As an input, we observe a noisy linear system of equations with real coefficients and variables:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"        mathbfz=mathbfh(mathbfx)+mathbfu","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathbf x=x_1dotsx_n^T is the vector of the state variables, mathbfh(mathbfx)= h_1(mathbfx), dots, h_k(mathbfx)^T is the vector of observation or measurement functions,  mathbfz = z_1dotsz_m^T is the vector of measurement values, and mathbfu = u_1dotsu_k^T is the vector of uncorrelated measurement errors. The linear system of equations is an overdetermined mn arising in many technical fields, such as statistics, signal processing, and control theory. ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Each observation is associated with measured value z_i, measurement error  u_i, and measurement function h_i(mathbfx). Under the assumption that measurement errors u_i follow a zero-mean Gaussian distribution, the probability density function associated with the i-th measurement is proportional to:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    mathcalN(z_imathbfxv_i) propto expBigg-cfracz_i-h_i(mathbfx)^22v_iBigg","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where v_i is the measurement variance defined by the measurement error u_i, and the measurement function h_i(mathbfx) connects the vector of state variables mathbfx to the value of the i-th measurement.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The goal is to determine state variables mathbfx according to the noisy observed data mathbfz and a prior knowledge: ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":" \tp(mathbfxmathbfz)= cfracp(mathbfzmathbfx)p(mathbfx)p(mathbfz)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Assuming that the prior probability distribution p(mathbfx) is uniform, and given that p(mathbfz) does not depend on mathbfx, the maximum a posteriori solution reduces to the maximum likelihood solution, as given below:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\thatmathbfx= mathrmargmax_mathbfxp(mathbfxmathbfz)= mathrmargmax_mathbfxp(mathbfzmathbfx)=\n\tmathrmargmax_mathbfxmathcalL(mathbfzmathbfx)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"One can find the solution via maximization of the likelihood function mathcalL(mathbfzmathbfx), which is defined via likelihoods of m independent measurements:  ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\thatmathbf x= mathrmarg max_mathbfxmathcalL(mathbfzmathbfx)= \n    mathrmarg max_mathbfx prod_i=1^m mathcalN(z_imathbfxv_i)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"It can be shown that the maximum a posteriori solution can be obtained by solving the following optimization problem, known as the weighted least-squares (WLS) problem:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\thatmathbf x = mathrmargmin_mathbfx sum_i=1^m  cfracz_i-h_i(mathbf x)^2v_i","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The state estimate hatmathbf x representing the solution of the optimization problem is known as the WLS estimator. The maximum likelihood and WLS estimator are equivalent to the maximum a posteriori solution.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"","category":"page"},{"location":"man/theoretical/#vanillaGBP","page":"Theoretical Background","title":"Linear GBP Algorithm","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"In the standard setup, the goal of the belief propagation (BP) algorithm is to efficiently evaluate the marginals of a set of random variables mathcalX = x_1dotsx_n described via the joint probability density function g(mathcalX). Assuming the function g(mathcalX) can be factorised proportionally (propto) to a product of local functions:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    g(mathcalX) propto prod_i=1^m psi(mathcalX_i)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathcalX_i subseteq mathcalX. The first step is forming a factor graph, which is a bipartite graph that describes the structure of the factorisation. Factor graph allows a graph-based representation of probability density functions using variable and factor nodes connected by edges. In contrast to directed and undirected graphical models, factor graphs provide the details of the factorisation in more explicit way. The factor graph structure comprises the set of factor nodes mathcalF=f_1dotsf_m, where each factor node  f_i represents local function psi(mathcalX_i), and the set of variable nodes mathcalX. The factor node f_i connects to the variable node x_s if and only if x_s in mathcalX_i. ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The BP algorithm on factor graphs proceeds by passing two types of messages along the edges of the factor graph: ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"a variable node x_s in mathcalX to a factor node f_i in mathcalF message mu_x_s to f_i(x_s), and  \na factor node f_i in mathcalF to a variable node x_s in mathcalX message mu_f_i to x_s(x_s).","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Both variable and factor nodes in a factor graph process the incoming messages and calculate outgoing messages, where an output message on any edge depends on incoming messages from all other edges. The BP messages represent ``beliefs\" about variable nodes, thus a message that arrives or departs a certain variable node is a function (distribution) of the random variable corresponding to the variable node.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The Gaussian belief propagation (GBP) represents a class of the BP, where local function psi(mathcalX_i) is defined as a continuous Gaussian distribution:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    mathcalN(z_imathcalX_iv_i) propto expBigg-cfracz_i-h(mathcalX_i)^22v_iBigg","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where v_i is the variance, and the function h(mathcalX_i) connects the set of state variables mathcalX_i to the known z_i value. The \\emph{linear}-GBP model implies the linear function h(mathcalX_i). If the linear-GBP algorithm converges, it will converge to a fixed point representing a true means \\cite{bickson}, regardless of the structure of the factor graph. Unlike means, the variances of the linear-GBP algorithm may not converge to correct values for graphical models with loops, while for models without loops (i.e., tree factor graph) variances will have exact values.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Under the native GBP algorithm , we imply the algorithm in which messages are calculated as described below.","category":"page"},{"location":"man/theoretical/#Message-from-a-variable-node-to-a-factor-node","page":"Theoretical Background","title":"Message from a variable node to a factor node","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Consider a part of a factor graph with a group of factor nodes mathcalF_s=f_if_wf_W subseteq mathcalF that are neighbours of the variable node x_s in mathcalX. The message mu_x_s to f_i(x_s) from the variable node x_s to the factor node f_i is equal to the product of all incoming factor node to variable node messages arriving at all the other incident edges: ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    mu_x_s to f_i(x_s) =prod_f_a in mathcalF_s setminus f_i mu_f_a to x_s(x_s)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathcalF_s setminus f_i represents the set of factor nodes incident to the variable node x_s, excluding the factor node f_i. Note that each message is a function of the variable x_s.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Let us assume that the incoming messages mu_f_w to x_s(x_s), dots, mu_f_W to x_s(x_s) into the variable node x_s are Gaussian and represented by their mean-variance pairs (z_f_w to x_sv_f_w to x_s), dots, (z_f_W to x_sv_f_W to x_s). Note that these messages carry beliefs about the variable node x_s provided by its neighbouring factor nodes mathcalF_ssetminus f_i. It can be shown that the message mu_x_s to f_i(x_s) from the variable node x_s to the factor node f_i is proportional to:  ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\tmu_x_s to f_i(x_s) propto mathcalN(x_sz_x_s to f_i v_x_s to f_i)\t\t","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"with mean z_x_s to f_i and variance v_x_s to f_i obtained as: ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    z_x_s to f_i = Bigg( sum_f_a in mathcalF_ssetminus f_i cfracz_f_a to x_sv_f_a to x_sBigg) v_x_s to f_i \n\tcfrac1v_x_s to f_i = sum_f_a in mathcalF_ssetminus f_i cfrac1v_f_a to x_s","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"After the variable node x_s receives the messages from all of the neighbouring factor nodes from the set mathcalF_ssetminus f_i, it evaluates the message mu_x_s to f_i(x_s), and sends it to the factor node f_i. ","category":"page"},{"location":"man/theoretical/#Message-from-a-factor-node-to-a-variable-node","page":"Theoretical Background","title":"Message from a factor node to a variable node","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Consider a part of a factor graph that consists of a group of variable nodes mathcalX_i = x_s x_lx_L subseteq mathcal X that are neighbours of the factor node f_i in mathcalF. The message mu_f_i to x_s(x_s) from the factor node f_i to the variable node x_s is defined as a product of all incoming variable node to factor node messages arriving at other incident edges, multiplied by the function psi_i(mathcalX_i) associated to the factor node f_i, and marginalised over all of the variables associated with the incoming messages:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    mu_f_i to x_s(x_s)= intlimits_x_ldotsintlimits_x_L psi_i(mathcalX_i)\n\tprod_x_b in mathcalX_isetminus x_s bigmu_x_b to f_i(x_b) cdot mathrmdx_bbig ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathcalX_isetminus x_s is the set of variable nodes incident to the factor node f_i, excluding the variable node x_s.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Due to linearity of measurement functions h_i(mathcalX_i), closed form expressions for these messages is easy to obtain and follow a Gaussian form:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\tmu_f_i to x_s(x_s) propto mathcalN(x_sz_f_i to x_sv_f_i to x_s)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The message mu_f_i to x_s(x_s) can be computed only when all other incoming messages (variable to factor node messages) are known. Let us assume that the messages into factor nodes are Gaussian, denoted by: ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\t\tmu_x_l to f_i(x_l) propto mathcalN(x_lz_x_l to f_i v_x_l to f_i)\n\t\tvdots\n\t\tmu_x_L to f_i(x_L) propto mathcalN(x_Lz_x_L to f_i v_x_L to f_i)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The Gaussian function associated with the factor node f_i is:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\tmathcalN(z_imathcalX_i v_i) propto expBigg-cfracz_i-h_i(mathcalX_i)^2 2v_iBigg","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The model contains only linear functions which we represent in a general form as:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\th_i(mathcalX_i) = C_x_s x_s + sum_x_b in mathcalX_isetminus x_s C_x_b x_b","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathcalX_isetminus x_s is the set of variable nodes incident to the factor node f_i, excluding the variable node x_s. ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"It can be shown that the message mu_f_i to x_s(x_s) from the factor node f_i to the variable node x_s is represented by the Gaussian function \\eqref{BPGaussfv}, with mean z_f_i to x_s and variance v_f_i to x_s obtained as: ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\t\tz_f_i to x_s = cfrac1C_x_s Bigg(z_i - sum_x_b in mathcalX_i setminus x_s \n        C_x_b z_x_b to f_i Bigg)\n        v_f_i to x_s = cfrac1C_x_s^2 Bigg( v_i + sum_x_b in mathcalX_i setminus x_s C_x_b^2 v_x_b to f_i  Bigg)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"To summarise, after the factor node f_i receives the messages from all of the neighbouring variable nodes from the set mathcalX_isetminus x_s, it evaluates the message mu_f_i to x_s(x_s), and sends it to the variable node x_s. ","category":"page"},{"location":"man/theoretical/#Marginal-inference","page":"Theoretical Background","title":"Marginal inference","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The marginal of the variable node x_s is obtained as the product of all incoming messages into the variable node x_s:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    p(x_s) =prod_f_c in mathcalF_s mu_f_c to x_s(x_s)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathcalF_s is the set of factor nodes incident to the variable node x_s. It can be shown that the marginal of the state variable x_s is represented by: ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    p(x_s) propto mathcalN(x_shat x_sv_x_s)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"with the mean value hat x_s and variance v_x_s:\t\t","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    hat x_s = Bigg( sum_f_c in mathcalF_s cfracz_f_c to x_sv_f_c to x_sBigg) v_x_s \n\tcfrac1v_x_s = sum_f_c in mathcalF_s cfrac1v_f_c to x_s","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Finally, the mean-value hat x_s is adopted as the estimated value of the state variable x_s. ","category":"page"},{"location":"man/theoretical/#efficientGBP","page":"Theoretical Background","title":"Computation-efficient GBP Algorithm","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"We can make a substantial improvement to the vanilla GBP algorithm's complexity by reducing the number of calculations per outgoing messages. We achieve this reduction by summarisation of all incoming messages for each variable and factor node instead of summarising all incoming messages per each outgoing message. This simple trick, allow a single variable or factor node to share these summations across all outgoing messages, hence calculating these summations only once. As a result, each outgoing message involves a constant number of operations improving the worst-case running complexity to mathcalO(nm). In this framework, we calculate the message from the variable node to the factor node as:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"        z_x_s to f_i = Bigg(alpha_x_s - cfracz_f_i to x_sv_f_i to x_sBigg) v_x_s to f_i \n\t\tcfrac1v_x_s to f_i = beta_x_s - cfrac1v_f_i to x_s","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    alpha_x_s = sum_f_a in mathcalF_s cfracz_f_a to x_sv_f_a to x_s  quad\n\tbeta_x_s = sum_f_a in mathcalF_s cfrac1v_f_a to x_s ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Likewise, the message from the factor node to the variable node is:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\tz_f_i to x_s = cfrac1C_x_s left(z_i - alpha_f_i right) + z_x_s to f_i \n    v_f_i to x_s = cfrac1C_x_s^2 left( v_i +  beta_f_i  right) - v_x_s to f_i","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    alpha_f_i = sum_x_b in mathcalX_i C_x_b z_x_b to f_i  quad\n\tbeta_f_i = sum_x_b in mathcalX_i C_x_b^2 v_x_b to f_i ","category":"page"},{"location":"man/theoretical/#kahanGBP","page":"Theoretical Background","title":"The GBP and Kahan–Babuška Algorithm","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The major drawback of the computation-efficient GBP algorithm is sensitivity to numerical errors because of the summation of floating-point numbers, due to possible significant differences in the values of incoming means and variances. However, this limitation can be alleviated with a compensated summation algorithm, such as the Kahan summation or the improved Kahan–Babuška algorithm. These algorithms increase the complexity of the operations by a constant factor, which means the time complexity of the worst-case remains unaffected. More precisely, we do summation that exist in the messages as:  ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"function kahan(summands, total, epsilon)\n    t = total + summands\n    if abs(total) >= abs(summands)\n        epsilon += (total - t) + summands\n    else\n        epsilon += (summands - t) + total\n    end\n    total = t\n    \n    return total, epsilon\nend","category":"page"},{"location":"man/theoretical/#dynamicGBP","page":"Theoretical Background","title":"The Dynamic GBP Algorithm","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"To recall, each factor node is associated with the measurement value z_i and the measurement variance  v_i. The dynamic framework allows the update of these values in any GBP iteration. This framework is an extension to the real-time model that operates continuously and accepts asynchronous measurements from different measurement subsystems. Such measurements are continuously integrated into the running instances of the GBP algorithm. Hence, the GBP algorithm can update the state estimate vector in a time-continuous process.","category":"page"},{"location":"#GaussBP","page":"Home","title":"GaussBP","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The GaussBP solver provides the solution of the linear system of equations with/without Gaussian noise using the Gaussian belief propagation (GBP) algorithm applied over the factor graph in a static or dynamic framework.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The software package includes:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Vanilla GBP algorithm;\nComputation-efficient GBP algorithm;\nComputation-efficient GBP algorithm with Kahan–Babuška algorithm;\nDynamic GBP algorithm.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GaussBP requires Julia 1.6 and higher. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the GaussBP package, run the following command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add GaussBP","category":"page"},{"location":"","page":"Home","title":"Home","text":"To load the package, use the command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using GaussBP","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using GaussBP\n\nresults, system = gbp(\"data33_14.h5\"; max = 20, out = terminal)","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GaussBP\n\nresults, system = gbp(\"data33_14.h5\"; algorithm = efficient, out = [evaluation, terminal])","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GaussBP\nusing Plots\n\nresults, system = gbp(\"data33_14.xlsx\"; variance = 1e60, out = [iteration, evaluation])\nplot(results.gbp.iteration, results.gbp.rmse)","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GaussBP\n\nH = [1.5 0.0 2.0; 0.0 3.1 4.6; 2.6 8.1 0.4]\nz = [0.8; 4.1; 2.2]\nv = [1.0; 1.0; 1.0]     \n\nresults, settings = gbp(H, z, v; algorithm = kahan, out = [wls, terminal])","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GaussBP\n\nH = [1.5 0.0 2.0; 0.0 3.1 4.6; 2.6 8.1 0.4]\nz = [0.8; 4.1; 2.2]\nv = [1.0; 1.0; 1.0]  \nd = [2 3 2.4 1.5; 15 1 0.85 0.9]\n\nresults, settings = gbp(H, z, v, d; algorithm = vanillaDynamic, out = terminal)","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#More-Information","page":"Home","title":"More Information","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"M. Cosovic and D. Vukobratovic, \"Distributed Gauss-Newton Method for State Estimation Using Belief Propagation,\" in IEEE Transactions on  Power Systems, vol. 34, no. 1, pp. 648-658, Jan. 2019. arxiv.org\nM. Cosovic, \"Design and Analysis of Distributed State Estimation Algorithms Based on Belief Propagation and Applications in Smart Grids.\" arXiv preprint arXiv:1811.08355 (2018). arxiv.org","category":"page"}]
}
