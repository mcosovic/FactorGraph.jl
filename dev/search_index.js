var documenterSearchIndex = {"docs":
[{"location":"man/input/#inputdata","page":"Input Data","title":"Input Data","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The package GaussBP supports HDF5 and CSV input files or accept arguments passed directly. The basic input data structure used to describe a linear system of equations includes the matrix H that contains coefficients of the equations, while vectors z and v represent observation or measurement values and observation variances, respectively. Note that, in the case of large-scale systems, we strongly recommend to use the HDF5 files for the input data.  ","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/input/#HDF5","page":"Input Data","title":"HDF5","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The HDF5 input file must contain the following elements:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"model_name.h5/H, model_name.h5/z, model_name.h5/v \ncoefficient data H::Array{Float64, 2} = [row column coefficient]\nobservation values z::Array{Float64, 1}\nobservation variances v::Array{Float64, 1}","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/input/#CSV","page":"Input Data","title":"CSV","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The structure of the CSV input file must contain:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"model_name.csv \ndata with columns row | column | coefficient | observation | variance","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/input/#Passing-arguments","page":"Input Data","title":"Passing arguments","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The structure of the arguments should be:","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"gbp(H, z, v)\ncoefficient data H::Union{Array{Float64, 2}, SparseMatrixCSC{Float64, Int64}}\nobservation values z::Array{Float64, 1}\nobservation variances v::Array{Float64, 1} ","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"","category":"page"},{"location":"man/input/#Use-Cases","page":"Input Data","title":"Use Cases","text":"","category":"section"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"The pre-defined data are located in the src/example as the .h5 or .csv files.","category":"page"},{"location":"man/input/","page":"Input Data","title":"Input Data","text":"Case Variables Equations\ndata33_14.csv 14 33\ndata33_14.h5 33 14\ndata897_300.h5 300 897\ndata3119_1354.h5 1354 3119\ndata5997_2000.csv 2000 5997\ndata5997_2000.h5 2000 5997\ndata7149_2000.h5 2000 7149\ndata29997_10000.h5 10000 29997\ndata283803_70000.h5 70000 283803","category":"page"},{"location":"man/runsettings/#runpf","page":"Run Settings","title":"Run Settings","text":"","category":"section"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"Input arguments of the function gbp() describe the GBP algorithm settings. The order of inputs and their appearance is arbitrary, with only DATA input required. Still, for methodological reasons, the syntax examples follow a certain order.","category":"page"},{"location":"man/runsettings/#Syntax","page":"Run Settings","title":"Syntax","text":"","category":"section"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"gbp(DATA)\ngbp(DATA; ALGORITHM)\ngbp(DATA; ALGORITHM, ITERATIONS)\ngbp(DATA; ALGORITHM, ITERATIONS, VIRTUAL)\ngbp(DATA; ALGORITHM, ITERATIONS, VIRTUAL, OUT)","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"&nbsp;","category":"page"},{"location":"man/runsettings/#Description","page":"Run Settings","title":"Description","text":"","category":"section"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"gbp(DATA) runs the GBP algorithm using DATA input \ngbp(DATA; ALGORITHM) selects the type of the GBP algorithm \ngbp(DATA; ALGORITHM, ITERATIONS) sets the iteration scheme\ngbp(DATA; ALGORITHM, ITERATIONS, VIRTUAL) sets virtual factor nodes\ngbp(DATA; ALGORITHM, ITERATIONS, VIRTUAL, OUT) controls output variable structure and display ","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"&nbsp;","category":"page"},{"location":"man/runsettings/#Output","page":"Run Settings","title":"Output","text":"","category":"section"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"results, system = bp() returns results and input system data","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"&nbsp;","category":"page"},{"location":"man/runsettings/#Examples","page":"Run Settings","title":"Examples","text":"","category":"section"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"results, system = gbp(\"data897_300.h5\"; algorithm = \"efficient\", out = \"display\")","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"results, system = gbp(\"data897_300.h5\"; algorithm = \"kahan\", out = [\"error\", \"display\"])","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"results, system = gbp(\"data33_14.csv\"; variance = 1e60, out = [\"iterate\" \"error\"])","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"","category":"page"},{"location":"man/runsettings/#Variable-Arguments","page":"Run Settings","title":"Variable Arguments","text":"","category":"section"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"The GBP function gbp() receives the variable argument DATA. ","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"DATA input system data\n \nExample \"data33_14.h5\"\nDescription loads the system data using h5-file from the package\n \nExample \"data33_14.csv\"\nDescription loads the system data using csv-file from the package\n \nExample \"C:/name.h5\"\nDescription loads the system data using h5-file from a custom path\n \nExample \"C:/name.csv\"\nDescription loads the system data using csv-file from a custom path\n \nExample \"C:/case14.xlsx\"\nDescription loads the power system data using xlsx-file from a custom path\n \nExample Jacobian, means, variances\nDescription loads the system data passing arguments directly","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"","category":"page"},{"location":"man/runsettings/#Keyword-Arguments","page":"Run Settings","title":"Keyword Arguments","text":"","category":"section"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"The GBP function gbp() receives a group of arguments by keyword: ALGORITHM, ITERATIONS, VIRTUAL, OUT.","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"ALGORITHM selects the type of the algorithm\n \nCommand algorithm = \"gbp\"\nDescription runs the solver using native GBP algorithm, default ALGORITHM setting\n \nCommand algorithm = \"efficient\"\nDescription runs the solver using computation-efficient GBP algorithm\n \nCommand algorithm = \"kahan\"\nDescription runs the solver using computation-efficient GBP algorithm with compensated summation\n \nCommand algorithm = \"recursion\"\nDescription runs the solver using recursion GBP algorithm","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"&nbsp;","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"ITERATIONS sets the iteration scheme\n \nCommand max = value\nDescription specifies the maximum number of iterations, default setting: max = 30\n \nCommand bump = value\nDescription specifies the iteration where suspend the computation of variances (in a usual scenario, variances converge much faster than means) default setting: bump = max\n \nCommand damp = value\nDescription specifies the iteration where applied randomized damping, default setting: damp = max\n \nCommand prob = value\nDescription a Bernoulli random variable with probability prob = value independently sampled for each mean value message from factor node to a variable node, applied for randomized damping iteration scheme with value between 0 and 1, default setting: prob = 0.6\n \nCommand alpha = value\nDescription the damped message is evaluated as a linear combination of the message from the previous and the current iteration, with weights alpha = value and 1 - alpha, applied for randomized damping iteration scheme where alpha is between 0 and 1, default setting: alpha = 0.4","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"note: Randomized Damping\nWe provide an improved GBP algorithm that applies synchronous scheduling with randomized damping. The randomized damping parameter pairs lead to a trade-off between the number of non-converging simulations and the rate of convergence. In general, for the selection of prob and alpha for which only a small fraction of messages are combined with their values in a previous iteration, and that is a case for prob close to 0 or alpha close to 1, we observe a large number of non-converging simulations.","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"&nbsp;","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"VIRTUAL sets virtual factor nodes\n \nCommand mean = value\nDescription the mean value of virtual factor nodes\n \nCommand variance = value\nDescription the variance value of the virtual factor nodes","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"note: Virtual Factor Nodes\nThe virtual factor node is a singly-connected factor node used if the variable node is not directly observed. In a usual scenario, without prior knowledge, the variance of virtual factor nodes tend to infinity.","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"&nbsp;","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"OUT controls output variable structure and display\n \nCommand out = \"iterate\"\nDescription saves means and variances of the marginals through iterations\n \nCommand out = \"error\"\nDescription computes error metrics of the GBP algorithm, note that if combined \"error\" with iterate (out = [\"iterate\", \"error\"]) computes error metrics through iterations\n \nCommand out = \"wls\"\nDescription computes the solution using WLS method and error metrics of the GBP algorithm according to WLS solution, note that if combined \"wls\" with iterate (out = [\"iterate\", \"wls\"]) computes error metrics through iterations\n \nCommand out = \"display\"\nDescription shows data display in the Julia REPL","category":"page"},{"location":"man/runsettings/","page":"Run Settings","title":"Run Settings","text":"note: OUT\nThe keyword out accept any subset of commands \"iterate\", \"error\", \"wls\", \"display\". ","category":"page"},{"location":"man/output/#outputdata","page":"Output Data","title":"Output Data","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The function gbp() returns a struct variable results with fields means, variances and iterations containing the GBP algorithm results, and the additional fields rmse, mae and wrss if used command out = \"error\". ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Also, if used command out = \"wls\", the struct variable results contains additional fields meansWLS, rmseWLS, maeWLS, wrssWLS, rmseBPWLS, maeBPWLS.  ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Finally, a struct variable system contains the input data.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"","category":"page"},{"location":"man/output/#The-GBP-algorithm-results","page":"Output Data","title":"The GBP algorithm results","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The vectors means and variances contains means and variances of the marginals obtained using the GBP algorithm, while iterations save the maximum number of iterations. Using the keyword out = \"iterate\", fields means and variances are given as matrices, where each column contains mean and variance values in the corresponding iteration according to the vector iterations.     ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"&nbsp;","category":"page"},{"location":"man/output/#The-GBP-error-metrics","page":"Output Data","title":"The GBP error metrics","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Using the command out = \"error\", we obtained root mean square error rmse, mean absolute error mae, and weighted residual sum of squares wrss of the GBP algorithm at the converged point. Further, fields rmse, mae and wrss become vectors if out = [\"iterate\", \"error\"] command is used, where each error value corresponds to the iteration according to the vector iterations.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The root mean square error is obtained as:","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"  beginaligned\n    textrmse = sqrt cfracsum_i=1^m leftz_i - h_i(hatmathbf x) right^2m\n  endaligned","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"where m denotes the number of observations, z_i is observation value, and corresponding equation h_i(hatmathbf x) is evaluated at the point hatmathbf x obtained using the GBP throughout iterations or at the final iteration.  ","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The mean absolute error is obtained as:","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"  beginaligned\n    textmae = cfracsum_i=1^m leftz_i - h_i(hatmathbf x) rightm\n  endaligned","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"The weighted residual sum of squares is obtained as:","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"  beginaligned\n    textwrss = sum_i=1^m cfracleftz_i - h_i(hatmathbf x) right^2v_i\n  endaligned","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"where v_i is observation variance. Note that wrss is the value of the objective function of the optimization problem we are solving.","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"&nbsp;","category":"page"},{"location":"man/output/#The-WLS-and-GBP-error-metrics","page":"Output Data","title":"The WLS and GBP error metrics","text":"","category":"section"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Using command out = \"wls\", we obtained error metrics rmseWLS, maeWLS and wrssWLS, evaluated according to above equations, where hatmathbf x is obtained by solving WLS problem. Fields rmseBPWLS and maeBPWLS determine distance beetwen the GBP estimate hatx_itextgbp and WLS estimate hatx_itextwls, where root mean square is obtainde using:","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"  beginaligned\n    textrmse = sqrt cfracsum_i=1^n lefthatx_itextwls - hatx_itextgbp) right^2n\n  endaligned","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"where n is the number of state variabels. The mean absolute error is computed as:","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"  beginaligned\n    textmae = cfracsum_i=1^n lefthatx_itextwls - hatx_itextgbp) rightn\n  endaligned","category":"page"},{"location":"man/output/","page":"Output Data","title":"Output Data","text":"Note that error metrics rmseBPWLS and maeBPWLS can be given also for each GBP iteration, if out = [\"iterate\", \"wls\"] command is used.","category":"page"},{"location":"man/theoretical/#theoretical","page":"Theoretical Background","title":"Theoretical Background","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"As an input, we observe a noisy linear system of equations with real coefficients and variables:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"        mathbfz=mathbfh(mathbfx)+mathbfu","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathbf x=x_1dotsx_n^T is the vector of the state variables, mathbfh(mathbfx)= h_1(mathbfx), dots, h_k(mathbfx)^T is the vector of observation or measurement functions,  mathbfz = z_1dotsz_m^T is the vector of measurement values, and mathbfu = u_1dotsu_k^T is the vector of uncorrelated measurement errors. The linear system of equations is an overdetermined mn that arises in many technical fields such as statistics, signal processing, and control theory. ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Each observation is associated with measured value z_i, measurement error  u_i, and measurement function h_i(mathbfx). Under the assumption that measurement errors u_i follow a zero-mean Gaussian distribution, the probability density function associated with the i-th measurement is proportional to:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    mathcalN(z_imathbfxv_i) propto expBigg-cfracz_i-h_i(mathbfx)^22v_iBigg","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where v_i is the measurement variance defined by the measurement error u_i, and the measurement function h_i(mathbfx) connects the vector of state variables mathbfx to the value of the i-th measurement.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The goal is to determine state variables mathbfx according to the noisy observed data mathbfz and a prior knowledge: ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":" \tp(mathbfxmathbfz)= cfracp(mathbfzmathbfx)p(mathbfx)p(mathbfz)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Assuming that the prior probability distribution p(mathbfx) is uniform, and given that p(mathbfz) does not depend on mathbfx, the maximum a posteriori solution reduces to the maximum likelihood solution, as given below:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\thatmathbfx= mathrmargmax_mathbfxp(mathbfxmathbfz)= mathrmargmax_mathbfxp(mathbfzmathbfx)=\n\tmathrmargmax_mathbfxmathcalL(mathbfzmathbfx)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"One can find the solution via maximization of the likelihood function mathcalL(mathbfzmathbfx), which is defined via likelihoods of m independent measurements:  ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\thatmathbf x= mathrmarg max_mathbfxmathcalL(mathbfzmathbfx)= \n    mathrmarg max_mathbfx prod_i=1^m mathcalN(z_imathbfxv_i)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"It can be shown that the maximum a posteriori solution can be obtained by solving the following optimization problem, known as the weighted least-squares (WLS) problem:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\thatmathbf x = mathrmargmin_mathbfx sum_i=1^m  cfracz_i-h_i(mathbf x)^2v_i","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The state estimate hatmathbf x representing the solution of the optimization problem is known as the WLS estimator, the maximum likelihood and WLS estimator are equivalent to the maximum a posteriori solution.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"","category":"page"},{"location":"man/theoretical/#nativeGBP","page":"Theoretical Background","title":"Linear GBP Algorithm","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"In the standard setup, the goal of the belief propagation (BP) algorithm is to efficiently evaluate the marginals of a set of random variables mathcalX = x_1dotsx_n described via the joint probability density function g(mathcalX). Assuming that the function g(mathcalX) can be factorised proportionally (propto) to a product of local functions:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    g(mathcalX) propto prod_i=1^m psi(mathcalX_i)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathcalX_i subseteq mathcalX. The first step is forming a factor graph, which is a bipartite graph that describes the structure of the factorisation. Factor graph allows a graph-based representation of probability density functions using variable and factor nodes connected by edges. In contrast to directed and undirected graphical models, factor graphs provide the details of the factorisation in more explicit way. The factor graph structure comprises the set of factor nodes mathcalF=f_1dotsf_m, where each factor node  f_i represents local function psi(mathcalX_i), and the set of variable nodes mathcalX. The factor node f_i connects to the variable node x_s if and only if x_s in mathcalX_i. ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The BP algorithm on factor graphs proceeds by passing two types of messages along the edges of the factor graph: ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"a variable node x_s in mathcalX to a factor node f_i in mathcalF message mu_x_s to f_i(x_s), and  \na factor node f_i in mathcalF to a variable node x_s in mathcalX message mu_f_i to x_s(x_s).","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Both variable and factor nodes in a factor graph process the incoming messages and calculate outgoing messages, where an output message on any edge depends on incoming messages from all other edges. The BP messages represent ``beliefs\" about variable nodes, thus a message that arrives or departs a certain variable node is a function (distribution) of the random variable corresponding to the variable node.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The Gaussian belief propagation (GBP) represents a class of the BP, where local function psi(mathcalX_i) is defined as a continuous Gaussian distribution:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    mathcalN(z_imathcalX_iv_i) propto expBigg-cfracz_i-h(mathcalX_i)^22v_iBigg","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where v_i is the variance, and the function h(mathcalX_i) connects the set of state variables mathcalX_i to the known z_i value. The \\emph{linear}-GBP model implies the linear function h(mathcalX_i), then if the linear-GBP algorithm converges, it will converge to a fixed point representing a true means \\cite{bickson}, regardless of the structure of the factor graph. Unlike means, the variances of the linear-GBP algorithm need not converge to correct values for graphical models with loops, while for models without loops (i.e., tree factor graph) variances will have exact values.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Under the native GBP algorithm , we imply the algorithm in which messages are calculated as described below.","category":"page"},{"location":"man/theoretical/#Message-from-a-variable-node-to-a-factor-node","page":"Theoretical Background","title":"Message from a variable node to a factor node","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Consider a part of a factor graph with a group of factor nodes mathcalF_s=f_if_wf_W subseteq mathcalF that are neighbours of the variable node x_s in mathcalX. The message mu_x_s to f_i(x_s) from the variable node x_s to the factor node f_i is equal to the product of all incoming factor node to variable node messages arriving at all the other incident edges: ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    mu_x_s to f_i(x_s) =prod_f_a in mathcalF_s setminus f_i mu_f_a to x_s(x_s)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathcalF_s setminus f_i represents the set of factor nodes incident to the variable node x_s, excluding the factor node f_i. Note that each message is a function of the variable x_s.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Let us assume that the incoming messages mu_f_w to x_s(x_s), dots, mu_f_W to x_s(x_s) into the variable node x_s are Gaussian and represented by their mean-variance pairs (z_f_w to x_sv_f_w to x_s), dots, (z_f_W to x_sv_f_W to x_s). Note that these messages carry beliefs about the variable node x_s provided by its neighbouring factor nodes mathcalF_ssetminus f_i. It can be shown that the message mu_x_s to f_i(x_s) from the variable node x_s to the factor node f_i is proportional to:  ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\tmu_x_s to f_i(x_s) propto mathcalN(x_sz_x_s to f_i v_x_s to f_i)\t\t","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"with mean z_x_s to f_i and variance v_x_s to f_i obtained as: ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    z_x_s to f_i = Bigg( sum_f_a in mathcalF_ssetminus f_i cfracz_f_a to x_sv_f_a to x_sBigg) v_x_s to f_i \n\tcfrac1v_x_s to f_i = sum_f_a in mathcalF_ssetminus f_i cfrac1v_f_a to x_s","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"After the variable node x_s receives the messages from all of the neighbouring factor nodes from the set mathcalF_ssetminus f_i, it evaluates the message mu_x_s to f_i(x_s), and sends it to the factor node f_i. ","category":"page"},{"location":"man/theoretical/#Message-from-a-factor-node-to-a-variable-node","page":"Theoretical Background","title":"Message from a factor node to a variable node","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Consider a part of a factor graph that consists of a group of variable nodes mathcalX_i = x_s x_lx_L subseteq mathcal X that are neighbours of the factor node f_i in mathcalF. The message mu_f_i to x_s(x_s) from the factor node f_i to the variable node x_s is defined as a product of all incoming variable node to factor node messages arriving at other incident edges, multiplied by the function psi_i(mathcalX_i) associated to the factor node f_i, and marginalised over all of the variables associated with the incoming messages:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    mu_f_i to x_s(x_s)= intlimits_x_ldotsintlimits_x_L psi_i(mathcalX_i)\n\tprod_x_b in mathcalX_isetminus x_s bigmu_x_b to f_i(x_b) cdot mathrmdx_bbig ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathcalX_isetminus x_s is the set of variable nodes incident to the factor node f_i, excluding the variable node x_s.","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Due to linearity of measurement functions h_i(mathcalX_i), closed form expressions for these messages is easy to obtain and follow a Gaussian form:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\tmu_f_i to x_s(x_s) propto mathcalN(x_sz_f_i to x_sv_f_i to x_s)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The message mu_f_i to x_s(x_s) can be computed only when all other incoming messages (variable to factor node messages) are known. Let us assume that the messages into factor nodes are Gaussian, denoted by: ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\t\tmu_x_l to f_i(x_l) propto mathcalN(x_lz_x_l to f_i v_x_l to f_i)\n\t\tvdots\n\t\tmu_x_L to f_i(x_L) propto mathcalN(x_Lz_x_L to f_i v_x_L to f_i)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The Gaussian function associated with the factor node f_i is:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\tmathcalN(z_imathcalX_i v_i) propto expBigg-cfracz_i-h_i(mathcalX_i)^2 2v_iBigg","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The model contains only linear functions which we represent in a general form as:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\th_i(mathcalX_i) = C_x_s x_s + sum_x_b in mathcalX_isetminus x_s C_x_b x_b","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathcalX_isetminus x_s is the set of variable nodes incident to the factor node f_i, excluding the variable node x_s. ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"It can be shown that the message mu_f_i to x_s(x_s) from the factor node f_i to the variable node x_s is represented by the Gaussian function \\eqref{BPGaussfv}, with mean z_f_i to x_s and variance v_f_i to x_s obtained as: ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\t\tz_f_i to x_s = cfrac1C_x_s Bigg(z_i - sum_x_b in mathcalX_i setminus x_s \n        C_x_b z_x_b to f_i Bigg)\n        v_f_i to x_s = cfrac1C_x_s^2 Bigg( v_i + sum_x_b in mathcalX_i setminus x_s C_x_b^2 v_x_b to f_i  Bigg)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"To summarise, after the factor node f_i receives the messages from all of the neighbouring variable nodes from the set mathcalX_isetminus x_s, it evaluates the message mu_f_i to x_s(x_s), and sends it to the variable node x_s. ","category":"page"},{"location":"man/theoretical/#Marginal-inference","page":"Theoretical Background","title":"Marginal inference","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The marginal of the variable node x_s is obtained as the product of all incoming messages into the variable node x_s:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    p(x_s) =prod_f_c in mathcalF_s mu_f_c to x_s(x_s)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where mathcalF_s is the set of factor nodes incident to the variable node x_s. It can be shown that the marginal of the state variable x_s is represented by: ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    p(x_s) propto mathcalN(x_shat x_sv_x_s)","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"with the mean value hat x_s and variance v_x_s:\t\t","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    hat x_s = Bigg( sum_f_c in mathcalF_s cfracz_f_c to x_sv_f_c to x_sBigg) v_x_s \n\tcfrac1v_x_s = sum_f_c in mathcalF_s cfrac1v_f_c to x_s","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Finally, the mean-value hat x_s is adopted as the estimated value of the state variable x_s. ","category":"page"},{"location":"man/theoretical/#efficientGBP","page":"Theoretical Background","title":"Computation-efficient GBP Algorithm","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"We can make a substantial improvement of the complexity by reducing the number of calculations per outgoing messages. We achieve this reduction by summarisation of all incoming messages for each variable and factor node instead of summarising all incoming messages per each outgoing message. This simple trick, allow a single variable or factor node to share these summations across all outgoing messages, hence calculating these summations only once. As a result, each outgoing message involves a constant number of operations improving the worst-case running complexity to mathcalO(nm). In this framework, we calculate the message from the variable node to the factor node as:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"        z_x_s to f_i = Bigg(alpha_x_s - cfracz_f_i to x_sv_f_i to x_sBigg) v_x_s to f_i \n\t\tcfrac1v_x_s to f_i = beta_x_s - cfrac1v_f_i to x_s","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    alpha_x_s = sum_f_a in mathcalF_s cfracz_f_a to x_sv_f_a to x_s  quad\n\tbeta_x_s = sum_f_a in mathcalF_s cfrac1v_f_a to x_s ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"Likewise, the message from the factor node to the variable node is:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"\tz_f_i to x_s = cfrac1C_x_s left(z_i - alpha_f_i right) + z_x_s to f_i \n    v_f_i to x_s = cfrac1C_x_s^2 left( v_i +  beta_f_i  right) - v_x_s to f_i","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"where:","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"    alpha_f_i = sum_x_b in mathcalX_i C_x_b z_x_b to f_i  quad\n\tbeta_f_i = sum_x_b in mathcalX_i C_x_b^2 v_x_b to f_i ","category":"page"},{"location":"man/theoretical/#kahanGBP","page":"Theoretical Background","title":"The GBP and Kahan–Babuška Algorithm","text":"","category":"section"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"The major drawback of the computation-efficient GBP algorithm is sensitivity to numerical errors because of the summation of floating-point numbers, due to possible significant differences in values of incoming means and variances. However, this limitation can be alleviated with a compensated summation algorithm, such as the Kahan summation or the improved Kahan–Babuška algorithm. These algorithms increase the complexity of the operations by a constant factor, which means the time complexity of the worst-case remains unaffected. More precisly, we do summation that exist in messages as:  ","category":"page"},{"location":"man/theoretical/","page":"Theoretical Background","title":"Theoretical Background","text":"function kahan(summands, total, epsilon)\n    t = total + summands\n    if abs(total) >= abs(summands)\n        epsilon += (total - t) + summands\n    else\n        epsilon += (summands - t) + total\n    end\n    total = t\n    \n    return total, epsilon\nend","category":"page"},{"location":"#GaussBP","page":"Home","title":"GaussBP","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The GaussBP solver provides the solution of the linear system of equations with/without Gaussian noise using the Gaussian belief propagation (GBP) algorithm applied over the factor graph.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The software package includes:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Native GBP algorithm;\nComputation-efficient GBP algorithm;\nComputation-efficient GBP algorithm with Kahan–Babuška algorithm.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GaussBP requires Julia 1.6 and higher. To install the GaussBP package, run the following command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add https://github.com/mcosovic/GaussBP.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"To load the package, use the command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using GaussBP","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using GaussBP\n\nresults, system = gbp(\"data33_14.h5\"; max = 50, out = \"display\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GaussBP\n\nresults, system = gbp(\"data33_14.h5\"; algorithm = \"kahan\", out = [\"error\", \"display\"])","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GaussBP\nusing Plots\n\nresults, system = gbp(\"data33_14.csv\"; variance = 1e60, out = [\"iterate\", \"error\", \"display\"])\nplot(results.iterations, results.rmse)","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GaussBP\n\nH = [1.5 0.0 2.0; 0.0 3.1 4.6; 2.6 8.1 0.6]\nz = [0.8; 4.1; 2.2]\nv = [1.0; 1.0; 1.0]     \n\nresults, settings = gbp(H, z, v; out = [\"error\", \"wls\", \"display\"], algorithm = \"kahan\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#More-Information","page":"Home","title":"More Information","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"M. Cosovic and D. Vukobratovic, \"Distributed Gauss-Newton Method for State Estimation Using Belief Propagation,\" in IEEE Transactions on  Power Systems, vol. 34, no. 1, pp. 648-658, Jan. 2019. arxiv.org\nM. Cosovic, \"Design and Analysis of Distributed State Estimation Algorithms Based on Belief Propagation and Applications in Smart Grids.\" arXiv preprint arXiv:1811.08355 (2018). arxiv.org","category":"page"}]
}
